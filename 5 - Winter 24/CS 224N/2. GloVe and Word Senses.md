---
Week: Week 1
---
# Gradient Descent

- We will use GD to minimze the objective cost function $J(\theta)$﻿

![[attachments/Untitled 205.png|Untitled 205.png]]

- In practice, the learning rate has a decay function. It is big in the beginning, then small once it gets closer to the optimum.

![[attachments/Untitled 1 168.png|Untitled 1 168.png]]

- Notice that the calculation of the gradient should be done before updating any parameter.
    - Calculate gradient w.r.t all parameters, then update all parameters

## SGD

![[attachments/Untitled 2 167.png|Untitled 2 167.png]]

- Randomly sample a center word (and its windows) or a batch of 32 center words, and calculate the gradient of the objective based on that.
    - This is much faster because we can get more updates, even if they are of lower quality
- In complex networks, SGD computes more quickly and does a better job
- In SGD, our gradient vectors will be very sparse, since the window we pick will only contain some words.
    
    ![[attachments/Untitled 3 164.png|Untitled 3 164.png]]
    
- We can computationally optimize this given that we know its a sparse matrix
    
    ![[attachments/Untitled 4 159.png|Untitled 4 159.png]]
    

# Word2vec algorithm family

![[attachments/Untitled 5 156.png|Untitled 5 156.png]]

## Negative Sampling

- With the naive softmax, the summation on the bottom is very expensive because you have to enumerate through all of the words

![[attachments/Untitled 6 154.png|Untitled 6 154.png]]

- Words from the same context will have higher similarity compared to words not in the same context, so we’ll train a binary classifier (log reg) to find these.

![[attachments/Untitled 7 150.png|Untitled 7 150.png]]

- To use gradient descent packages, we turn this into a minimizing function with negatives.

![[attachments/Untitled 8 141.png|Untitled 8 141.png]]

- Given the example “I really like Palo Alto”
    - A positive pair of words is “really” and “like”, because they appear in the example. You want to maximize the similarity of these words.
    - We also want to randomly sample som words that don’t appear in the example, e.g. “potato”. You want to minimize the similarity of the center word and this random sample.

# Co-occurence

- Look through documents, count how many times two words appear in each other’s windows, which we can use to calculate their similarity.

![[attachments/Untitled 9 137.png|Untitled 9 137.png]]

- Notice that “I” and “like” appear next to each other in two documents in the corpus, so their count is two.
- We can use the columns like word vectors, comparing their similarity that way
    - The column for `like` and `enjoy` are pretty similar.

## Dimensionality

![[attachments/Untitled 10 131.png|Untitled 10 131.png]]

![[attachments/Untitled 11 126.png|Untitled 11 126.png]]

- Singular values are ordered from largest to smallest, so we can pick the top k singular values to get the best rank-k approximation.

![[attachments/Untitled 12 123.png|Untitled 12 123.png]]

# GloVe

- The ratio of co-occurence probablities can encode meaning components
    
    - Meaning components is like going from male to female, queen to king, etc
    
    ![[attachments/Untitled 13 114.png|Untitled 13 114.png]]
    
    - Solid and water often co-occur with ice, so their probabilities are large.
    - If we take the ratio of these two probabilities, we can see the likeliness of a certain word co-occuring with one over the other.
- To do this, build a log-bilinear model such that the dot product between two word vectors approximated the log probability of the co-occurence
    
    ![[attachments/Untitled 14 101.png|Untitled 14 101.png]]
    
- This loss function captures this setup. We want the dot product of word vectors to be similar to the log of the co-occurence, in addition with some bias terms.
    
    - The dot product of $w_i^T w_j$﻿, and the log co-occurence is $\log X_{ij}$﻿
    
    ![[attachments/Untitled 15 96.png|Untitled 15 96.png]]
    
    - There’s also a weighting function $f$﻿ to prevent words that appear very often from overpowering the loss.

# Evaluating word vectors

![[attachments/Untitled 16 90.png|Untitled 16 90.png]]

## Intrinsic

- Test whether word analogies are correct to evaluate word vectors.

![[attachments/Untitled 17 84.png|Untitled 17 84.png]]

- The expected semantic answer would be something like man is to woman as king is to **queen**.
- This method can also be used to find the potential biases, such as searching for analogies of man and woman to nurse.

## Extrinsic (named entity recognition)

- Once example where good word vectors should help directly is **named entity recognition**
    - Identifying references to a person, organization or location: Chris Manning lives in Palo Alto.

![[attachments/Untitled 18 76.png|Untitled 18 76.png]]

# Word sense and ambiguity

![[attachments/Untitled 19 68.png|Untitled 19 68.png]]

- Instead of having a single word vector for each word, we can have a word vector for each word sense (meaning of a word)

![[attachments/Untitled 20 64.png|Untitled 20 64.png]]

- Multiple instances of a single word, one for each definition of the word.

## Linear structures

- You can separate out the senses of a single word

![[attachments/Untitled 21 58.png|Untitled 21 58.png]]