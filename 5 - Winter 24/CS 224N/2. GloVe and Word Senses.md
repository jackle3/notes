---
Week: Week 1
---
# 1 Gradient Descent

- We will use GD to minimize the objective cost function $J(\theta)$﻿

![Untitled 205.png](../../attachments/Untitled%20205.png)

- In practice, the learning rate has a decay function. It is big in the beginning, then small once it gets closer to the optimum.

![Untitled 1 168.png](../../attachments/Untitled%201%20168.png)

- Notice that the calculation of the gradient should be done before updating any parameter.
	- Calculate gradient w.r.t all parameters, then update all parameters

## 1.1 SGD

![Untitled 2 167.png](../../attachments/Untitled%202%20167.png)

- Randomly sample a center word (and its windows) or a batch of 32 center words, and calculate the gradient of the objective based on that.
	- This is much faster because we can get more updates, even if they are of lower quality
- In complex networks, SGD computes more quickly and does a better job
- In SGD, our gradient vectors will be very sparse, since the window we pick will only contain some words.

	![Untitled 3 164.png](../../attachments/Untitled%203%20164.png)

- We can computationally optimize this given that we know its a sparse matrix

	![Untitled 4 159.png](../../attachments/Untitled%204%20159.png)

# 2 Word2vec Algorithm Family

![Untitled 5 156.png](../../attachments/Untitled%205%20156.png)

## 2.1 Negative Sampling

- With the naive softmax, the summation on the bottom is very expensive because you have to enumerate through all of the words

![Untitled 6 154.png](../../attachments/Untitled%206%20154.png)

- Words from the same context will have higher similarity compared to words not in the same context, so we’ll train a binary classifier (log reg) to find these.

![Untitled 7 150.png](../../attachments/Untitled%207%20150.png)

- To use gradient descent packages, we turn this into a minimizing function with negatives.

![Untitled 8 141.png](../../attachments/Untitled%208%20141.png)

- Given the example “I really like Palo Alto”
	- A positive pair of words is “really” and “like”, because they appear in the example. You want to maximize the similarity of these words.
	- We also want to randomly sample some words that don’t appear in the example, e.g. “potato”. You want to minimize the similarity of the center word and this random sample.

# 3 Co-occurence

- Look through documents, count how many times two words appear in each other’s windows, which we can use to calculate their similarity.

![Untitled 9 137.png](../../attachments/Untitled%209%20137.png)

- Notice that “I” and “like” appear next to each other in two documents in the corpus, so their count is two.
- We can use the columns like word vectors, comparing their similarity that way
	- The column for `like` and `enjoy` are pretty similar.

## 3.1 Dimensionality

![Untitled 10 131.png](../../attachments/Untitled%2010%20131.png)

![Untitled 11 126.png](../../attachments/Untitled%2011%20126.png)

- Singular values are ordered from largest to smallest, so we can pick the top k singular values to get the best rank-k approximation.

![Untitled 12 123.png](../../attachments/Untitled%2012%20123.png)

# 4 GloVe

- The ratio of co-occurence probabilities can encode meaning components

	- Meaning components is like going from male to female, queen to king, etc

	![Untitled 13 114.png](../../attachments/Untitled%2013%20114.png)

	- Solid and water often co-occur with ice, so their probabilities are large.
	- If we take the ratio of these two probabilities, we can see the likeliness of a certain word co-occuring with one over the other.
- To do this, build a log-bilinear model such that the dot product between two word vectors approximated the log probability of the co-occurence

	![Untitled 14 101.png](../../attachments/Untitled%2014%20101.png)

- This loss function captures this setup. We want the dot product of word vectors to be similar to the log of the co-occurence, in addition with some bias terms.

	- The dot product of $w_i^T w_j$﻿, and the log co-occurence is $\log X_{ij}$﻿

	![Untitled 15 96.png](../../attachments/Untitled%2015%2096.png)

	- There’s also a weighting function $f$﻿ to prevent words that appear very often from overpowering the loss.

# 5 Evaluating Word Vectors

![Untitled 16 90.png](../../attachments/Untitled%2016%2090.png)

## 5.1 Intrinsic

- Test whether word analogies are correct to evaluate word vectors.

![Untitled 17 84.png](../../attachments/Untitled%2017%2084.png)

- The expected semantic answer would be something like man is to woman as king is to **queen**.
- This method can also be used to find the potential biases, such as searching for analogies of man and woman to nurse.

## 5.2 Extrinsic (named Entity recognition)

- Once example where good word vectors should help directly is **named entity recognition**
	- Identifying references to a person, organization or location: Chris Manning lives in Palo Alto.

![Untitled 18 76.png](../../attachments/Untitled%2018%2076.png)

# 6 Word Sense and Ambiguity

![Untitled 19 68.png](../../attachments/Untitled%2019%2068.png)

- Instead of having a single word vector for each word, we can have a word vector for each word sense (meaning of a word)

![Untitled 20 64.png](../../attachments/Untitled%2020%2064.png)

- Multiple instances of a single word, one for each definition of the word.

## 6.1 Linear Structures

- You can separate out the senses of a single word

![Untitled 21 58.png](../../attachments/Untitled%2021%2058.png)
