---
Week: Week 2
---
# Views of Linguistic Structure

- There are two primary views for the linguistic structure of sentences

## CFG

- The first view is constituency = phrase structure grammar = CFGs

![[attachments/Untitled 209.png|Untitled 209.png]]

- We essentially put words into phrases using grammar rules. Some examples include:
    - Noun phrase = determinant + noun
    - Prepositional phrase = preposition + noun phrase
- It has non terminals and terminals. Terminals are like the leaf nodes, and can be like determinants, nouns, propositions, etc. Nonterminals are phrases that are built from terminals, or even other nonterminals, etc.

## Dependency Structure

- Dependency structure shows which words depend on (modify, attach to, or are arguments of) which other words.
- Represent the words with some dependency, and you can build sentences from those dependencies.

![[attachments/Untitled 1 172.png|Untitled 1 172.png]]

- In the above, there are a lot of structures. Some examples are:
    - look is modified by crate, since crate is the subject of the verb.
    - crate is modified by in, the, and large, since those words describe it.
    - crate is also modified by kitchen, since that tells us where the crate is.
- Arrows point from the head to the words that modify it.

# Why do we need sentence structures?

![[attachments/Untitled 2 171.png|Untitled 2 171.png]]

## Prepositional phrase ambiguity

- There a lot of ambiguity in sentences due to **prepositional phrase attachment.** Examples of ambiguous sentences are:
    
    - Scientists count whales from space → not sure if from space is attached to whales or scientists.
    - San Jose police kills man with knife → not sure if with knife is attached to man or police
    
    ![[attachments/Untitled 3 168.png|Untitled 3 168.png]]
    

## Coordination scope ambiguity

![[attachments/Untitled 4 163.png|Untitled 4 163.png]]

- In the first example, “shuttle veteran” and “longtime NASA executive Fred Gregory” can be interpreted as two different people, and both are appointed to the board.
- In the second example, the two terms are modifies for Fred Gregory, and it refers to just one person being appointed to the board.

# Dependency Paths

![[attachments/Untitled 5 160.png|Untitled 5 160.png]]

# Depedency Grammar and Structure

- Dependency syntax postulates that syntactic structure consists of **relations between lexical items**, normally binary asymmetric relations (“**arrows**”) called dependencies
- The arrows are typed with the name of the grammatical relation
    
    ![[attachments/Untitled 6 158.png|Untitled 6 158.png]]
    
- The arrows connect a head lexical item with a dependent lexical item.
    
    ![[attachments/Untitled 7 154.png|Untitled 7 154.png]]
    
- Below is an example of a dependency structure. Notice the fake ROOT
    
    ![[attachments/Untitled 8 145.png|Untitled 8 145.png]]
    

## Universal Dependencies treebanks

- Pieces of annotated data that can be used to train future models.

![[attachments/Untitled 9 141.png|Untitled 9 141.png]]

- Building the treebank will be slower and less useful than writing a grammar (in the beginning). However, it gives us:
    - Reusabiliity of the labor: many parsers, models, etc, can be built on it. It’s a valuable resource for linguistics.
    - It has broad coverage of many other languages, built by many institutions
    - It’s a good way to evaluate and benchmark NLP systems

## Algorithm

- To parse a sentence: for each word, choose other words (including ROOT) that it is dependent on

![[attachments/Untitled 10 135.png|Untitled 10 135.png]]

## Projectivity

![[attachments/Untitled 11 130.png|Untitled 11 130.png]]

- It’s hard to come up with algorithms to parse it fast and automatically when there are overlapping dependency arcs.

# Methods of Dependency Parsing

![[attachments/Untitled 12 127.png|Untitled 12 127.png]]

## Greedy transition-based

![[attachments/Untitled 13 118.png|Untitled 13 118.png]]

- Shift operations mean we just put that word from the buffer into the stack.
    
    ![[attachments/Untitled 14 105.png|Untitled 14 105.png]]
    
- Once you have “I” and “ate” in your stack, you can make a left arc there, since “ate” depends on “I”. Once that arc is made, you can remove “I” from the stack and add the arc to $A$﻿
    
    ![[attachments/Untitled 15 100.png|Untitled 15 100.png]]
    
- We can repeat this with “fish” and “ate”
    
    ![[attachments/Untitled 16 94.png|Untitled 16 94.png]]
    
- Therefore, our final arcs are **A = { nsubj(ate → I), obj(ate → fish), root([root] → ate) }**
- One issue with greedy is that at every step, we’re making the correct decision (or finding the dependency) in a supervised way. The algorithm should be able to figure this out.

## Malt Parser

- This is very similar to greedy, but we decide the action (shift, right arc, left arc) using a discriminative classifier.

![[attachments/Untitled 17 88.png|Untitled 17 88.png]]

- There are a lot of features that we can use, through feature engineering, to figure out the best decision to make when classifying.

# Feature Representation

- We need to somehow represent the words as features, so that we can use them to learn and do machine learning with them.

# Evaluation of Depedency Parsing

- To evaluate it, we can compare the arcs that are createed and the ground truth.

![[attachments/Untitled 18 80.png|Untitled 18 80.png]]

- UAS is whether the depedency arc is correct (head and depdency matches for words)
- LAS is whether the depedency arc is correct **and** the relation is correct (e.g. nsubj, root, etc)

# Neural Dependency Parser

![[attachments/Untitled 19 72.png|Untitled 19 72.png]]

## Representing words as vectors

- Using this, we can also represent words as word embeddings, similar to word2vec

![[attachments/Untitled 20 68.png|Untitled 20 68.png]]

- When building phrases, we use the word2vec relations and simply add them up.

![[attachments/Untitled 21 62.png|Untitled 21 62.png]]

## Nonlinear classifiers

![[attachments/Untitled 22 57.png|Untitled 22 57.png]]

## Architecture

![[attachments/Untitled 23 53.png|Untitled 23 53.png]]

- The first thing to do is to create a dense feature representation of the data.
    - The input layer is the concatenation of different kinds of embeddings.
- Then we just pass it through hidden layers, and then we use softmax for the output layer.
    - Note that this is multi-class classification, classifying what action to take

# Graph-based dependency parsers

![[attachments/Untitled 24 49.png|Untitled 24 49.png]]

- We represent the parsing as a tree, where edges have weights
- We can use tree search algorithms, such as the MST to find the best parse