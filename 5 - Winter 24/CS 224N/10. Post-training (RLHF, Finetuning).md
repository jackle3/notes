---
Week: Week 5
---
- This is how ChatGPT and modern user-facing systems are built today.
- The goal is to turn **language models into assistants** → do any language-related tasks

# Benchmarks for multitask LMs: MMLU

- Stands for Massive Multitask Language Understanding
    - This is a benchmark for multitask LMs → measures performance on 56 diverse knowledge-intensive tasks
    - The benchmark asks specific multiple-choice questions in each of these domains.

  

![Untitled 210.png](attachments/Untitled%20210.png)

  

- There has been rapid, impressive progress on knowledge-intensive benchmarks.
    
    ![Untitled 1 173.png](attachments/Untitled%201%20173.png)
    

# Instruction finetuning

- A different idea than finetuning.
    - Before, we are finetuning for certain tasks, such as sentiment analysis.
    - Instruction finetuning is training a model to do many things → more than one task.
- As it is now, language models are not very good assistants.
    
    - Language models like to repeat things. In this example below, given the prompt, it just repeats the prompt using different wording
    - It does not align with the intent of the user’s prompt.
    
    ![Untitled 2 172.png](attachments/Untitled%202%20172.png)
    

## Scaling up finetuning

- Before, we finetuned on a specific task. Now, since we are building an assistant, we finetune on many tasks at once.

![Untitled 3 169.png](attachments/Untitled%203%20169.png)

## Unseen tasks

- The goal is to generalize from seen tasks to unseen tasks.

![Untitled 4 164.png](attachments/Untitled%204%20164.png)

## Advantages

- Instruction finetuned models are significantly outperforming regular models on benchmarks.

![Untitled 5 161.png](attachments/Untitled%205%20161.png)

## Data

- You can generate data synthetically (from bigger LMs)
- You don’t need many samples to instruction tune → only need about a few thousand examples to generalize to unseen tasks.
- Crowdsourcing can be pretty effective!

## Limitations of instruction finetuning

- We don’t want the language model to make catastrophic mistakes.

![Untitled 6 159.png](attachments/Untitled%206%20159.png)

## Summary

![Untitled 7 155.png](attachments/Untitled%207%20155.png)

# RLHF

- Stands for Reinforcement Learning from Human Feedback
    - Make the model behave even more nicely baesd on human feedback
- We do this step after instruction finetuning → further refine our model
- This is helpful for tasks where there is no right answer.
    - In language modeling, there is a reference sequence that we try to match.
    - In RLHF, we score the outputs, and try to maximize the score (reward).

![Untitled 8 146.png](attachments/Untitled%208%20146.png)

## Pipeline

![Untitled 9 142.png](attachments/Untitled%209%20142.png)

## Policy gradients

- The language model is $p_\theta$﻿, the model that is randomly sampling and generating $\hat{s}$﻿

![Untitled 10 136.png](attachments/Untitled%2010%20136.png)

- We do some tricks so that instead of taking the gradient of the expectation, we take the expectation of a gradient.

![Untitled 11 131.png](attachments/Untitled%2011%20131.png)

![Untitled 12 128.png](attachments/Untitled%2012%20128.png)

- The issue with this is that most sequences are pretty bad, giving you no reward.
    - For instance, sequences that not grammatical, random, etc, have $R(s) = 0$﻿
    - Because there is no reward, we would make no updates in our policy gradient.
- For policy gradient estimators to work, we need sequences with high magnitude rewards.

## Reward: model human preferences

- For each prompt, have the language model generate a ton of output sequences.
    - Then, have humans label these outputs and rank order them.
    - Then, train a model over these labeled examples, training it to distinguish good from bad outputs.
    - Then, do reinforcement learning against that reward model.

![Untitled 13 119.png](attachments/Untitled%2013%20119.png)

![Untitled 14 106.png](attachments/Untitled%2014%20106.png)

- We have to **make sure the reward model works first**

## Final algorithm

![Untitled 15 101.png](attachments/Untitled%2015%20101.png)

- We want to maximize our reinforcement-learning model, but still keep it close to the pretrained model → otherwise we might overly optimize objective.

![Untitled 16 95.png](attachments/Untitled%2016%2095.png)

- IFT stands for instruction fine tuning.

## Summary

![Untitled 17 89.png](attachments/Untitled%2017%2089.png)

# Limtations of RL + Reward Modeling

- Human preferences are unreliable and ”reward hacking” is a common problem in RL.
    - It can hack to maximize the reward, while failing at the intended task
- Chatbots are rewarded for responses that seem authoritative and helpful, regardless of truth
    - This can result in making up facts and hallucinations
- Reward models of human preferences are even more unreliable! It might not fully capture the actual human preference if they are complex.
    
    - Could lead to over optimization of the reward model.
    
    ![Untitled 18 81.png](attachments/Untitled%2018%2081.png)
    

# DPO

- This is much more simple than reinforcement learning.
- We still take gradient steps, just like policy gradients.
    - The first gradient is the weighting term → higher weight when the reward estimate is wrong.
    - The next two gradients are for the examples itself.
        - Upweight the example that we rated as good, $y_w$﻿
        - Downweight the example that we rated as bad, $y_t$﻿
    - Subtract gradient of bad stuff, add gradient of good stuff, all under a weight.

![Untitled 19 73.png](attachments/Untitled%2019%2073.png)

- Open source LLMs now almost all just use DPO (and it works well!)