---
Week: Week 5
---
- This is how ChatGPT and modern user-facing systems are built today.
- The goal is to turn **language models into assistants** → do any language-related tasks

# Benchmarks for multitask LMs: MMLU

- Stands for Massive Multitask Language Understanding
    - This is a benchmark for multitask LMs → measures performance on 56 diverse knowledge-intensive tasks
    - The benchmark asks specific multiple-choice questions in each of these domains.

  

![[attachments/Untitled 210.png|Untitled 210.png]]

  

- There has been rapid, impressive progress on knowledge-intensive benchmarks.
    
    ![[attachments/Untitled 1 173.png|Untitled 1 173.png]]
    

# Instruction finetuning

- A different idea than finetuning.
    - Before, we are finetuning for certain tasks, such as sentiment analysis.
    - Instruction finetuning is training a model to do many things → more than one task.
- As it is now, language models are not very good assistants.
    
    - Language models like to repeat things. In this example below, given the prompt, it just repeats the prompt using different wording
    - It does not align with the intent of the user’s prompt.
    
    ![[attachments/Untitled 2 172.png|Untitled 2 172.png]]
    

## Scaling up finetuning

- Before, we finetuned on a specific task. Now, since we are building an assistant, we finetune on many tasks at once.

![[attachments/Untitled 3 169.png|Untitled 3 169.png]]

## Unseen tasks

- The goal is to generalize from seen tasks to unseen tasks.

![[attachments/Untitled 4 164.png|Untitled 4 164.png]]

## Advantages

- Instruction finetuned models are significantly outperforming regular models on benchmarks.

![[attachments/Untitled 5 161.png|Untitled 5 161.png]]

## Data

- You can generate data synthetically (from bigger LMs)
- You don’t need many samples to instruction tune → only need about a few thousand examples to generalize to unseen tasks.
- Crowdsourcing can be pretty effective!

## Limitations of instruction finetuning

- We don’t want the language model to make catastrophic mistakes.

![[attachments/Untitled 6 159.png|Untitled 6 159.png]]

## Summary

![[attachments/Untitled 7 155.png|Untitled 7 155.png]]

# RLHF

- Stands for Reinforcement Learning from Human Feedback
    - Make the model behave even more nicely baesd on human feedback
- We do this step after instruction finetuning → further refine our model
- This is helpful for tasks where there is no right answer.
    - In language modeling, there is a reference sequence that we try to match.
    - In RLHF, we score the outputs, and try to maximize the score (reward).

![[attachments/Untitled 8 146.png|Untitled 8 146.png]]

## Pipeline

![[attachments/Untitled 9 142.png|Untitled 9 142.png]]

## Policy gradients

- The language model is $p_\theta$﻿, the model that is randomly sampling and generating $\hat{s}$﻿

![[attachments/Untitled 10 136.png|Untitled 10 136.png]]

- We do some tricks so that instead of taking the gradient of the expectation, we take the expectation of a gradient.

![[attachments/Untitled 11 131.png|Untitled 11 131.png]]

![[attachments/Untitled 12 128.png|Untitled 12 128.png]]

- The issue with this is that most sequences are pretty bad, giving you no reward.
    - For instance, sequences that not grammatical, random, etc, have $R(s) = 0$﻿
    - Because there is no reward, we would make no updates in our policy gradient.
- For policy gradient estimators to work, we need sequences with high magnitude rewards.

## Reward: model human preferences

- For each prompt, have the language model generate a ton of output sequences.
    - Then, have humans label these outputs and rank order them.
    - Then, train a model over these labeled examples, training it to distinguish good from bad outputs.
    - Then, do reinforcement learning against that reward model.

![[attachments/Untitled 13 119.png|Untitled 13 119.png]]

![[attachments/Untitled 14 106.png|Untitled 14 106.png]]

- We have to **make sure the reward model works first**

## Final algorithm

![[attachments/Untitled 15 101.png|Untitled 15 101.png]]

- We want to maximize our reinforcement-learning model, but still keep it close to the pretrained model → otherwise we might overly optimize objective.

![[attachments/Untitled 16 95.png|Untitled 16 95.png]]

- IFT stands for instruction fine tuning.

## Summary

![[attachments/Untitled 17 89.png|Untitled 17 89.png]]

# Limtations of RL + Reward Modeling

- Human preferences are unreliable and ”reward hacking” is a common problem in RL.
    - It can hack to maximize the reward, while failing at the intended task
- Chatbots are rewarded for responses that seem authoritative and helpful, regardless of truth
    - This can result in making up facts and hallucinations
- Reward models of human preferences are even more unreliable! It might not fully capture the actual human preference if they are complex.
    
    - Could lead to over optimization of the reward model.
    
    ![[attachments/Untitled 18 81.png|Untitled 18 81.png]]
    

# DPO

- This is much more simple than reinforcement learning.
- We still take gradient steps, just like policy gradients.
    - The first gradient is the weighting term → higher weight when the reward estimate is wrong.
    - The next two gradients are for the examples itself.
        - Upweight the example that we rated as good, $y_w$﻿
        - Downweight the example that we rated as bad, $y_t$﻿
    - Subtract gradient of bad stuff, add gradient of good stuff, all under a weight.

![[attachments/Untitled 19 73.png|Untitled 19 73.png]]

- Open source LLMs now almost all just use DPO (and it works well!)