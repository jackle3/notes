---
Week: Week 3
---
# Problems with RNNs

- The biggest problem is vanishing and exploding gradients.

## Vanishing gradients

![[attachments/Untitled 199.png|Untitled 199.png]]

- **Vanishing gradient** occurs when gradients are successively small, which causes us to basically kill the effects of the past.

### Proof (linear case)

- For simplexity, suppose the non-linearity wasn’t there
    
    ![[attachments/Untitled 1 162.png|Untitled 1 162.png]]
    
- Now consider the loss of step $i$﻿ with the hidden state at step $j$﻿. Let $l = i - j$﻿
    
    ![[attachments/Untitled 2 161.png|Untitled 2 161.png]]
    
    - The first step is just the chain rule. However, recall that the derivative of hidden state $t$﻿ with respect to the previous hidden state is just $W_h$﻿.
- We see that $W_h$﻿ acts exponentially in the gradient. This can be bad if the exponential is big, and $W_h$﻿ is small.
    
    ![[attachments/Untitled 3 158.png|Untitled 3 158.png]]
    

### Why is this a problem?

- The gradient signal closest to the loss are dominant, and the signal from far away is lost.
- Consider what happens if we focus at $h^{(1)}$﻿
    
    ![[attachments/Untitled 4 153.png|Untitled 4 153.png]]
    
- This issue of vanishing gradients means that in reality, RNNs only condition on near tokens, even if input sequences can have infinite length.
    
    ![[attachments/Untitled 5 150.png|Untitled 5 150.png]]
    

### Fix: memory

- Instead of rewriting hidden states, design an RNN with a separate memory box that can be written to.
- Basically, instead of constantly updating, update as we need.

![[attachments/Untitled 6 148.png|Untitled 6 148.png]]

- This idea is where **LSTMs** come from.

## Exploding gradient

- This happens when the gradient becomes too big.
- If we refer back to the gradient equation calculated previously.
    
    ![[attachments/Untitled 7 144.png|Untitled 7 144.png]]
    
    - If $W_h$﻿ is big or greater than one, the gradient grows exponentially as $l$﻿ increases.

![[attachments/Untitled 8 135.png|Untitled 8 135.png]]

### Fix: gradient clipping

- To fix exploding gradients, we can simply just clip the calculated gradients.

![[attachments/Untitled 9 131.png|Untitled 9 131.png]]

- This prevents exploding gradients, but the fact that we have to clip still means we have some optimization difficulties.

## Summary

- Vanishing gradients make it so that the RNNs can’t look infinitely back. In practice it only looks ~7 tokens back.
- Exploding gradients can lead to Inf or NaN because it can cause gradient descent to massively overshoot, possibly leading to overflows.
- However, RNNs still fixes the issue of sparse matrices.

# LSTMs

- The name is short for Long Short-Term Memory RNNs
    - The RNN is a model of human short-term memory. It makes new memory by replacing hidden state with stuff about previous states.
    - The name LSTM comes from the fact that this is still short-term memory, but it’s longer than the traditional RNN.
- It’s a type of RNN proposed as a **solution to the problem of vanishing gradients**

## Architecture

- It separates the hidden state from RNNs into a hidden state and a cell state.

![[attachments/Untitled 10 125.png|Untitled 10 125.png]]

- The cell state is not always going to be written to. It stores the important information that you may need in the future.
- The hidden state will always be written to and read from. It’s used as the control flow of the LSTM.

![[attachments/Untitled 11 120.png|Untitled 11 120.png]]

- All of the gate vectors and the cell state and hidden states are vectors of length $n$﻿

## Gates

- There are three types of gates in an LSTM. They are functions of the hidden state and input.

![[attachments/Untitled 12 117.png|Untitled 12 117.png]]

- Each of the gates control information flow.
    - The forget gate controls how much of the prevous cell state remains.
        - Allows you to throw away irrelevant information.
    - The input gate controls what kind of information is written into the cell.
        - Controls how much goes into the cell.
    - The output gate controls what part of the cell goes into the hidden state.
        - Controls how much goes out of the cell.
- The sigmoid gives a number between zero and one.
    - Zero means forget nothing.
    - One means forget everything.

## Update Equations

![[attachments/Untitled 13 108.png|Untitled 13 108.png]]

- The **new cell content** is similar to traditional RNNS.
    - Take weighted input at time step $t$﻿ and weighted hidden state at time step $t - 1$﻿
    - It’s writing these to $\tilde{c}$﻿, which is like a temporary proposed update to cell state
- The **cell state** is choosing which of new cell content is saved to the cell state
    - Uses forget gate to keep some parts of the previous cell state $c^{(t-1)}$﻿
    - Uses input gate to keep some parts of new cell content $\tilde{c}^{(t)}$﻿
- The **hidden state** is created from the cell state, using the output gate to determine how.
- **Note:** the cell serves as a memory. The output itself is the hidden state.
    - The cell state is the main thing passed across time.

## Visualization

![[attachments/Untitled 14 95.png|Untitled 14 95.png]]

![[attachments/Untitled 15 91.png|Untitled 15 91.png]]

- The + where we write in some new cell content is the key to fixing vanishing gradients.

## How does it solve vanishing gradients?

![[attachments/Untitled 16 85.png|Untitled 16 85.png]]

# More about vanishing gradients

![[attachments/Untitled 17 79.png|Untitled 17 79.png]]

- This skip-connection basically directly add the input to the output, so that it learns better.

![[attachments/Untitled 18 72.png|Untitled 18 72.png]]

- Basically, the generic solution to vanishing gradients is to add gates or skip connections to allow gradients to flow easier from input to output.

# Applications of RNNs

- There are many more applications of RNNs beyond language modeling and generating text.

## Sequence tagging

- In this task, you try to tag the part-of-speech (e.g. verbs, nouns, etc) of each token in a sequence.

![[attachments/Untitled 19 64.png|Untitled 19 64.png]]

- Instead of generating the next word, the RNN generates the part-of-speech tag associated with the input word.

## Sentiment classification

- In this task, you try to generate and figure out whether a sentence is positive of negative.
    - Try to figure out the **sentiment** of the sequence.

![[attachments/Untitled 20 60.png|Untitled 20 60.png]]

- The reason we do this way instead of taking **the max of the last hidden state** is because the last hidden state might be missing information from earlier time steps.
    - Allows us to get around the problem of vanishing gradients and long-range dependence.

## Generate text using other information

- RNNs can also take in information like audio, images, etc

![[attachments/Untitled 21 54.png|Untitled 21 54.png]]

- We first use an audio encoder model to turn the audio into a vector. Using that vector, we feed that into our RNN as the first hidden state.
- From here, just run a normal language model and generate the transcription of the audio.

# Bidirectional RNNs

- Traditional RNNs are structured from left to right. The current hidden state only has information about the states to the left of it.

![[attachments/Untitled 22 50.png|Untitled 22 50.png]]

- The bidirectional RNN has a backwards RNN and a forwards RNN.
    - Have one RNN going left to right and generating hidden states.
    - Have another RNN going right to left and generating hidden states.

![[attachments/Untitled 23 46.png|Untitled 23 46.png]]

![[attachments/Untitled 24 42.png|Untitled 24 42.png]]

![[attachments/Untitled 25 37.png|Untitled 25 37.png]]

- Bidirectional RNNs can’t really be used to generate text, because it would require it to know the future in order to get the backwards steps.
    - If you’re generating text, you have to use a unidirectional RNN.
    - If you’re not generating text, use a bidirectional because its better.

![[attachments/Untitled 26 32.png|Untitled 26 32.png]]

- We do both directions because of the issues with long-range dependencies. Even though the last hidden state should technically contain all the information, in practice that’s not the case.

# Multi-layer RNNs

![[attachments/Untitled 27 28.png|Untitled 27 28.png]]

- We essentially just vertically stack the RNNs, where each acts as a layer.
    - Each layer computes different levels of features.

![[attachments/Untitled 28 26.png|Untitled 28 26.png]]

- The hidden states from RNN layer $i$﻿ is the inputs to RNN layer $i + 1$﻿

![[attachments/Untitled 29 25.png|Untitled 29 25.png]]

# Machine Translation

![[attachments/Untitled 30 25.png|Untitled 30 25.png]]

## Statistical Machine Translation

![[attachments/Untitled 31 21.png|Untitled 31 21.png]]

- Output the most likely translated sentence $y$﻿ given the original sentence $x$﻿
    - Instead of going from $x$﻿ to $y$﻿, use Bayes rule to reverse the direction.
- Noisy channel model: find the sequence with the highest score, where score is the product between “**translating from** $y$﻿ **to** $x$﻿” and “**how good** $x$﻿ **is”**
    - This reverse direction is good because we can use a language model, which is easier to learn.

## Neural Machine Translation

- Neural Machine Translation (NMT) is a way to do Machine Translation with **a single end-to-end neural network**
- The neural network architecture is called a sequence-to-sequence model (aka seq2seq) and it involves **two RNNs**

![[attachments/Untitled 32 21.png|Untitled 32 21.png]]

- The encoder RNN outputs a single vector, representing the meaning of the source sentence.
- The decoder RNN is a language model that
    1. takes the hidden state encoding from the encoder RNN → $h^{(0)}$﻿ of decoder RNN is the one given by the encoder RNN (such as last hidden state, etc)
    2. uses it to generate a sentence in the target language

### Advantages of sequence-to-sequence

![[attachments/Untitled 33 20.png|Untitled 33 20.png]]

### Conditional Language Model

![[attachments/Untitled 34 16.png|Untitled 34 16.png]]

- Instead of calculating the reverse direction $P(x | y) * P(y)$﻿, this directly calculates $P(y | x)$﻿
- A parallel corpus is a dataset where the same text has been written in both the source and the target languages. This can be used as the ground truth.

### Training

- Since it’s a single end-to-end neural network, the loss from the decoder can backpropagate to the encoder to train both.

![[attachments/Untitled 35 14.png|Untitled 35 14.png]]

- The $\hat{y}$﻿ is the distribution over the prediction of what the next word is.
    - If you were generating the next word, you would sample this $\hat{y}$﻿
    - In this case, you can compare the $\hat{y}$﻿ with the ground truth (the word at the next time step at the bottom) to get the loss.

### Multi-layer

![[attachments/Untitled 36 13.png|Untitled 36 13.png]]

- This was the state of the art structure for NMTs back in 2014 and 2015.

# Summary

![[attachments/Untitled 37 13.png|Untitled 37 13.png]]