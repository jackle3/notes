---
Week: Week 6
---
# Emergent abilities of LLMs

- **GPT** showed that language modeling at scale can be an effective pretraining technique for downstream tasks like natural language inference.
- **GPT-2** showed that using more parameters and more data significantly improves the performance of models.

## Zero-shot learning

- One key emergent ability in GPT-2 is zero-shot learning: the ability to do many tasks with no examples, and no gradient updates, by simply using natural language.
    
    ![[attachments/Untitled 208.png|Untitled 208.png]]
    
- GPT-2 beats SoTA on language modeling benchmarks with **no task-specific fine-tuning**

## Few-shot learning

- This is an example of **prompting**. We simply prepend examples and prompt the model to complete the task.
    - No gradient updates or model-based learning is done here. It is simply done while generating the output.

![[attachments/Untitled 1 171.png|Untitled 1 171.png]]

- As the size of the model (in terms of parameters) increases, it gets better at few-shot.

# Prompt engineering

- In traditional fine-tuning, you take each example and use it to make a gradient update.
- With prompting, we simply give all of the examples, and the model does not do any gradient updates to make its prediction.

![[attachments/Untitled 2 170.png|Untitled 2 170.png]]

## Limits for prompting: hard tasks

![[attachments/Untitled 3 167.png|Untitled 3 167.png]]

## Chain-of-throught prompting

- To make the model perform better, we do some prompt engineering to give the model a better chance.
- In this case, we teach the model to include the **step-by-step reasoning** that it used to get to the correct answer within its response.
    
    ![[attachments/Untitled 4 162.png|Untitled 4 162.png]]
    
- To do this, just ask the model to reason through things.
    
    - For instance, include **“Let’s think step by step”** in the prompt.
    
    ![[attachments/Untitled 5 159.png|Untitled 5 159.png]]
    
- As seen below, chain-of-thought greatly increases model performance.
    
    ![[attachments/Untitled 6 157.png|Untitled 6 157.png]]
    
- There are many prompts we can use to trigger chain-of-thought reasoning.
    
    ![[attachments/Untitled 7 153.png|Untitled 7 153.png]]
    

## Downside of prompt-based learning

![[attachments/Untitled 8 144.png|Untitled 8 144.png]]

# Why efficient adaptation?

![[attachments/Untitled 9 140.png|Untitled 9 140.png]]

- Training large language models use up a large amount of energy and money.

![[attachments/Untitled 10 134.png|Untitled 10 134.png]]

# PEFT

- Stands for **parameter-efficient** fine-tuning.

![[attachments/Untitled 11 129.png|Untitled 11 129.png]]

- There are three main places where we can make our models more efficient.
    
    - Is it possible to make our parameter size smaller?
    - Is it possible to change our input to make our models more efficient?
    - Is it possible to change functions in our transformer architecture to make our models more efficient?
    
    ![[attachments/Untitled 12 126.png|Untitled 12 126.png]]
    

# Parameter perspective

- There are two ways to do this with respect to parameters.
    - The first is sparse subnetworks, or pruning.
    - The second is low-rank composition.

## Sparse Subnetworks

- Not all model parameters are important.
    - Use some sparsity method like pruning to remove parameters that are not important.
- We want to achieve sparsity in our network parameters. We leverage sparsity to make our networks more efficient.

![[attachments/Untitled 13 117.png|Untitled 13 117.png]]

### Pruning

- Pruning is task-specific. We repeatedly train and prune low-magnitude weights.
    - Smaller weights don’t carry a lot of information. Therefore, remove them.

![[attachments/Untitled 14 104.png|Untitled 14 104.png]]

### Binary mask

![[attachments/Untitled 15 99.png|Untitled 15 99.png]]

### Winning subnetworks

![[attachments/Untitled 16 93.png|Untitled 16 93.png]]

### Pruning pre-trained models

- In the fine-tuning process, if certain parameters move a lot, that might indicate that those regions are more important for my task.
    - In the visualization below, parameters that are gray are removed.

![[attachments/Untitled 17 87.png|Untitled 17 87.png]]

## Low-rank composition (LORA)

- Recall that in full fine-tuning, we update our pre-trained parameters to find the best parameters for our downstream task.
    
    ![[attachments/Untitled 18 79.png|Untitled 18 79.png]]
    

### LoRA

- **LoRA: low rank adaptation →** use a smaller set of parameters to encode the change in our task-specific parameters.
    
    ![[attachments/Untitled 19 71.png|Untitled 19 71.png]]
    
    - $W_o$﻿ is our old parameters, the pretrained weight matrix.
    - We want to approximate the change between our old parameters and our new parameters after pretraining, $\Delta W$﻿
        - We do so using two low-rank matrices, $BA$﻿
    
    ![[attachments/Untitled 20 67.png|Untitled 20 67.png]]
    
- We basically freeze our pretrained weights, and we only learn our matrices $A$﻿ and $B$﻿.

### Multiple downstream tasks

- $r$﻿ is a parameter that defines how many trainable parameters there are.
    - As we increase $r$﻿, then number of trainable parameters goes up, and training LoRA converges to training the original model.
- If we switch from one task to another, we can easily recover the original weights $W_o$﻿ by subtracting $BA$﻿ and adding a different $B’A’$﻿, that is associated with the new task.

### Results compared to finetuning

- In the table below, FT stands for full fine-tuning. Notice the difference in the number of trainable parameters.

![[attachments/Untitled 21 61.png|Untitled 21 61.png]]

### Where to apply LoRA?

- There are many weight matrices within the transformer architecture.
    - Often LoRA is applied to the weight matrices in the self-attention module (query, key, and value)

![[attachments/Untitled 22 56.png|Untitled 22 56.png]]

# Input perspective

- This case is very similar to prompt tuning. We **prefix** parameters to the input of the transformer.
    - The **prefix** is a low dimensional vector, which we can view as virtual words.

![[attachments/Untitled 23 52.png|Untitled 23 52.png]]

- During training, we freeze the model and just learn the prefixes.
    - In this case, this prefix is only applied to the **input-layer**

![[attachments/Untitled 24 48.png|Untitled 24 48.png]]

## Multi-layer prompt tuning

![[attachments/Untitled 25 43.png|Untitled 25 43.png]]

## Scale

![[attachments/Untitled 26 38.png|Untitled 26 38.png]]

# Functional perspective (adapters)

- In this perspective, we specifically change the model’s functions with respect to the task.

![[attachments/Untitled 27 34.png|Untitled 27 34.png]]

## Adapter

- Insert a mini-network between layers of a pretrained model. The goal of these layers is to help the model adapt to downstream tasks.
- The input of the **adapter** is the hidden states.

![[attachments/Untitled 28 32.png|Untitled 28 32.png]]

![[attachments/Untitled 29 31.png|Untitled 29 31.png]]

- Adapter based tuning attains a similar performance to full finetuning with **two orders of magnitude fewer** trained parameters

## Task knowledge

- We can use the adapter to learn useful information about a task or a language.

![[attachments/Untitled 30 31.png|Untitled 30 31.png]]

- You can then use the adapter and plug it into other tasks or languages.

![[attachments/Untitled 31 27.png|Untitled 31 27.png]]

## Rescaling

![[attachments/Untitled 32 27.png|Untitled 32 27.png]]

## Parameter Generalization

- Instead of learning module parameters for every task from scratch, use a small neural network to directly generate the module parameters.

![[attachments/Untitled 33 26.png|Untitled 33 26.png]]

![[attachments/Untitled 34 21.png|Untitled 34 21.png]]

# Summary

![[attachments/Untitled 35 19.png|Untitled 35 19.png]]

![[attachments/Untitled 36 18.png|Untitled 36 18.png]]