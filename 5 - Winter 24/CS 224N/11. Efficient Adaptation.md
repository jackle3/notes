---
Week: Week 6
---
# Emergent abilities of LLMs

- **GPT** showed that language modeling at scale can be an effective pretraining technique for downstream tasks like natural language inference.
- **GPT-2** showed that using more parameters and more data significantly improves the performance of models.

## Zero-shot learning

- One key emergent ability in GPT-2 is zero-shot learning: the ability to do many tasks with no examples, and no gradient updates, by simply using natural language.
    
    ![Untitled 208.png](../../attachments/Untitled%20208.png)
    
- GPT-2 beats SoTA on language modeling benchmarks with **no task-specific fine-tuning**

## Few-shot learning

- This is an example of **prompting**. We simply prepend examples and prompt the model to complete the task.
    - No gradient updates or model-based learning is done here. It is simply done while generating the output.

![Untitled 1 171.png](../../attachments/Untitled%201%20171.png)

- As the size of the model (in terms of parameters) increases, it gets better at few-shot.

# Prompt engineering

- In traditional fine-tuning, you take each example and use it to make a gradient update.
- With prompting, we simply give all of the examples, and the model does not do any gradient updates to make its prediction.

![Untitled 2 170.png](../../attachments/Untitled%202%20170.png)

## Limits for prompting: hard tasks

![Untitled 3 167.png](../../attachments/Untitled%203%20167.png)

## Chain-of-throught prompting

- To make the model perform better, we do some prompt engineering to give the model a better chance.
- In this case, we teach the model to include the **step-by-step reasoning** that it used to get to the correct answer within its response.
    
    ![Untitled 4 162.png](../../attachments/Untitled%204%20162.png)
    
- To do this, just ask the model to reason through things.
    
    - For instance, include **“Let’s think step by step”** in the prompt.
    
    ![Untitled 5 159.png](../../attachments/Untitled%205%20159.png)
    
- As seen below, chain-of-thought greatly increases model performance.
    
    ![Untitled 6 157.png](../../attachments/Untitled%206%20157.png)
    
- There are many prompts we can use to trigger chain-of-thought reasoning.
    
    ![Untitled 7 153.png](../../attachments/Untitled%207%20153.png)
    

## Downside of prompt-based learning

![Untitled 8 144.png](../../attachments/Untitled%208%20144.png)

# Why efficient adaptation?

![Untitled 9 140.png](../../attachments/Untitled%209%20140.png)

- Training large language models use up a large amount of energy and money.

![Untitled 10 134.png](../../attachments/Untitled%2010%20134.png)

# PEFT

- Stands for **parameter-efficient** fine-tuning.

![Untitled 11 129.png](../../attachments/Untitled%2011%20129.png)

- There are three main places where we can make our models more efficient.
    
    - Is it possible to make our parameter size smaller?
    - Is it possible to change our input to make our models more efficient?
    - Is it possible to change functions in our transformer architecture to make our models more efficient?
    
    ![Untitled 12 126.png](../../attachments/Untitled%2012%20126.png)
    

# Parameter perspective

- There are two ways to do this with respect to parameters.
    - The first is sparse subnetworks, or pruning.
    - The second is low-rank composition.

## Sparse Subnetworks

- Not all model parameters are important.
    - Use some sparsity method like pruning to remove parameters that are not important.
- We want to achieve sparsity in our network parameters. We leverage sparsity to make our networks more efficient.

![Untitled 13 117.png](../../attachments/Untitled%2013%20117.png)

### Pruning

- Pruning is task-specific. We repeatedly train and prune low-magnitude weights.
    - Smaller weights don’t carry a lot of information. Therefore, remove them.

![Untitled 14 104.png](../../attachments/Untitled%2014%20104.png)

### Binary mask

![Untitled 15 99.png](../../attachments/Untitled%2015%2099.png)

### Winning subnetworks

![Untitled 16 93.png](../../attachments/Untitled%2016%2093.png)

### Pruning pre-trained models

- In the fine-tuning process, if certain parameters move a lot, that might indicate that those regions are more important for my task.
    - In the visualization below, parameters that are gray are removed.

![Untitled 17 87.png](../../attachments/Untitled%2017%2087.png)

## Low-rank composition (LORA)

- Recall that in full fine-tuning, we update our pre-trained parameters to find the best parameters for our downstream task.
    
    ![Untitled 18 79.png](../../attachments/Untitled%2018%2079.png)
    

### LoRA

- **LoRA: low rank adaptation →** use a smaller set of parameters to encode the change in our task-specific parameters.
    
    ![Untitled 19 71.png](../../attachments/Untitled%2019%2071.png)
    
    - $W_o$﻿ is our old parameters, the pretrained weight matrix.
    - We want to approximate the change between our old parameters and our new parameters after pretraining, $\Delta W$﻿
        - We do so using two low-rank matrices, $BA$﻿
    
    ![Untitled 20 67.png](../../attachments/Untitled%2020%2067.png)
    
- We basically freeze our pretrained weights, and we only learn our matrices $A$﻿ and $B$﻿.

### Multiple downstream tasks

- $r$﻿ is a parameter that defines how many trainable parameters there are.
    - As we increase $r$﻿, then number of trainable parameters goes up, and training LoRA converges to training the original model.
- If we switch from one task to another, we can easily recover the original weights $W_o$﻿ by subtracting $BA$﻿ and adding a different $B’A’$﻿, that is associated with the new task.

### Results compared to finetuning

- In the table below, FT stands for full fine-tuning. Notice the difference in the number of trainable parameters.

![Untitled 21 61.png](../../attachments/Untitled%2021%2061.png)

### Where to apply LoRA?

- There are many weight matrices within the transformer architecture.
    - Often LoRA is applied to the weight matrices in the self-attention module (query, key, and value)

![Untitled 22 56.png](../../attachments/Untitled%2022%2056.png)

# Input perspective

- This case is very similar to prompt tuning. We **prefix** parameters to the input of the transformer.
    - The **prefix** is a low dimensional vector, which we can view as virtual words.

![Untitled 23 52.png](../../attachments/Untitled%2023%2052.png)

- During training, we freeze the model and just learn the prefixes.
    - In this case, this prefix is only applied to the **input-layer**

![Untitled 24 48.png](../../attachments/Untitled%2024%2048.png)

## Multi-layer prompt tuning

![Untitled 25 43.png](../../attachments/Untitled%2025%2043.png)

## Scale

![Untitled 26 38.png](../../attachments/Untitled%2026%2038.png)

# Functional perspective (adapters)

- In this perspective, we specifically change the model’s functions with respect to the task.

![Untitled 27 34.png](../../attachments/Untitled%2027%2034.png)

## Adapter

- Insert a mini-network between layers of a pretrained model. The goal of these layers is to help the model adapt to downstream tasks.
- The input of the **adapter** is the hidden states.

![Untitled 28 32.png](../../attachments/Untitled%2028%2032.png)

![Untitled 29 31.png](../../attachments/Untitled%2029%2031.png)

- Adapter based tuning attains a similar performance to full finetuning with **two orders of magnitude fewer** trained parameters

## Task knowledge

- We can use the adapter to learn useful information about a task or a language.

![Untitled 30 31.png](../../attachments/Untitled%2030%2031.png)

- You can then use the adapter and plug it into other tasks or languages.

![Untitled 31 27.png](../../attachments/Untitled%2031%2027.png)

## Rescaling

![Untitled 32 27.png](../../attachments/Untitled%2032%2027.png)

## Parameter Generalization

- Instead of learning module parameters for every task from scratch, use a small neural network to directly generate the module parameters.

![Untitled 33 26.png](../../attachments/Untitled%2033%2026.png)

![Untitled 34 21.png](../../attachments/Untitled%2034%2021.png)

# Summary

![Untitled 35 19.png](../../attachments/Untitled%2035%2019.png)

![Untitled 36 18.png](../../attachments/Untitled%2036%2018.png)