---
Week: Week 2
---
# 1 Named Entity Recognition

![Untitled 211.png](../../attachments/Untitled%20211.png)

![Untitled 1 174.png](../../attachments/Untitled%201%20174.png)

- In the above, $f$﻿ is a elementwise non-linear function. Applying that to the weights and biases $Wx + b$﻿ gives us the hidden transformation $h$﻿

# 2 Non-linearities

![Untitled 2 173.png](../../attachments/Untitled%202%20173.png)

- Neural networks do function approximation, e.g., regression or classification
    - Without non-linearities, deep neural networks can’t do anything more than a linear transform.
    - Extra layers could just be compiled down into a single linear transform: $W_1 W_2 x = Wx$﻿
        - By using non-linearities, we can have each layer be represented as a matrix $W_i$﻿
    - But, with more layers that include non-linearities, they can approximate any complex function!

# 3 Cross-entropy loss

- Essentially finds the distance between two different distributions.
    - The ground truth probability distribution is a one-hot vector. This makes it so that the cross-entropy loss is just the negative log probability of the true class $\log q(c)$﻿

![Untitled 3 170.png](../../attachments/Untitled%203%20170.png)

# 4 Stochastic Gradient Descent

![Untitled 4 165.png](../../attachments/Untitled%204%20165.png)

- There are a lot of parameters in deep learning problems.

# 5 Matrix Calculus

![Untitled 5 162.png](../../attachments/Untitled%205%20162.png)

## 5.1 Gradients

![Untitled 6 160.png](../../attachments/Untitled%206%20160.png)

![Untitled 7 156.png](../../attachments/Untitled%207%20156.png)

## 5.2 Jacobian Matrix

- An $m \times n$﻿ matrix of partial derivatives, for a function with $m$﻿ outputs and $n$﻿ inputs

![Untitled 8 147.png](../../attachments/Untitled%208%20147.png)

## 5.3 Chain Rule

![Untitled 9 143.png](../../attachments/Untitled%209%20143.png)

- Because $h$﻿ and $z$﻿ are both in $\R^n$﻿, the function $f$﻿ has $n$﻿ inputs $n$﻿ outputs, so the Jacobian will be an $n \times n$﻿ matrix

![Untitled 10 137.png](../../attachments/Untitled%2010%20137.png)

## 5.4 Common Jacobians

![Untitled 11 132.png](../../attachments/Untitled%2011%20132.png)

# 6 Matrix Calculus with NER

![Untitled 12 129.png](../../attachments/Untitled%2012%20129.png)

1. The first step to calculate this gradient is to break it up into simpler pieces
    
    ![Untitled 13 120.png](../../attachments/Untitled%2013%20120.png)
    
2. From here, apply the chain rule with the function compositions
    
    ![Untitled 14 107.png](../../attachments/Untitled%2014%20107.png)
    
    ![Untitled 15 102.png](../../attachments/Untitled%2015%20102.png)
    

## 6.1 Reusing Computations

- Notice that we have already calculated some major gradients that can be reused

![Untitled 16 96.png](../../attachments/Untitled%2016%2096.png)

![Untitled 17 90.png](../../attachments/Untitled%2017%2090.png)

- This upstream gradient is the error that is being passed from above to the lower layers. It is shared between the gradients of lower layers.
- These gradients are used to update our parameters during SGD

# 7 Output shape

- In the case below, $s$﻿ is a scalar and $W$﻿ is an $n \times m$﻿ matrix. Therefore, the Jacobian will be a matrix of size $1 \times nm$﻿, basically a huge row vector.
    
    ![Untitled 18 82.png](../../attachments/Untitled%2018%2082.png)
    
- The update step for $W$﻿ would be $W^{new} = W^{old} - \alpha \frac{\partial{s}}{\partial{W}}$﻿. These dimensions don’t match.

## 7.1 Shape convention

- We will instead use **shape convention**. We’ll make the shape of the gradient the same as the shape of the parameters, for easier computation.
    
    ![Untitled 19 74.png](../../attachments/Untitled%2019%2074.png)
    
- The shape convention contains the same numbers as the Jacobian. Instead, it just makes the shape the same shape as the parameters for easier SGD computation.
    - In Jacobian, all of these entries was in a row vector. In this case, we turn it into a matrix.

![Untitled 20 69.png](../../attachments/Untitled%2020%2069.png)

![Untitled 21 63.png](../../attachments/Untitled%2021%2063.png)

- These are just useful tricks to make our computations efficient and work out

## 7.2 Gradient w.r.t a single W_ij

![Untitled 22 58.png](../../attachments/Untitled%2022%2058.png)

- When taking the derivative, we essentially follow the computational graph and focus only on the ones that $W_{23}$﻿ matter.
    - In this case, $W_{23}$﻿ is only used for $z_2$﻿, and it’s multiplied with $x_3$﻿.

## 7.3 Derivative Shape

![Untitled 23 54.png](../../attachments/Untitled%2023%2054.png)

![Untitled 24 50.png](../../attachments/Untitled%2024%2050.png)

# 8 Backpropagation

- It’s taking derivatives and using the (generalized, multivariate, or matrix) chain rule
- We re-use derivatives computed for higher layers in computing derivatives for lower layers to minimize computation

## 8.1 Forwards step

- **Computation graph**: represents the neural net equations as a graph.
    
    ![Untitled 25 44.png](../../attachments/Untitled%2025%2044.png)
    
    - In the first layer, it represents $x$﻿ being multiplied with $W$﻿.
    - Then, the result of that is added to $b$﻿ to make $z = Wx + b$﻿
    - Then, we apply the result through $f$﻿ to get $h = f(z)$﻿
    - Finally, we multiply that result with $u$﻿ to get $s = u^T h$﻿
- This is the **forward propagation step.**

## 8.2 Backwards step

- Now we need to go backwards and compute the gradients. These gradients are the gradient of our final result (the loss or score) with respect to each parameter.
    
    ![Untitled 26 39.png](../../attachments/Untitled%2026%2039.png)
    
    - Notice that $b$﻿, $z$﻿, and $h$﻿ each represent the parameters of our network.
- Each node essentially receives an **upstream gradient** and passes a **downstream gradient** for the next step of the chain rule.
    
    ![Untitled 27 35.png](../../attachments/Untitled%2027%2035.png)
    

### 8.2.1 Local Gradient

- Each node has a **local gradient**, the gradient of its output parameter ($h$﻿) with respect to its input parameter ($z$﻿)
    
    ![Untitled 28 33.png](../../attachments/Untitled%2028%2033.png)
    
    - The downstream gradient is the product of the local gradient and the upstream gradient.
        
        ![Untitled 29 32.png](../../attachments/Untitled%2029%2032.png)
        

### 8.2.2 Multiple inputs

- Having multiple inputs into a node means it has multiple local gradients

![Untitled 30 32.png](../../attachments/Untitled%2030%2032.png)

### 8.2.3 Multiple outputs

![Untitled 31 28.png](../../attachments/Untitled%2031%2028.png)

### 8.2.4 Node intuitions

- + distributes the upstream gradient (the 2) to each summand (the 2 in x and y)
    
    ![Untitled 32 28.png](../../attachments/Untitled%2032%2028.png)
    
- max routes the upstream gradient to whichever was the max
    
    ![Untitled 33 27.png](../../attachments/Untitled%2033%2027.png)
    

## 8.3 Implementation

- Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output

![Untitled 34 22.png](../../attachments/Untitled%2034%2022.png)

![Untitled 35 20.png](../../attachments/Untitled%2035%2020.png)

- In the backward step, the $dz$﻿ is the gradient of the loss with respect to $z$﻿
- The return value is the gradient of the loss with respect to the inputs $x$﻿ and $y$﻿

## 8.4 Example

- Look look at an example, look at slide 57 on the [lecture slides](https://web.stanford.edu/class/cs224n/slides/cs224n-2024-lecture03-neuralnets.pdf)

## 8.5 Summary

![Untitled 36 19.png](../../attachments/Untitled%2036%2019.png)

![Untitled 37 18.png](../../attachments/Untitled%2037%2018.png)

![Untitled 38 17.png](../../attachments/Untitled%2038%2017.png)