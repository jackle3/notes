---
Week: Week 2
---
# Named Entity Recognition

![[attachments/Untitled 211.png|Untitled 211.png]]

![[attachments/Untitled 1 174.png|Untitled 1 174.png]]

- In the above, $f$﻿ is a elementwise non-linear function. Applying that to the weights and biases $Wx + b$﻿ gives us the hidden transformation $h$﻿

# Non-linearities

![[attachments/Untitled 2 173.png|Untitled 2 173.png]]

- Neural networks do function approximation, e.g., regression or classification
    - Without non-linearities, deep neural networks can’t do anything more than a linear transform.
    - Extra layers could just be compiled down into a single linear transform: $W_1 W_2 x = Wx$﻿
        - By using non-linearities, we can have each layer be represented as a matrix $W_i$﻿
    - But, with more layers that include non-linearities, they can approximate any complex function!

# Cross-entropy loss

- Essentially finds the distance between two different distributions.
    - The ground truth probability distribution is a one-hot vector. This makes it so that the cross-entropy loss is just the negative log probability of the true class $\log q(c)$﻿

![[attachments/Untitled 3 170.png|Untitled 3 170.png]]

# Stochastic Gradient Descent

![[attachments/Untitled 4 165.png|Untitled 4 165.png]]

- There are a lot of parameters in deep learning problems.

# Matrix Calculus

![[attachments/Untitled 5 162.png|Untitled 5 162.png]]

## Gradients

![[attachments/Untitled 6 160.png|Untitled 6 160.png]]

![[attachments/Untitled 7 156.png|Untitled 7 156.png]]

## Jacobian Matrix

- An $m \times n$﻿ matrix of partial derivatives, for a function with $m$﻿ outputs and $n$﻿ inputs

![[attachments/Untitled 8 147.png|Untitled 8 147.png]]

## Chain Rule

![[attachments/Untitled 9 143.png|Untitled 9 143.png]]

- Because $h$﻿ and $z$﻿ are both in $\R^n$﻿, the function $f$﻿ has $n$﻿ inputs $n$﻿ outputs, so the Jacobian will be an $n \times n$﻿ matrix

![[attachments/Untitled 10 137.png|Untitled 10 137.png]]

## Common Jacobians

![[attachments/Untitled 11 132.png|Untitled 11 132.png]]

# Matrix Calculus with NER

![[attachments/Untitled 12 129.png|Untitled 12 129.png]]

1. The first step to calculate this gradient is to break it up into simpler pieces
    
    ![[attachments/Untitled 13 120.png|Untitled 13 120.png]]
    
2. From here, apply the chain rule with the function compositions
    
    ![[attachments/Untitled 14 107.png|Untitled 14 107.png]]
    
    ![[attachments/Untitled 15 102.png|Untitled 15 102.png]]
    

## Reusing Computations

- Notice that we have already calculated some major gradients that can be reused

![[attachments/Untitled 16 96.png|Untitled 16 96.png]]

![[attachments/Untitled 17 90.png|Untitled 17 90.png]]

- This upstream gradient is the error that is being passed from above to the lower layers. It is shared between the gradients of lower layers.
- These gradients are used to update our parameters during SGD

# Output shape

- In the case below, $s$﻿ is a scalar and $W$﻿ is an $n \times m$﻿ matrix. Therefore, the Jacobian will be a matrix of size $1 \times nm$﻿, basically a huge row vector.
    
    ![[attachments/Untitled 18 82.png|Untitled 18 82.png]]
    
- The update step for $W$﻿ would be $W^{new} = W^{old} - \alpha \frac{\partial{s}}{\partial{W}}$﻿. These dimensions don’t match.

## Shape convention

- We will instead use **shape convention**. We’ll make the shape of the gradient the same as the shape of the parameters, for easier computation.
    
    ![[attachments/Untitled 19 74.png|Untitled 19 74.png]]
    
- The shape convention contains the same numbers as the Jacobian. Instead, it just makes the shape the same shape as the parameters for easier SGD computation.
    - In Jacobian, all of these entries was in a row vector. In this case, we turn it into a matrix.

![[attachments/Untitled 20 69.png|Untitled 20 69.png]]

![[attachments/Untitled 21 63.png|Untitled 21 63.png]]

- These are just useful tricks to make our computations efficient and work out

## Gradient w.r.t a single W_ij

![[attachments/Untitled 22 58.png|Untitled 22 58.png]]

- When taking the derivative, we essentially follow the computational graph and focus only on the ones that $W_{23}$﻿ matter.
    - In this case, $W_{23}$﻿ is only used for $z_2$﻿, and it’s multiplied with $x_3$﻿.

## Derivative Shape

![[attachments/Untitled 23 54.png|Untitled 23 54.png]]

![[attachments/Untitled 24 50.png|Untitled 24 50.png]]

# Backpropagation

- It’s taking derivatives and using the (generalized, multivariate, or matrix) chain rule
- We re-use derivatives computed for higher layers in computing derivatives for lower layers to minimize computation

## Forwards step

- **Computation graph**: represents the neural net equations as a graph.
    
    ![[attachments/Untitled 25 44.png|Untitled 25 44.png]]
    
    - In the first layer, it represents $x$﻿ being multiplied with $W$﻿.
    - Then, the result of that is added to $b$﻿ to make $z = Wx + b$﻿
    - Then, we apply the result through $f$﻿ to get $h = f(z)$﻿
    - Finally, we multiply that result with $u$﻿ to get $s = u^T h$﻿
- This is the **forward propagation step.**

## Backwards step

- Now we need to go backwards and compute the gradients. These gradients are the gradient of our final result (the loss or score) with respect to each parameter.
    
    ![[attachments/Untitled 26 39.png|Untitled 26 39.png]]
    
    - Notice that $b$﻿, $z$﻿, and $h$﻿ each represent the parameters of our network.
- Each node essentially receives an **upstream gradient** and passes a **downstream gradient** for the next step of the chain rule.
    
    ![[attachments/Untitled 27 35.png|Untitled 27 35.png]]
    

### Local Gradient

- Each node has a **local gradient**, the gradient of its output parameter ($h$﻿) with respect to its input parameter ($z$﻿)
    
    ![[attachments/Untitled 28 33.png|Untitled 28 33.png]]
    
    - The downstream gradient is the product of the local gradient and the upstream gradient.
        
        ![[attachments/Untitled 29 32.png|Untitled 29 32.png]]
        

### Multiple inputs

- Having multiple inputs into a node means it has multiple local gradients

![[attachments/Untitled 30 32.png|Untitled 30 32.png]]

### Multiple outputs

![[attachments/Untitled 31 28.png|Untitled 31 28.png]]

### Node intuitions

- + distributes the upstream gradient (the 2) to each summand (the 2 in x and y)
    
    ![[attachments/Untitled 32 28.png|Untitled 32 28.png]]
    
- max routes the upstream gradient to whichever was the max
    
    ![[attachments/Untitled 33 27.png|Untitled 33 27.png]]
    

## Implementation

- Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output

![[attachments/Untitled 34 22.png|Untitled 34 22.png]]

![[attachments/Untitled 35 20.png|Untitled 35 20.png]]

- In the backward step, the $dz$﻿ is the gradient of the loss with respect to $z$﻿
- The return value is the gradient of the loss with respect to the inputs $x$﻿ and $y$﻿

## Example

- Look look at an example, look at slide 57 on the [lecture slides](https://web.stanford.edu/class/cs224n/slides/cs224n-2024-lecture03-neuralnets.pdf)

## Summary

![[attachments/Untitled 36 19.png|Untitled 36 19.png]]

![[attachments/Untitled 37 18.png|Untitled 37 18.png]]

![[attachments/Untitled 38 17.png|Untitled 38 17.png]]