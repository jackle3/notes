---
Week: Week 5
---
# Advantage of Transformers

![[attachments/Untitled 52 4.png|Untitled 52 4.png]]

## Transformers + Pretraining

- Most good Transformer results also included **pretraining**
    - Transformers’ parallelizability allows for efficient pretraining.

![[attachments/Untitled 206.png|Untitled 206.png]]

- GLUE stands for General Language Understanding Evaluation

# Issues with Transformers

![[attachments/Untitled 1 169.png|Untitled 1 169.png]]

## Quadratic computation

- The advantage of transformers is that you can take information from anywhere in the input and use it for anywhere in the decoder.
    - The issue is that the self-attention requires you to compute these interactions, which is quadratic.
- Recurrent models are linear because you just calculate it at one time step and move it to the next time step.

![[attachments/Untitled 2 168.png|Untitled 2 168.png]]

- In practice, the cap of the sequence length used for attention is $n = 512$﻿

## Improving quadratic self-attention cost

- The increasing line is the regular transformer.

![[attachments/Untitled 3 165.png|Untitled 3 165.png]]

## Is it necessary to remove quadratic cost of attention?

- All the big models right now still use the quadratic attention.

![[attachments/Untitled 4 160.png|Untitled 4 160.png]]

# Pretraining

- When Pretraining was introduced in around 2018, performance gains increased significantly.

![[attachments/Untitled 5 157.png|Untitled 5 157.png]]

## Key Ideas

- Pretraining is doing large-scale unsupervised learning using data on the internet
- The main requirements of pretraining are:
    - Be able to make sure your model can process large-scale, diverse datasets
    - Don’t use labeled data (otherwise you can’t scale)
    - Scale in computationally-efficient ways

![[attachments/Untitled 6 155.png|Untitled 6 155.png]]

# Word structure and subword models

- Common words can be easily mapped to the vocabulary to extract an embedding.
- Variations, mispelleings, and novel words can’t really be mapped.

![[attachments/Untitled 7 151.png|Untitled 7 151.png]]

## Subword tokens

- In modern NLP tasks, it now uses subword tokenization instead of word tokenization.
    - Allows you to map any sort of string without unknown tokens into a sequence of tokens / sequence of embeddings.
    - Lets you handle variations, mispellings, etc.
- Instead of having one embedding for each word, you have one embedding for each subword.

![[attachments/Untitled 8 142.png|Untitled 8 142.png]]

- The term `##` means connect to next subword without a space.

## Byte-pair encoding algorithm

![[attachments/Untitled 9 138.png|Untitled 9 138.png]]

- We essentially build up a vocabulary of common subwords, merging frequently occuring patterns into a new subword.
    - We recursively repeat until we have a good enough vocab size of subwords, which may contain full words (if the words are common enough)
    - In the worst case, if it can’t get tokens for a word, it’ll be tokenized into individual characters.

# Importance of context

![[attachments/Untitled 10 132.png|Untitled 10 132.png]]

- We need context in order to truly understand the meaning of a word.

## Pretrained word embeddings (word2vec)

- **word2vec** was a pretraining algorithm without context
    - It just created a static embedding for each word, without considering the current context that it’s being used on.

![[attachments/Untitled 11 127.png|Untitled 11 127.png]]

- Every word gets the same word embedding, regardless of the context that its being used in.
- The biggest issue is that you’re **learning everything about the task from scratch**.
    - You start out knowing only the meaning of words, and not anything else about grammar or sentence structure or etc.

## Pretraining models (modern)

- Here, we’re more thinking about a pretraining algorithm that considers context.
- Almost all parts of the model is pretrained, meaning it has already learned most of what it needs to do downstream tasks.
    - Most pretraining methods involve either:
        - Masking parts of the input and training to reconstruct those parts
        - Language modeling → completely unsupervised and learning how to generate pieces of text

![[attachments/Untitled 12 124.png|Untitled 12 124.png]]

# Pretraining methods

- In general, pretraining means using a model that has already been trained on a general task, and finetuning it for your downstream task.

## Pretraining by reconstructing input

- This is how a lot of pretrained models are trained, because you can learn a lot of general information about the world and language.
- For example, some training examples it might learn from is:
    - Stanford University is located in _____, California.
        - It can learn format from this: e.g. X, Y is City, State
        - It can learn relation: Stanford is in Palo Alto, and Palo Alto is in CA
    - I put ____ fork down on the table.
        - It can learn sentence structure, parts of speech, etc.
    - The woman walked across the street, checking for traffic over ___ shoulder.
        - It might learn pronouns, gender agreement, dependencies, etc.
- If we create a model that is super good at this task, it can likely generalize very well to other downstream tasks since it knows a lot about generating text.

## Pretraining through language modeling

- Language modeling is unsupervised and very easy to scale. As such, it makes a really good pretraining method.

![[attachments/Untitled 13 115.png|Untitled 13 115.png]]

## Pretraining / Finetuning Paradigm

- First, we train a language model using pretraining. Then, you use that model and finetune it to do your task.
    - One way to finetune for sentiment classification is to attach a linear classifier after the last hidden state, and train it to predict sentiment.

![[attachments/Untitled 14 102.png|Untitled 14 102.png]]

# What does pretraining learn?

![[attachments/Untitled 15 97.png|Untitled 15 97.png]]

# Why does pretraining work?

- Generally with stochastic gradient descent on a pretrained model, it won’t move very far from the intialization.

![[attachments/Untitled 16 91.png|Untitled 16 91.png]]

# Where does data come from?

- As seen below, the data used to pretrain these models is extremely diverse.

![[attachments/Untitled 17 85.png|Untitled 17 85.png]]

# Pretraining architecture

- The goal of encoders is to build very strong representations, by allowing it to look and condition on the future.
- The goal of decoders is to generate text. It can’t condition on future words.

![[attachments/Untitled 18 77.png|Untitled 18 77.png]]

# Pretraining encoders

- Since encoders have bidirectional context, it can’t do language modeling.
- Instead of traditional language modeling where we predict the next word, we just predict a random word by masking words out.
    - This is **pretraining by reconstructing input**

![[attachments/Untitled 19 69.png|Untitled 19 69.png]]

- The model learns to predict words at places that are masked out.

## BERT and Masked LM

- This stands for "Bidirectional Encoder Representations from Transformers”
- Instead of specifically selecting which words to mask and only predicting on those, just randomly pick 15% of the tokens and predict with them.
    
    - Do some data augmentation to force the model to learn strong representations independent of the masks.
    
    ![[attachments/Untitled 20 65.png|Untitled 20 65.png]]
    
- In the original model, it was trained to predict whether one chunk followed the other.
    
    - However, future research showed that this task was not really necessary.
    
    ![[attachments/Untitled 21 59.png|Untitled 21 59.png]]
    
- Below are a few in-depth details of BERT
    
    ![[attachments/Untitled 22 54.png|Untitled 22 54.png]]
    
- BERT was an extremely successful model, for many downstream tasks.
    
    ![[attachments/Untitled 23 50.png|Untitled 23 50.png]]
    

## Limitations of pretrained encoders

- Decoders are naturally better at generating sequences than encoders.

![[attachments/Untitled 24 46.png|Untitled 24 46.png]]

## Extensions of BERT

- RoBERTa changse the pretraining method, and it works a lot better.

![[attachments/Untitled 25 41.png|Untitled 25 41.png]]

- Often times, just training for longer with more data gets massive improvements.

![[attachments/Untitled 26 36.png|Untitled 26 36.png]]

# Pretraining encoder-decoders

![[attachments/Untitled 27 32.png|Untitled 27 32.png]]

- On the decoder side, we’re doing language modeling and generating text.
    - Below, it sees the sees the encoder states and tries to generate the red.
- On the encoder side, we get to look at the whole sequence.
    - Below, it will see the green inputs → the masked text.

![[attachments/Untitled 28 30.png|Untitled 28 30.png]]

- This is different from SpanBERT in that:
    - SpanBERT knew how many tokens to generate, and it just predicts words for masked portions.
    - This one is just generating a full sequence. It does not know how many tokens to generate, when to stop, etc.

## General knowledge

- If we pretrain an encoder-decoder on internet data, it’ll learn a lot of general knowledge about the world.
    - It’s really good at question and answering.

![[attachments/Untitled 29 29.png|Untitled 29 29.png]]

# Pretraining decoders

- Most of the modern pretrained models today are decoder-only models.
- You can use pretrained language models to perform various downstream tasks by building linear layers on top of the last hidden state.
    
    - Don’t have to limit to just generating and modeling text.
    
    ![[attachments/Untitled 30 29.png|Untitled 30 29.png]]
    
- You can also use it to generate and model text, as designed.
    
    - Examples for this would be summarization → input a long sequence, output is a shorter sequence that acts as a summary
    
    ![[attachments/Untitled 31 25.png|Untitled 31 25.png]]
    

# Generative Pretrained Transformer (GPT)

![[attachments/Untitled 32 25.png|Untitled 32 25.png]]

- In order to finetune the models, we have to figure out a good way to format inputs so that it performs well on the tasks.

![[attachments/Untitled 33 24.png|Untitled 33 24.png]]

## GPT-2

- The number of parameters in this was scaled up dramatically.

![[attachments/Untitled 34 19.png|Untitled 34 19.png]]

## GPT-3

- The number of parameters was again dramatically increased.

![[attachments/Untitled 35 17.png|Untitled 35 17.png]]

- This model was able to perform in-context learning.
    - If you give it input and output examples into its prompt, generating from the model often continued the pattern and did good predictions, even thought it did not fully learn from the data.

![[attachments/Untitled 36 16.png|Untitled 36 16.png]]

![[attachments/Untitled 37 16.png|Untitled 37 16.png]]

# Scaling laws

- As compute, dataset size, and parameters increase, the test loss reliably decreases.

![[attachments/Untitled 38 15.png|Untitled 38 15.png]]

## Plateau and Convergence

- This also extends to how long it takes to plateau. Smaller models plateau pretty quickly, while bigger models can keep training and improving.
    - You often want to train big models, but don’t fully converge them. Usually, this means training in less than one epoch over the corpus.
- In the visualization below, each line is a model with different number of paramters.
    
    ![[attachments/Untitled 39 14.png|Untitled 39 14.png]]
    

## Architecture Decisions

- Scaling laws allows you to intelligently decide architecture, instead of guessing and checking like before.

![[attachments/Untitled 40 13.png|Untitled 40 13.png]]

## Scaling Efficiency

- Research has found that it’s better to train smaller models on a lot more data, as opposed to big models on less data.

![[attachments/Untitled 41 10.png|Untitled 41 10.png]]

- This is the most compute-efficient thing to do to still get the best model.

# What does pretraining teach?

- Pretraining allows it to learn about a lot of things.
    
    ![[attachments/Untitled 42 10.png|Untitled 42 10.png]]
    
- It can also learn some things that you don’t want it to learn, such as private information or memorizing the training data.
    
    ![[attachments/Untitled 43 8.png|Untitled 43 8.png]]