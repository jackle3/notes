---
Week: Week 6
---
# What is question answering?

- The goal is to build systems that automatically answer questions posed by humans in natural language.

![Untitled 207.png](attachments/Untitled%20207.png)

- Lots of immediate applications: search engines, dialogue systems, etc
- QA can be used to evaluate how well systems understand human language.
    - Being able to answer any question is the strongest possible demonstration of understanding.

## Taxonomy

- A factoid question is a yes/no question, or one with a very specific fact-based answer.
- A non-factoid question can have a variety of answers → open-ended

![Untitled 1 170.png](attachments/Untitled%201%20170.png)

- The context is: based on what are we answering the question?

# IBM Watson (2011)

- This was a question & answering system that managed to beat Jeopardy champions.

![Untitled 2 169.png](attachments/Untitled%202%20169.png)

- The first step is question processing: figure out named entities, coreference, how to parse the question.
- The second step is candidate answer generation: generate potential answers to the question.
- The third step is candidate answer scoring: give a score to each candidate answer
- The last step is ranking and confidence merging the candidate answers

# QA In Deep Learning

![Untitled 3 166.png](attachments/Untitled%203%20166.png)

- Researchers used BERT as encoder to get representation for questions, and then generate an answer.
- There are many types of question-answering systems. Look at slides for more.

## Reading Comprehension

- We give the system a paragraph containing information, then we ask it questions about the text. The answer is usually inside the text.

![Untitled 4 161.png](attachments/Untitled%204%20161.png)

## Textual Question Answering

- Similar to before, we give it a text and a question about the text. The difference is that now we give it multiple choice options, and it must choose the best one.

![Untitled 5 158.png](attachments/Untitled%205%20158.png)

## Conversational Question Answering

- Build questions on top of each other. The LLM must be able to remember the previous answers.

![Untitled 6 156.png](attachments/Untitled%206%20156.png)

# Stanford Question Answering Dataset (SQuAD)

- A good reading comprehension dataset used as a benchmark for evaluating language models
    - → though it’s almost solved (most LLMs now achieve performance better than humans)

![Untitled 7 152.png](attachments/Untitled%207%20152.png)

# Feature-based methods

- The task here is just like classification.
    - We first generate candidate answers.
    - Then, we define a feature vector using some sort of feature engineering.
        - It takes it the context $p$﻿, the question $q$﻿, and the candidate answer $a_i$﻿.
    - Finally, we throw the feature vector into logistic regression to classify.

![Untitled 8 143.png](attachments/Untitled%208%20143.png)

# Neural models

- The input is a context that be up to $N$﻿ tokens, and a question that can be up to $M$﻿ tokens.
- The output is just the start index and the end index of the answer to the question.
    - This is because for reading comprehension, the answer is assumed to be inside the text.

![Untitled 9 139.png](attachments/Untitled%209%20139.png)

## Stanford Attentive Reader

![Untitled 10 133.png](attachments/Untitled%2010%20133.png)

- To get the representation for the question:
    
    - We use word2vec or GLoVE to get a dense embedding for each word in the question.
    - Then, we add a bidirectional LSTM on top of these word embeddings to get a more comprehensive representation.
    - Then, we use attention to calculate an output representation for the question.
    
    ![Untitled 11 128.png](attachments/Untitled%2011%20128.png)
    
- To process the passage/context, we also use a bidirectional LSTM to get the representation for the passage.
    
    ![Untitled 12 125.png](attachments/Untitled%2012%20125.png)
    
- Once we have the representation for both, we use attention and joint their representations to get our predictions.
    
    - We need two predictions. The start token in the passage where our answer starts, and the end token where our answer ends.
    
    ![Untitled 13 116.png](attachments/Untitled%2013%20116.png)
    
    - We can make this very efficient because we know that `start <= end`

## BiDAF

- This model encodes the question using word and character embeddings.

![Untitled 14 103.png](attachments/Untitled%2014%20103.png)

- The innovative part is that it troduces two attention mechanisms:
    - Question-to-passage attention: find out which context word have the closest similarity to the query words.
    - Passage-to-question attention: finds out which query words are most relevant to each context words.
    - Allows us to find our which words are important from the query and which words are important from the context.
- After we get the representations, it also introduces a modeling layer that extracts semantic information.

![Untitled 15 98.png](attachments/Untitled%2015%2098.png)

- We can visualize the attention:
    
    - Each row is a query word, each column is a context word.
    
    ![Untitled 16 92.png](attachments/Untitled%2016%2092.png)
    
    - It’s very interpretable to see what context words matter the most to each query word.

# Ablation Study

- Used to figure out whether a change is actually working → is the reason for improvement because of the change?
    - In research, this would mean removing certain parts of model to see which is most important.

![Untitled 17 86.png](attachments/Untitled%2017%2086.png)

# BERT for reading comprehension

- This about BERT as a feature extractor.
    - We simply concatentate the question and the paragraph to create our input. No need to do any manual representation learning or engineering.
- The goal is still to predict the start and end token in the context.
    
    ![Untitled 18 78.png](attachments/Untitled%2018%2078.png)
    
- The loss function will have to be jointly-optimized between the start and end tokens.
    
    ![Untitled 19 70.png](attachments/Untitled%2019%2070.png)
    
- BERT is very simple. People tend to train all of the BERT parameters as well as the newly introduced ones for the classifiers.
    
    ![Untitled 20 66.png](attachments/Untitled%2020%2066.png)
    

## Comparison between BERT and BiDAF

![Untitled 21 60.png](attachments/Untitled%2021%2060.png)

# Is reading comprehension solved?

- Modern LLMs already surpass human performance on SQuAD dataset.
- However, it is not solved because they still perform poorly on adversarial examples.
- In the example below, we add extraneous information to distract the model from the actual answer.

![Untitled 22 55.png](attachments/Untitled%2022%2055.png)

- There are modern tests that measure the failure rate of models on certain difficult tasks.
    
    ![Untitled 23 51.png](attachments/Untitled%2023%2051.png)
    

# Open-domain question answering

- In this case, we don’t assume a given context. We find our answer from a large collection of documents such as Wikipedia.

![Untitled 24 47.png](attachments/Untitled%2024%2047.png)

## Reriever-Reader framework

![Untitled 25 42.png](attachments/Untitled%2025%2042.png)

- Given a question, first have a model retrieve the relevant document. Then, have another model read the document and output the answer.

![Untitled 26 37.png](attachments/Untitled%2026%2037.png)

- The retriever returns the $K$﻿ most relevant documents based on the documents given and the query.
- The reader uses those $K$﻿ documents as the context, and gives the answer just like reading comprehension.

![Untitled 27 33.png](attachments/Untitled%2027%2033.png)

- Note that the retriever here isn’t really trained. It’s a heuristic-based model.

# Training the retriever

- We can use BERT to learn the retriever score (in red on the left), and then retrieve the documents with the top K scores.

![Untitled 28 31.png](attachments/Untitled%2028%2031.png)

## REALM: retrieval-augmented LM

- One modern work is REALM: retrieval-augmented LM
    
    - Augments LM pre-training with a neural knowledge retriever → retrieves knowledge from a textual knowledge corpus (e.g. Wikipedia)
    - Signal from the language modeling objective backpropagates through the retriever.
    
    ![Untitled 29 30.png](attachments/Untitled%2029%2030.png)
    
    - $x$﻿ is the given article. We care about whether $y$﻿, your answer, is correct.
        - From a finetuning perspective, $x$﻿ is the question and $y$﻿ is the answer.
    - We use the law of total probability to expand this probability.
        - $z$﻿ is the latent article that we retrieved, given the question $x$﻿.
        - This is a pretty expensive operation because we have to enumerate through all articles in $Z$﻿
            
            ![Untitled 30 30.png](attachments/Untitled%2030%2030.png)
            
- To find $p(z | x)$﻿, we can represent the input and the document as embeddings.
    
    ![Untitled 31 26.png](attachments/Untitled%2031%2026.png)
    
- To find $p(y | z, x)$﻿ → we need to enumerate over all the $z$﻿ to get a good $y$﻿ output.
    
    ![Untitled 32 26.png](attachments/Untitled%2032%2026.png)
    

## DPR: Dense passage retrieval

- Look at some amount of question-answer pairs and just learn directly from them, instead of from passages.

![Untitled 33 25.png](attachments/Untitled%2033%2025.png)

## Deep retrieval + generative models

- Instead of extracting answers from passages, we can generate the answers.

![Untitled 34 20.png](attachments/Untitled%2034%2020.png)

- GIven a question, and the top K articles relevant to it, we merge them into one giant input for the generative encoder-decoder model.
    - Then, we use that to generate the answer directly.

## Fine-tuned LLMs

- Pretrained LLMs have already seen a large amount of information during the pretraining stage. We can just simply use its pretraining knowledge to answer questions during finetuning.

![Untitled 35 18.png](attachments/Untitled%2035%2018.png)

# Reader model is not necessary

- Instead of having a reader model, simply encode all the potential answer phrases using dense vectors, and do nearest neighbor search during inference.

![Untitled 36 17.png](attachments/Untitled%2036%2017.png)

# LLMs cannot memorize everything

- There is a limit to the context that it remembers.

![Untitled 37 17.png](attachments/Untitled%2037%2017.png)

- The biggest issue is that its opaque. It’s hard to verify if the answer is correct or not.

## Using retrieval to overcome LLM’s issues

- Instead of using the black-box LLM to answer it directly, provide it with relevant information just-in-time using a retriever.

![Untitled 38 16.png](attachments/Untitled%2038%2016.png)

## Retrieval augmented generation (RAG)

- Given the query, this model:
    1. First encodes the query to get a representation $q(x)$﻿
    2. Then uses MIPs to get top candidate documents for retrieval.
    3. Combines and passes those documents into the generator.
    4. The generator generates the answer.

![Untitled 39 15.png](attachments/Untitled%2039%2015.png)

- You only need to learn parameters for the generator and the query encoder.
    - Learning how to get a better candidate document index does not add much information.

# Problem \#1: how many documents can we use?

![Untitled 40 14.png](attachments/Untitled%2040%2014.png)

- The LLM does not pay attention to its context very well.
    - If the relevant document is in the middle of the set of documents, the model doesn’t really pay much attention to it, and accuracy goes down.

![Untitled 41 11.png](attachments/Untitled%2041%2011.png)

# Problem \#2: useful and usable citations

- A unique benefit of RAG is that it can directly cite your sources.
    
    ![Untitled 42 11.png](attachments/Untitled%2042%2011.png)
    
- However, models can often hallucinate the citations for its sources.
    
    ![Untitled 43 9.png](attachments/Untitled%2043%209.png)