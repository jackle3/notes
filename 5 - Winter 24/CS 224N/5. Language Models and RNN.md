---
Week: Week 3
---
# 1 Lecture Plan

![Untitled 202.png](../../attachments/Untitled%20202.png)

# 2 Modern Neural Networks

- These new networks, especially language models, are enormous. They have billions of parameters now.

![Untitled 1 165.png](../../attachments/Untitled%201%20165.png)

- There are a lot of tricks that are necessary in order to get these large models to work.

## 2.1 Regularization

- Since we have so many parameters, we need to do regularizaztion to prevent overfitting.

![Untitled 2 164.png](../../attachments/Untitled%202%20164.png)

- It’s okay if our models overfit the training data, **as long as test error keeps going down**

## 2.2 Dropout

- Randomly set some activations of the neural network to be zero. Since its random, each training iteration will be different, with different weights zeroed out.
	- Also, never drop out the bias term

![Untitled 3 161.png](../../attachments/Untitled%203%20161.png)

- At test time, there is **no dropout**. We only do dropout during training.
	- Since we had dropout during training, our parameters will be shrunk. We need to scale it back up, so we multiply weights by $1 - p$﻿

![Untitled 4 156.png](../../attachments/Untitled%204%20156.png)

- It prevents feature co-adaptation (a form of overfitting).
	- Ensures that the activations are not dependent on each other. Makes them robust and encode independent features.

## 2.3 Vectorization

- The last two ideas were statistical improvements. This one is a computational improvement.

![Untitled 5 153.png](../../attachments/Untitled%205%20153.png)

## 2.4 Parameter Initialization

- The initial values of parameters are also important because they may affect how your model optimizes, especially when its not convex.

![Untitled 6 151.png](../../attachments/Untitled%206%20151.png)

## 2.5 Optimizers

- There are a lot of hyperparameters in our optimizers. Instead of manually tuning learning rates, we can use adaptive optimizers that tune it as it trains.

![Untitled 7 147.png](../../attachments/Untitled%207%20147.png)

# 3 Language Modeling

- Language models are essentially just a stochastic autocomplete system.

	![Untitled 8 138.png](../../attachments/Untitled%208%20138.png)

	- More specifically, this is an autoregressive language model because it builds sentences from left to right, predicting the next word.
- You can also think of language models as a system that **assigns a probability to a piece of text** that exists.

	![Untitled 9 134.png](../../attachments/Untitled%209%20134.png)

	- The left side of the equation is the joint probability over sequences (of this text).
	- The right side is a factorization using the chain rule. We predict each word $i$﻿ given every word that **came before it.**

# 4 N-gram Language Models

![Untitled 10 128.png](../../attachments/Untitled%2010%20128.png)

- The idea behind this is very similar to word embeddings.
	- It uses statistics from data of how often different words appear together to make predictions about what words comes next.
	- More specifically, we use those counts to estimate the probability of an n-gram.
- First, we assume that **the word we’re predicting only depends on the last n-1 words**

	![Untitled 11 123.png](../../attachments/Untitled%2011%20123.png)

	- It’s Markov because it depends on a finite chunk of the past, and nothing else.

## 4.1 Example

- First we make the markov assumption, discarding everything that it doesn’t depend on

![Untitled 12 120.png](../../attachments/Untitled%2012%20120.png)

- Then we use counting to compute the probabilities of a word $w$﻿ following this.

![Untitled 13 111.png](../../attachments/Untitled%2013%20111.png)

- The issue with n-grams is that we discarded some context that might’ve been necessary.
	- Since we used a 4-gram, we discarded proctors. This told us that “books” appearing was more likely, but “exams” would actually be more likely considering that there is a proctor.

## 4.2 Sparsity Problems

![Untitled 14 98.png](../../attachments/Untitled%2014%2098.png)

- For sparsity problem 1, we can use smoothing to fix the issue.
- For sparsity problem 2, we can’t really fix it while keeping our infrastructure as is because the denominator is now zero.
	- To fix, we have to use backoff. Instead of using a 5-gram, back off to a 4-gram, etc

## 4.3 Storage Problems

- It’s hard to have $n$﻿ bigger than 5 because of storage and data limitations

![Untitled 15 93.png](../../attachments/Untitled%2015%2093.png)

## 4.4 In Practice

- You can build n-gram language models using a pre-collected corpus very quickly.

![Untitled 16 87.png](../../attachments/Untitled%2016%2087.png)

- This can also be used to **generate text** by taking a sample of our probability distribution. Below is an example of a trigram model. **We create a trigram conditioning on a bigram.**

![Untitled 17 81.png](../../attachments/Untitled%2017%2081.png)

![Untitled 18 74.png](../../attachments/Untitled%2018%2074.png)

![Untitled 19 66.png](../../attachments/Untitled%2019%2066.png)

- The issue with generating text like this is that it’s incoherent.

![Untitled 20 62.png](../../attachments/Untitled%2020%2062.png)

# 5 Fixed-window Neural Language Model

![Untitled 21 56.png](../../attachments/Untitled%2021%2056.png)

- Similar to before, we take a window and discard all the words outside the window.

![Untitled 22 52.png](../../attachments/Untitled%2022%2052.png)

- Then we take our tokens and represent them as one-hot vectors. From there, we can learn the next word using a hidden layer.
	- Instead of using the one-hot vectors directly, we will take our words, look them up in a table, and use the corresponding word-embeddings. **This prevents issues with sparsity** since our word vectors are dense embeddings.

![Untitled 23 48.png](../../attachments/Untitled%2023%2048.png)

- Instead of outputting a classification prediction like we’ve done before, we output a probability distribution over words.

## 5.1 Advantages and Disadvantages

![Untitled 24 44.png](../../attachments/Untitled%2024%2044.png)

- The window can never be large enough → because every single position (or word in the window) has its own parameters, so increase the window adds many more parameters.
	- Since every word and position **is learned separately** with different parameters, we can’t learn the fact that seeing a word like “proctor” anywhere in the sentence increases the chances of “exam” being the output.
- Basically, this is learning separate parameters for every position in the window. We want to be able to share parameters between positions so that we can better learn contextual meaning.

# 6 Recurrent Neural Networks

- We repeatedly apply weights $W$﻿ as we move from one position to the next.

![Untitled 25 39.png](../../attachments/Untitled%2025%2039.png)

- Notice the position of the **hidden states**
	- Before, it was passing information upwards to directly make a prediction. It’s input was the weights applied to the entire window.
	- Now, it’s like a compressed summary of everything that has happening in the past, where $W$﻿ is applied between each hidden state.

## 6.1 Example

- Read this example from the bottom to the top.
- $e^{(t)}$﻿ is a word vector → it’s the embedding of the word at time step $t$﻿ (word $x^{(t)}$﻿)
- $W_e$﻿ is a matrix → weights for the word embedding to pass it into the current hidden state
- $W_h$﻿ is a matrix → weights for the previous hidden state
- $h^{(t)}$﻿ is a hidden state → represents the current state, summary of everything before time $t$﻿

![Untitled 26 34.png](../../attachments/Untitled%2026%2034.png)

- We start with our words. From there, we look up our word embeddings to get $e^{(t)}$﻿
- From here, we take that embeddings and process it.
	- We take the first position’s embedding $e^{(1)}$﻿. We use that embedding, as the initial hidden state $h^{(0)}$﻿, and apply a single-layer neural network to get the next hidden state $h^{(1)}$﻿
		- This single-layer neural network is the sigmoid equation above.
- Then, we pass that hidden state forwards to create the next hidden state using the next word embedding.
	- **Notice that we’re using the same weights throughout the time steps.**
		- We’re using $W_h$﻿ for all the hidden layers, and $W_e$﻿ for all the embeddings.
	- The number of parameters is **independent of the sequence length**
- Because it’s the same weights, we can continue for arbitrary length input sequences.
	- At the end, we use the last hidden state to make our prediction. Remember that the hidden state is the summary of everything that happened before, so the **last hidden state is the summary of the input sequence.**

## 6.2 Advantages and Disadvantages

- RNN Advantages
	- Can process any length input
	- Computation for step t can (in theory) use information from many steps back
	- Model size doesn’t increase for longer input context
	- Same weights applied on every timestep, so there is symmetry in how inputs are processed
- RNN Disadvantages
	- Recurrent computation is slow
		- You can’t really vectorize. You have to loop through each time step one by one.
	- In practice, difficult to access information from many steps back
		- $W_h$﻿ needs to be very finely tuned to keep magnitudes near one, so that information doesn’t explode or vanishes.
		- From $h^{(1)}$﻿ to $h^{(4)}$﻿, we are applying $W_h$﻿ three times. This means if $W_h$﻿ increases our magnitudes too much, the info from $h^{(1)}$﻿ might overpower the info from later times.

# 7 Training an RNN Language Model

![Untitled 27 30.png](../../attachments/Untitled%2027%2030.png)

- Loss is found by comparing the predicted probability distribution by the true distribution.
	- Loss at step $i$﻿ is the negative log probability of the word at step $i + 1$﻿

![Untitled 28 28.png](../../attachments/Untitled%2028%2028.png)

![Untitled 29 27.png](../../attachments/Untitled%2029%2027.png)

## 7.1 Backprop for RNNs

![Untitled 30 27.png](../../attachments/Untitled%2030%2027.png)

- This is because $W_h$﻿ appears at multiple time steps. Therefore, continue the gradient at each time step and sum them all up.
	- This is true because of the multivariable chain rule.

		![Untitled 31 23.png](../../attachments/Untitled%2031%2023.png)

		- Think of $f$﻿ as the RNN, and $t$﻿ as the weights $W_h$﻿. Just take the derivative against each instance of $t$﻿, and sum it up.

![Untitled 32 23.png](../../attachments/Untitled%2032%2023.png)

- We start from the output, and just go backwards through each timestep.

# 8 Generating with an RNN Language Model

- At every time step, we sample a word using the hidden state. The sampled output becomes the input for the next time step.
- We start the sentence using a starter token, such as the `<s>` token show below.

![Untitled 33 22.png](../../attachments/Untitled%2033%2022.png)

- Once we sample the token `</s>` , that’s the end of our sentence.

## 8.1 Generating with Style

- The sampled output will be dependent on the corpus that you train it on. If you train it using data written with a certain style, the output will have that same style.

![Untitled 34 18.png](../../attachments/Untitled%2034%2018.png)

- The RNN-LM above was trained with text from a book which has dialogue, so it generated diaglogue text.
	- Notice that the text still doesn’t make much sense. It’s quite incoherent.

## 8.2 Character-level RNN-LM

- Same as before, but it predicts what character comes next.

![Untitled 35 16.png](../../attachments/Untitled%2035%2016.png)

- In this example, it’s predicting the color names of paint based on the RGB codes.

# 9 Perplexity: Evaluating Language Models

- **Perplexity** is the number of guesses that you’ll need to predict the actual word.
	- If the perplexity is 1, that’s equal to the amount of uncertainty in a 1-sided dice.
		- You’ll need 1 guess to guess the output of a 1-sided dice.
	- If the perplexity is 10, that’s equal to the uncertainty in a 10-sided dice.
		- You’ll need roughly 10 guesses the guess the output of a 10-sided dice.

![Untitled 36 15.png](../../attachments/Untitled%2036%2015.png)

- The lower the perplexity → the fewer guesses you need → the better the model

![Untitled 37 15.png](../../attachments/Untitled%2037%2015.png)

## 9.1 Historical Performance

- Language models have been improving a lot in recent years.

![Untitled 38 14.png](../../attachments/Untitled%2038%2014.png)

- Large language models have perplexity in the 10s to 20s, depending on the corpus.

# 10 Summary

![Untitled 39 13.png](../../attachments/Untitled%2039%2013.png)

- RNNs are a type of language model
