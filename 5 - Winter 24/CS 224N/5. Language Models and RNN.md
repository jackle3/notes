---
Week: Week 3
---
# Lecture Plan

![[attachments/Untitled 202.png|Untitled 202.png]]

# Modern Neural Networks

- These new networks, especially language models, are enormous. They have billions of parameters now.

![[attachments/Untitled 1 165.png|Untitled 1 165.png]]

- There are a lot of tricks that are necessary in order to get these large models to work.

## Regularization

- Since we have so many parameters, we need to do regularizaztion to prevent overfitting.

![[attachments/Untitled 2 164.png|Untitled 2 164.png]]

- It’s okay if our models overfit the training data, **as long as test error keeps going down**

## Dropout

- Randomly set some activations of the neural network to be zero. Since its random, each training iteration will be different, with different weights zeroed out.
    - Also, never drop out the bias term

![[attachments/Untitled 3 161.png|Untitled 3 161.png]]

- At test time, there is **no dropout**. We only do dropout during training.
    - Since we had dropout during training, our parameters will be shrunk. We need to scale it back up, so we multiply weights by $1 - p$﻿

![[attachments/Untitled 4 156.png|Untitled 4 156.png]]

- It prevents feature co-adaptation (a form of overfitting).
    - Ensures that the activations are not dependent on each other. Makes them robust and encode independent features.

## Vectorization

- The last two ideas were statistical improvements. This one is a computational improvement.

![[attachments/Untitled 5 153.png|Untitled 5 153.png]]

## Parameter Initialization

- The initial values of parameters are also important because they may affect how your model optimizes, especially when its not convex.

![[attachments/Untitled 6 151.png|Untitled 6 151.png]]

## Optimizers

- There are a lot of hyperparameters in our optimizers. Instead of manually tuning learning rates, we can use adaptive optimizers that tune it as it trains.

![[attachments/Untitled 7 147.png|Untitled 7 147.png]]

# Language Modeling

- Language models are essentially just a stochastic autocomplete system.
    
    ![[attachments/Untitled 8 138.png|Untitled 8 138.png]]
    
    - More specifically, this is an autoregressive language model because it builds sentences from left to right, predicting the next word.
- You can also think of language models as a system that **assigns a probability to a piece of text** that exists.
    
    ![[attachments/Untitled 9 134.png|Untitled 9 134.png]]
    
    - The left side of the equation is the joint probability over sequences (of this text).
    - The right side is a factorization using the chain rule. We predict each word $i$﻿ given every word that **came before it.**

# n-gram Language Models

![[attachments/Untitled 10 128.png|Untitled 10 128.png]]

- The idea behind this is very similar to word embeddings.
    - It uses statistics from data of how often different words appear together to make predictions about what words comes next.
    - More specifically, we use those counts to estimate the probability of an n-gram.
- First, we assume that **the word we’re predicting only depends on the last n-1 words**
    
    ![[attachments/Untitled 11 123.png|Untitled 11 123.png]]
    
    - It’s Markov because it depends on a finite chunk of the past, and nothing else.

## Example

- First we make the markov assumption, discarding everything that it doesn’t depend on

![[attachments/Untitled 12 120.png|Untitled 12 120.png]]

- Then we use counting to compute the probabilities of a word $w$﻿ following this.

![[attachments/Untitled 13 111.png|Untitled 13 111.png]]

- The issue with n-grams is that we discarded some context that might’ve been necessary.
    - Since we used a 4-gram, we discarded proctors. This told us that “books” appearing was more likely, but “exams” would actually be more likely considering that there is a proctor.

## Sparsity Problems

![[attachments/Untitled 14 98.png|Untitled 14 98.png]]

- For sparsity problem 1, we can use smoothing to fix the issue.
- For sparsity problem 2, we can’t really fix it while keeping our infrastructure as is because the denominator is now zero.
    - To fix, we have to use backoff. Instead of using a 5-gram, back off to a 4-gram, etc

## Storage Problems

- It’s hard to have $n$﻿ bigger than 5 because of storage and data limitations

![[attachments/Untitled 15 93.png|Untitled 15 93.png]]

## In Practice

- You can build n-gram language models using a pre-collected corpus very quickly.

![[attachments/Untitled 16 87.png|Untitled 16 87.png]]

- This can also be used to **generate text** by taking a sample of our probability distribution. Below is an example of a trigram model. **We create a trigram conditioning on a bigram.**

![[attachments/Untitled 17 81.png|Untitled 17 81.png]]

![[attachments/Untitled 18 74.png|Untitled 18 74.png]]

  

![[attachments/Untitled 19 66.png|Untitled 19 66.png]]

- The issue with generating text like this is that it’s incoherent.

![[attachments/Untitled 20 62.png|Untitled 20 62.png]]

# Fixed-window Neural Language Model

![[attachments/Untitled 21 56.png|Untitled 21 56.png]]

- Similar to before, we take a window and discard all the words outside the window.

![[attachments/Untitled 22 52.png|Untitled 22 52.png]]

- Then we take our tokens and represent them as one-hot vectors. From there, we can learn the next word using a hidden layer.
    - Instead of using the one-hot vectors directly, we will take our words, look them up in a table, and use the corresponding word-embeddings. **This prevents issues with sparsity** since our word vectors are dense embeddings.

![[attachments/Untitled 23 48.png|Untitled 23 48.png]]

- Instead of outputting a classification prediction like we’ve done before, we output a probability distribution over words.

## Advantages and Disadvantages

![[attachments/Untitled 24 44.png|Untitled 24 44.png]]

- The window can never be large enough → because every single position (or word in the window) has its own parameters, so increase the window adds many more parameters.
    - Since every word and position **is learned separately** with different parameters, we can’t learn the fact that seeing a word like “proctor” anywhere in the sentence increases the chances of “exam” being the output.
- Basically, this is learning separate parameters for every position in the window. We want to be able to share parameters between positions so that we can better learn contextual meaning.

# Recurrent Neural Networks

- We repeatedly apply weights $W$﻿ as we move from one position to the next.

![[attachments/Untitled 25 39.png|Untitled 25 39.png]]

- Notice the position of the **hidden states**
    - Before, it was passing information upwards to directly make a prediction. It’s input was the weights applied to the entire window.
    - Now, it’s like a compressed summary of everything that has happening in the past, where $W$﻿ is applied between each hidden state.

## Example

- Read this example from the bottom to the top.
- $e^{(t)}$﻿ is a word vector → it’s the embedding of the word at time step $t$﻿ (word $x^{(t)}$﻿)
- $W_e$﻿ is a matrix → weights for the word embedding to pass it into the current hidden state
- $W_h$﻿ is a matrix → weights for the previous hidden state
- $h^{(t)}$﻿ is a hidden state → represents the current state, summary of everything before time $t$﻿

![[attachments/Untitled 26 34.png|Untitled 26 34.png]]

- We start with our words. From there, we look up our word embeddings to get $e^{(t)}$﻿
- From here, we take that embeddings and process it.
    - We take the first position’s embedding $e^{(1)}$﻿. We use that embedding, as the initial hidden state $h^{(0)}$﻿, and apply a single-layer neural network to get the next hidden state $h^{(1)}$﻿
        - This single-layer neural network is the sigmoid equation above.
- Then, we pass that hidden state forwards to create the next hidden state using the next word embedding.
    - **Notice that we’re using the same weights throughout the time steps.**
        - We’re using $W_h$﻿ for all the hidden layers, and $W_e$﻿ for all the embeddings.
    - The number of parameters is **independent of the sequence length**
- Because it’s the same weights, we can continue for arbitrary length input sequences.
    - At the end, we use the last hidden state to make our prediction. Remember that the hidden state is the summary of everything that happened before, so the **last hidden state is the summary of the input sequence.**

## Advantages and Disadvantages

- RNN Advantages
    - Can process any length input
    - Computation for step t can (in theory) use information from many steps back
    - Model size doesn’t increase for longer input context
    - Same weights applied on every timestep, so there is symmetry in how inputs are processed
- RNN Disadvantages
    - Recurrent computation is slow
        - You can’t really vectorize. You have to loop through each time step one by one.
    - In practice, difficult to access information from many steps back
        - $W_h$﻿ needs to be very finely tuned to keep magnitudes near one, so that information doesn’t explode or vanishes.
        - From $h^{(1)}$﻿ to $h^{(4)}$﻿, we are applying $W_h$﻿ three times. This means if $W_h$﻿ increases our magnitudes too much, the info from $h^{(1)}$﻿ might overpower the info from later times.

# Training an RNN Language Model

![[attachments/Untitled 27 30.png|Untitled 27 30.png]]

- Loss is found by comparing the predicted probability distribution by the true distribution.
    - Loss at step $i$﻿ is the negative log probability of the word at step $i + 1$﻿

![[attachments/Untitled 28 28.png|Untitled 28 28.png]]

![[attachments/Untitled 29 27.png|Untitled 29 27.png]]

## Backprop for RNNs

![[attachments/Untitled 30 27.png|Untitled 30 27.png]]

- This is because $W_h$﻿ appears at multiple time steps. Therefore, continue the gradient at each time step and sum them all up.
    - This is true because of the multivariable chain rule.
        
        ![[attachments/Untitled 31 23.png|Untitled 31 23.png]]
        
        - Think of $f$﻿ as the RNN, and $t$﻿ as the weights $W_h$﻿. Just take the derivative against each instance of $t$﻿, and sum it up.

![[attachments/Untitled 32 23.png|Untitled 32 23.png]]

- We start from the output, and just go backwards through each timestep.

# Generating with an RNN Language Model

- At every time step, we sample a word using the hidden state. The sampled output becomes the input for the next time step.
- We start the sentence using a starter token, such as the `<s>` token show below.

![[attachments/Untitled 33 22.png|Untitled 33 22.png]]

- Once we sample the token `</s>` , that’s the end of our sentence.

## Generating with style

- The sampled output will be dependent on the corpus that you train it on. If you train it using data written with a certain style, the output will have that same style.

![[attachments/Untitled 34 18.png|Untitled 34 18.png]]

- The RNN-LM above was trained with text from a book which has dialogue, so it generated diaglogue text.
    - Notice that the text still doesn’t make much sense. It’s quite incoherent.

## Character-level RNN-LM

- Same as before, but it predicts what character comes next.

![[attachments/Untitled 35 16.png|Untitled 35 16.png]]

- In this example, it’s predicting the color names of paint based on the RGB codes.

# Perplexity: Evaluating Language Models

- **Perplexity** is the number of guesses that you’ll need to predict the actual word.
    - If the perplexity is 1, that’s equal to the amount of uncertainty in a 1-sided dice.
        - You’ll need 1 guess to guess the output of a 1-sided dice.
    - If the perplexity is 10, that’s equal to the uncertainty in a 10-sided dice.
        - You’ll need roughly 10 guesses the guess the output of a 10-sided dice.

![[attachments/Untitled 36 15.png|Untitled 36 15.png]]

- The lower the perplexity → the fewer guesses you need → the better the model

![[attachments/Untitled 37 15.png|Untitled 37 15.png]]

## Historical Performance

- Language models have been improving a lot in recent years.

![[attachments/Untitled 38 14.png|Untitled 38 14.png]]

- Large language models have perplexity in the 10s to 20s, depending on the corpus.

# Summary

![[attachments/Untitled 39 13.png|Untitled 39 13.png]]

- RNNs are a type of language model