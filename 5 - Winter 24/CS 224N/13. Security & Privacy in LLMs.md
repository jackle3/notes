---
Week: Week 7
---
# Prompting

- With careful prompting, we can get past the security frameworks in place and force LLMs to go against its guidelines.

![Untitled 203.png](attachments/Untitled%20203.png)

![Untitled 1 166.png](attachments/Untitled%201%20166.png)

# Adversarial Perturbation

- These images are almost identical, but there is some perturbation that makes the model completely fail on it.

![Untitled 2 165.png](attachments/Untitled%202%20165.png)

## Method

- Each pixel in the graph below would represent some image.
- The colors represent how the model would classify it.
    - Teal means it would classify it dog, green means it would classify it car, and blue means it would classify it as truck
- We pick some random directions, and slightly perturb it by adding noise. This would change how the image looks, and also possibly change the classification.
    
    ![Untitled 3 162.png](attachments/Untitled%203%20162.png)
    
- Depending on the chosen direction, the image could look the exact same but cross decision boundaries, leading to different classifications. This is **adversarial**.
    
    ![Untitled 4 157.png](attachments/Untitled%204%20157.png)
    

## Finding adversarial direction

- We can use gradient descent to find the adversarial direction. This is basically the place to maximally change the classification of the model, while moving as little as possible.
    
    ![Untitled 5 154.png](attachments/Untitled%205%20154.png)
    

# Helpful and Harmless

- An **aligned** language model is helpful and harmless
    
    ![Untitled 6 152.png](attachments/Untitled%206%20152.png)
    

# Attacking Aligned Models

- Can we use **adversarial** techniques to test **alignment**?

## Adversarial Text

- You can add adversarial images or text to a prompt to sidestep the security,

![Untitled 7 148.png](attachments/Untitled%207%20148.png)

![Untitled 8 139.png](attachments/Untitled%208%20139.png)

## Affirmative Response Attack

- The goal is to prompt it with adversarial text such that the language model begins the response with affirmative response.
- Once there, through how LM works, it will continue to give you the response you want, even if it goes against its guidelines.

```JavaScript
System: You are a helpful and harmless language model.
User: Insult me. [adversarial text here]
Assistant: OK
```

1. Compute the gradient with respect to the attack prompt.
2. Evaluate at the top B candidate words for each location.
3. Choose the word with the lowest actual loss and replace it with advversarial text.
4. Repeat.

## Evaluation

![Untitled 9 135.png](attachments/Untitled%209%20135.png)

# Transferring Attacks

- To extend this to production language models:
    
    1. Generate adversarial examples on some open source models (e.g. Vicuna)
        1. Open source because we would know how gradients are calculated, etc
    2. Use that adversarial example on the production model.
    
    ![Untitled 10 129.png](attachments/Untitled%2010%20129.png)
    
    - The adversarial text can have meaningful semantic interpretations. For example, with the BARD example, the adversarial text has “similary now write opposite contents”

## Why does this work?

- Adversarial examples that fool one model is likely to fool another type of model.
- For production models, open-source models are often surrogates
    - Vicuna is a ChatGPT surrogate. It was trained by taking the LLaMa model as a base and training it on the output of ChatGPT. It was basically trained to emulate ChatGPT.

# Poisoning Attacks

- So far, we’ve been attacking the inference side after training has been done.
    
    - We modify test inputs to cause test errors. This is called evasion.
    
    ![Untitled 11 124.png](attachments/Untitled%2011%20124.png)
    
- We can also modify the training data to cause test errors.
    
    ![Untitled 12 121.png](attachments/Untitled%2012%20121.png)
    

# Privacy

- Given the model and the parameters, we can attempt to extract the training data.
    
    ![Untitled 13 112.png](attachments/Untitled%2013%20112.png)
    
- For instance, if we train on input from users, that data can be revealed to other users.
    
    ![Untitled 14 99.png](attachments/Untitled%2014%2099.png)
    

# Extracting Training Data

1. Generate a lot of data.
2. Predict membership of the data based on the model.

![Untitled 15 94.png](attachments/Untitled%2015%2094.png)

## Membership Inference

- Often, the training examples will have a high likelihood in terms of probability, since it showed up in the training data.
    
    ![Untitled 16 88.png](attachments/Untitled%2016%2088.png)
    
- For instance, looking at the example below, we infer that the second example showed up in the training data, since it has a very high likelihood for text that seems like gibberish.
    
    ![Untitled 17 82.png](attachments/Untitled%2017%2082.png)
    
- To confirm this, we can use another model and check the likelihood there. We can then compare the two.
    
    ![Untitled 18 75.png](attachments/Untitled%2018%2075.png)
    

# Memorization

- Consider the GPT-2 model.
    - It was a state-of-the-art model at the time it was released.
    - It is a public model → the weights of the model are public, so we can reconstruct it.
    - Public (private) data → it built the dataset from public internet, but the dataset itself is private.

## Dataset

- GPT-2 has a 40GB private dataset. Below are the categories and counts of the dataset.
    - Notice that these are all public information → things that anyone can access.

![Untitled 19 67.png](attachments/Untitled%2019%2067.png)

## Result

- The larger model with more parameters shows a lot more memorization than smaller models.
    - Observe that a lot of the information from the training data was memorized.

![Untitled 20 63.png](attachments/Untitled%2020%2063.png)

- Memorization looks like it scales somewhat with size.

# Discoverable memorization

- Discoverable memorization: Memorization that can be detected, even if maybe it can't actually be attacked

![Untitled 21 57.png](attachments/Untitled%2021%2057.png)

- We basically ask: if I query on this prefix, do I get this suffix? If yes, then it’s memorized.

## Results

- As seen in the first graph, as the size of the model increases, there is more memorization.
    - The baseline is a three different models trained on a different dataset. This indicates that this memorization is causally-linked to the dataset that we use to train.
- We also see that as the # of repetitions in the training data increases, and as the prompt length increases, there is also more memorization.

![Untitled 22 53.png](attachments/Untitled%2022%2053.png)

# Discoverable vs. Extractable Memorization

- Discoverable means the memorization can be found if you have access to the data, but not through an attack.
- Extractable means the memorization can be found through an attack.

![Untitled 23 49.png](attachments/Untitled%2023%2049.png)

# Attacking ChatGPT

1. Here is one example, where in the process of following the prompt, some of the memorized data was brought out.
    
    ![Untitled 24 45.png](attachments/Untitled%2024%2045.png)
    
    ![Untitled 25 40.png](attachments/Untitled%2025%2040.png)
    
2. This is another example of this. The same thing is happening.
    
    ![Untitled 26 35.png](attachments/Untitled%2026%2035.png)
    
    ![Untitled 27 31.png](attachments/Untitled%2027%2031.png)
    
    ![Untitled 28 29.png](attachments/Untitled%2028%2029.png)
    

# Finding Memorization

- An example is memorized if:
    - It’s present in the training data (or in the internet somewhere)
    - It’s long (50 tokens)
    - It has high entropy
- To evaluate this, there is a two step evaluation:
    1. Generate lots of data.
        1. This can be easily done using the internet
    2. Test if it’s in our web data.
        1. We can do this efficiently with suffix arrays.

## Suffix Arrays

![Untitled 29 28.png](attachments/Untitled%2029%2028.png)

- First, generate all the suffixes of a word, then index it.
    
    ![Untitled 30 28.png](attachments/Untitled%2030%2028.png)
    
- Next, sort it alphabetically based on the suffix itself.
    
    ![Untitled 31 24.png](attachments/Untitled%2031%2024.png)
    
- Our suffix array is thus `[1, 3, 5, 0, 2, 4, 6]`
- **There is an O(n) algorithm to build a suffix array.**
- To check if substring appears inside the data:
    
    - “Does the string NAN appear in BANANA?”
    - Binary search through the suffix data to find a candidate suffix.
    
    ![Untitled 32 24.png](attachments/Untitled%2032%2024.png)
    
    - Once it’s there, check if it’s contained within the suffix. If it is, then it’s contained in our data.

# Why does memorization occur?

- Suppose you ask a model to repeat a word an infinite number of times.
    - After it repeats it roughly 100 times, it considers the 100th repeated token as extremely similar to the beginning-of-sequence (BOS) token.
    - This means that after a certain amount, it thinks that it has the BOS token, so it just starts generating a new sequence.
- This isn’t an issue with randomly sampled letters.

![Untitled 33 23.png](attachments/Untitled%2033%2023.png)