---
Week: Week 4
---
# Lecture Plan

![[attachments/Untitled 201.png|Untitled 201.png]]

# Recap of NMT

![[attachments/Untitled 1 164.png|Untitled 1 164.png]]

# Issues with recurrent models

## Bottleneck Problem

![[attachments/Untitled 2 163.png|Untitled 2 163.png]]

- The information bottleneck is the blue state in the picture above.
    - All the information from the encoder has to pass through the blue to get to the decoder.
    - Encoded information for the start of the source sentence will be hard to bring over to the decoder.

![[attachments/Untitled 3 160.png|Untitled 3 160.png]]

- We’re basically encoding the entire source sentence into a single vector, which leads to an information bottleneck.

## Linear interaction distance

![[attachments/Untitled 4 155.png|Untitled 4 155.png]]

- Solve interaction distance:
    - Bring long-range information closer so that we can use it while decoding.

## Lack of parallelizability

- An RNN is going from left to right. At each step, we have a single unparallelizable operation, so each full pass has $O(\text{sequence length})$﻿ unparallel operatiosn.

![[attachments/Untitled 5 152.png|Untitled 5 152.png]]

- We have to wait for previous time steps in order to compute future time steps.
- Solve parallelization
    - We want to parallelize as many operations as possible.

# Attention

![[attachments/Untitled 6 150.png|Untitled 6 150.png]]

- Implement a long range connection from the encoder (or whatever past info we want) to where we’re using that info (e.g. to generate tokens)
- Given a set of vector `values` , and a vector `query` , attention is a technique to compute a weighted sum of the values, dependent on the query
    - We sometimes say that the `query` attends to the `values`
    - For example, in the seq2seq + attention model, each decoder hidden state (query) attends to all the encoder hidden states (values).

![[attachments/Untitled 7 146.png|Untitled 7 146.png]]

## Mean-pooling

- Average all of the hidden states and allow that to go into the classifier or decoder.

![[attachments/Untitled 8 137.png|Untitled 8 137.png]]

## Weighted-averaging

- Attention is similar to mean-pooling, but instead of taking just the average, it takes a weighted average.

![[attachments/Untitled 9 133.png|Untitled 9 133.png]]

- This allows you to do soft lookups, very similar to a lookup table.
- The key itself is a vector (a function of the previous token), and similarities for queries are inner products.

# Sequence-to-sequence NMT with attention

- We create attention scores from hidden states and directly connect it to the decoder
    - We take the `start` state of the decoder and make a `**query**` vector.
    - Then, we generate **`keys`** from the hidden state of the encoder and take the dot product. This dot product is the attention score.
    - This dot product tells us how similar or relevant the first position of the input to what we want to generate next.
- We repeat this process with all of the hidden states, taking dot products and generating attention scores.
    
    - Each of these are similarities, telling the relevance of the hidden state position for the next token.
    
    ![[attachments/Untitled 10 127.png|Untitled 10 127.png]]
    
- Then, we take these dot products (attention scores) between the query and the key and make a softmax distribution.
    
    - More formally, if we had a query vector $q \in \R^d$﻿ (representing the decoder state)
    - and key vectors $\{ k_1, \dots, k_n\}, k_i \in \R^d$﻿ (generated from encoder hidden states)
    - the attention weights (softmax of attention scores) would be:
        
        ![[attachments/Untitled 11 122.png|Untitled 11 122.png]]
        
    
    ![[attachments/Untitled 12 119.png|Untitled 12 119.png]]
    
- We use this distribution to take a weighted average of our `values` . In this case, the values are the hidden states, similar to mean-pooling.
    
    - More formally, our attention output is $c \in \R^d$﻿, an average over the value vectors weighted with respect to $\alpha$﻿
        
        ![[attachments/Untitled 13 110.png|Untitled 13 110.png]]
        
    
    ![[attachments/Untitled 14 97.png|Untitled 14 97.png]]
    
- We then pass this attention output into the decoder RNN to generate the sequence.
    
    - The $\hat{y}_1$﻿ is a distribution over words, which we can then sample to get our generated token.
    
    ![[attachments/Untitled 15 92.png|Untitled 15 92.png]]
    
- We can sometimes use the output from the attention to feed it into the decoder to generate a new **query vector**, in addition to the regular input.
    
    ![[attachments/Untitled 16 86.png|Untitled 16 86.png]]
    
- The key vectors (from the encoder) are fixed, but the query vectors (from the decoder) are changing, with each decoder timestep having a different query vector.
    
    - The attention distribution will change with each timestep, since the attention scores are the dot product between query vector and key vector.
    
    ![[attachments/Untitled 17 80.png|Untitled 17 80.png]]
    
- Every position is independent of other positions, since it’s now using a direct connection.

# Attention: in equations

- First, we’ve already computed our encoder hidden states. Then, we take attention scores by taking the dot product of the decoder hidden state with each encoder hidden state.
    
    ![[attachments/Untitled 18 73.png|Untitled 18 73.png]]
    
- Then, we generate a softmax distribution using the attention scores.
    
    ![[attachments/Untitled 19 65.png|Untitled 19 65.png]]
    
- Then, we use the distribution to take a weighted sum of the encoder hidden states.
    
    ![[attachments/Untitled 20 61.png|Untitled 20 61.png]]
    
- Finally, concatenate the attention output with the decoder hidden state.
    
    ![[attachments/Untitled 21 55.png|Untitled 21 55.png]]
    

# Attention advantages

- Recall that the keys for the encoder don’t change for each query.

![[attachments/Untitled 22 51.png|Untitled 22 51.png]]

- We’ve made it so that instead of going left to right, we go bottom to up. This means our only limitation is the number of layers, not the length of the RNN.
    - The horizontal axis above is the time steps.

![[attachments/Untitled 23 47.png|Untitled 23 47.png]]

# Attention variants

![[attachments/Untitled 24 43.png|Untitled 24 43.png]]

![[attachments/Untitled 25 38.png|Untitled 25 38.png]]

# Do we need recurrence?

![[attachments/Untitled 26 33.png|Untitled 26 33.png]]

- We lose positional-information when we remove the RNN. However, having attention in an RNN makes it so that each position is independent anyways.

# Self and cross attention

- The attention shown so far was cross attention. It was attention going **across** from the decoder back to the encoder. It’s going across to the two models.
    
    ![[attachments/Untitled 27 29.png|Untitled 27 29.png]]
    
- **Self-attention** is when the decoder only pays attention to itself and its own generated previous outputs.
    
    ![[attachments/Untitled 28 27.png|Untitled 28 27.png]]
    

## Example

- Instead of having some encoder-decoder scheme, just simply look back at the text generated so far.

![[attachments/Untitled 29 26.png|Untitled 29 26.png]]

## Keys, queries, values from same sequence

- The $w$﻿ are one-hot encodings for the words, and the embedding matrix turns that into a dense representation.
- We generate the queries, keys, and values all from the same embedding sequence.
    
    - The keys are the weights (the importance of the position), and the value is the content that you want to pass through to the next layer.
    - The attention output is computed using these generated things.
    
    ![[attachments/Untitled 30 26.png|Untitled 30 26.png]]
    

## Problem 1: Sequence order

- One issue with self-attention is that it doesn’t have an inherent of order in the sentences.
- The fix: each position in the sentence gets its own position vector, which we can add to our input to get a positioned embedding.

![[attachments/Untitled 31 22.png|Untitled 31 22.png]]

### Sine position representation

- This is one way to create the position vectors. It will change depending on which position it is.

![[attachments/Untitled 32 22.png|Untitled 32 22.png]]

### Learning position representations

- This allows us to learn position vectors from scratch, which will likely better fit the data.
- The con is that we don’t know anything about sequences longer than $n$﻿

![[attachments/Untitled 33 21.png|Untitled 33 21.png]]

## Problem 2: Non-linearities

![[attachments/Untitled 34 17.png|Untitled 34 17.png]]

- We can just keep adding layers to this. In practice, we can add around 12 to 24 layers.

## Problem 3: looking into the future → mask

- Attention “looks at” all of the tokens at once. However, we know decoders and models that generate text can’t look at the future — it can only look at the past.
- Decoders generate tokens one by one. We want to replicate this during training, so we make sure decoders can’t look at the future.
    - If we train on a model that can look at the future, it’ll be relying on information that’s not available during test time.
- To fix this, we can mask the future.

![[attachments/Untitled 35 15.png|Untitled 35 15.png]]

- Changing the set of keys and queries is inefficient because we’re recomputing the attention map every timestep, since the attention structure changes.
- The fix for this to just compute all of the attention, including everything in the future. Then, mask out the future by adding $-\infty$﻿ to everything you’re not allowed to look at.
    - Having a $-\infty$﻿ attention score means that after passing into softmax, it will get zeroed out.

## Summary of problems

![[attachments/Untitled 36 14.png|Untitled 36 14.png]]

# Building blocks of a transformer

- The core of a transformer is the self-attention block.

![[attachments/Untitled 37 14.png|Untitled 37 14.png]]

- Notice that the masked self-attention layer has three inputs. These are the queries, keys, and values from the embeddings.
- The feed-forward takes the self-attention and processes it in a way that makes it useful for the next layer.
    - The input to the feed forward is the attention weights times the values → the weighted average of the values.
- Each block consistents of a self-attention and a feed forward.
    - The block can be repeated multiple times. It can be stacked between 12 and 24 times.
- At the very top is a linear layer and a softmax to produce the output probability distribution.

# Multi-head attention

![[attachments/Untitled 38 13.png|Untitled 38 13.png]]

- Instead of having 1 attention head that deals with everything, split it up so that each head deals with one part of generating sequences.

![[attachments/Untitled 39 12.png|Untitled 39 12.png]]

- We can have one attention head attending to entities, and another one to attend to syntax.

## Sequence-stacked form of attention

- We first stack our input vectors, where $n$﻿ is the sequence length and $d$﻿ is the embedding size.
    - Then we multiply to get three different matrices for our keys, queries, and values, respectively.
    - $K, Q, V \in \R^{d \times d}$﻿

![[attachments/Untitled 40 12.png|Untitled 40 12.png]]

- The output is visualized below. At the end, we do softmax to get probabilities, then take the weighted average of the values $XV$﻿.

![[attachments/Untitled 41 9.png|Untitled 41 9.png]]

## Multi-head with matrices

- We have the same amount of total computation, we just now have more heads doing different things.

![[attachments/Untitled 42 9.png|Untitled 42 9.png]]

- To make this efficient, we use reshaping to make the number of heads into its own dimension, similar to batch sizes.

![[attachments/Untitled 43 7.png|Untitled 43 7.png]]

- Conceptually speaking, in the diagram above, there are three different $Q$﻿ matrices, three different $K$﻿ matrices, and three different $V$﻿ matrices, one for each head.

## Scaled dot product

- The attention scores are scaled down by a function of the dimension and the number of heads, to stop the scores from being too large.

![[attachments/Untitled 44 7.png|Untitled 44 7.png]]

# Add & Norm

- These tricks allow transformers to be very deep, because it lets the gradient flows more nicely.

![[attachments/Untitled 45 7.png|Untitled 45 7.png]]

## Residual connections

![[attachments/Untitled 46 7.png|Untitled 46 7.png]]

## Layer normalization

- This basically just standardize the activations across all the dimensions for each layer.

![[attachments/Untitled 47 7.png|Untitled 47 7.png]]

- This allows you to easily set the scale of all the activations at once, preventing the issue of uninformative learning once activations near zero.

# Transformers

- The very bottom is identical. The attention layer is a bit more advanced, and theres some extra layers to make training easier.

## Decoder

- Constrains to unidirectional context, as for language models

![[attachments/Untitled 48 6.png|Untitled 48 6.png]]

## Encoder

- Allows for bidirectional context. The only difference is that we remove the masking in self-attention, allowing it to “see the future” and the full context.

![[attachments/Untitled 49 6.png|Untitled 49 6.png]]

## Encoder-decoder

- This is a sequence-to-sequence model, similar to the machine translation system from before.

![[attachments/Untitled 50 5.png|Untitled 50 5.png]]

- The encoder is the left side. This is just a regular transformer decoder.
- The decoder is the right side. It uses cross-attention to take information from the encoder.

### Cross-attention

- The queries come from the decoder. The keys and values come from the encoder. These are then used to compute attention scores.

![[attachments/Untitled 51 4.png|Untitled 51 4.png]]

# Advantage of Transformers

![[attachments/Untitled 52 4.png|Untitled 52 4.png]]