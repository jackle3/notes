---
Week: Week 1
---
# Grading

![[attachments/Untitled 200.png|Untitled 200.png]]

# Word

![[attachments/Untitled 1 163.png|Untitled 1 163.png]]

- Tree is the symbol, and the emojis are the idea or thing that it signifies.

# Word Vector

- **Distributional semantics**: A word’s meaning is given by the words that frequently appear close-by

![[attachments/Untitled 2 162.png|Untitled 2 162.png]]

![[attachments/Untitled 3 159.png|Untitled 3 159.png]]

# Word2vec

![[attachments/Untitled 4 154.png|Untitled 4 154.png]]

- Given a word/sentence $w(t)$﻿, try to predict the context.
	- E.g. Suppose the sentence “I really like Palo Alto”
		- Use the word “like” to predict every other word in the sentence, with a size 2 window.
- Every word has two vector representations:
	- When they are in the center position
	- When they are a context for another word

![[attachments/Untitled 5 151.png|Untitled 5 151.png]]

## Objective Function

![[attachments/Untitled 6 149.png|Untitled 6 149.png]]

- The likelihood is the product, for every center, of the product of its context words probabilities.

![[attachments/Untitled 7 145.png|Untitled 7 145.png]]

![[attachments/Untitled 8 136.png|Untitled 8 136.png]]

- Our goal is to increase $u_o^T v_c$﻿, the similarity of the center $c$﻿ and the context $o$﻿

## Training

![[attachments/Untitled 9 132.png|Untitled 9 132.png]]

## Derivative of Likelihood

![[attachments/Untitled 10 126.png|Untitled 10 126.png]]

![[attachments/Untitled 11 121.png|Untitled 11 121.png]]

- We focus on $v_c$﻿ the vector for when $c$﻿ is the center word.

![[attachments/Untitled 12 118.png|Untitled 12 118.png]]

![[attachments/Untitled 13 109.png|Untitled 13 109.png]]

![[attachments/Untitled 14 96.png|Untitled 14 96.png]]

- The first term, $u_o$﻿ is the observed context, while the summation is the expectation of the context.
