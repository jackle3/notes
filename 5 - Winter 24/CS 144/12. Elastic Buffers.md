---
Details: Drain rate > arrival rate
Week: Week 8
---
# Bits

- Our model of the world right now:
    1. We have sources of information, such as you typing an email, taking a pic, etc.
    2. This source of information goes into an encoder that transmits our information.
        1. The encoder transmits **bits** across the channel. It sends bits in the form of voltages over the physical link.
        2. The encoder is like an ethernet sender
    3. This encoder sends data across a channel or a link to a decoder.
        1. The decoder is like an ethernet receiver.
    4. This decoder then sends it to sink, the user of the receiver.

![Untitled 136.png](attachments/Untitled%20136.png)

- The bits are an abstraction that can contain information from any source.
    - Any piece of information can be encapsulated in the form of bits.
    - This is known as the **Source-channel seapration theorem.**
        - You can separate the encoding from the source and the encoding from the encoder to the channel without losing any information.

![Untitled 1 100.png](attachments/Untitled%201%20100.png)

# Network Interface

- The network interface is a bridge.
    - The lower level abstractions are
        - `transmit(frame)` to send an ethernet frame
        - `recv_frame(frame)` to receive an ethernet frame.
    - The higher level abstractions are
        - `send_datagram(dgram)` to send a datagram through the interface
        - `datagrams_received()` to return a queue of the datagrams received at the interface.
- The network interface uses/consumes the lower-level abstractions to provide the higher-level abstractions.

![Untitled 2 100.png](attachments/Untitled%202%20100.png)

# Clock

- Suppose we have a sender and a receiver with a channel in between.
    - The sender is sending at a rate of 10 Mbit/s → this means the sender has an internal clock at a rate of 10MHz, where every time the clock fires a bit is sent.
        - The clock fires being 10MHz means it fires once every `1e-7` seconds or 100 nanoseconds. `Hz` means once per second.
    - The receiver will receive data at the same rate → receives at a rate of 10 Mbit/s with a clock of 10MHz.

![Untitled 3 99.png](attachments/Untitled%203%2099.png)

- **The issue is that the sender and receive may have different clocks, so they might not agree on what 10MHz actually is.**
    
    ![Untitled 4 95.png](attachments/Untitled%204%2095.png)
    
    - Suppose the CPU actually had a clock speed of `10.01 Mhz`.
        
        - 10.01 Mhz = 10Mhz + 1000 ppm → 1000 parts per million is 1e3 / 1e6 = 1e-3
        
        ![Untitled 5 95.png](attachments/Untitled%205%2095.png)
        
- There is now an issue at the receiver. It receives at 10 MHz, but the CPU (the user) is asking for bits at 10.01 MHz.
    - The receiver’s network interface has to translate between what the sender thinks is 10Mhz and what its CPU thinks is 10MHz.

## Parts-per-million

- If we had 1000, adding 1000 ppm is the same as adding 1 part per thousand.

![Untitled 6 94.png](attachments/Untitled%206%2094.png)

- If we had 100 GHz, and we wanted to go up by 1000 ppm, it would be 100.1 GHz.
    - 1000 ppm is `100Ghz * 1000/1000000 = 100Ghz * 1e3/1e6 = 100Ghz * 1e-3 = 0.1 Ghz`

## Clock tolerance

- Difference between drain rate to processor and arrival rate from sender.
- Essentially the maximum clock skew

# Elasticity Buffer

- This is a queue at the receiver → bridges the slower bits coming in (e.g. sender at 10 MHz) and the slightly faster bits coming out (e.g. CPU at 10.01 MHz).
    - A buffer between the receiver and the processor that is reading from the receiver
        - Similar to the data structure for storing bits you have in your ByteStream
- To do so, change the `recv_bit()` function so that when the CPU gets bits from the receiver, the function outputs either a bool (for the bit) or “idle between packets”

## Don’t give bits instantly

- If the receiver has a bit, and the CPU requests it, we shouldn’t give it to the CPU instantly.
    - Suppose the sender is currently sending a packet, sending one bit every 100ns.
    - However, the CPU clock rate asks for a bit every ~99ns.
    - If we give the first bit to the CPU instantly, the next time it asks, we’ll have to give it “idle between packets”.
    - But then when the CPU asks again, we would give it the next bit we received.
        - The CPU now thinks the two bits were part of two packets, since it had to idle between them, even though they were part of one.

![Untitled 7 91.png](attachments/Untitled%207%2091.png)

## Buffering packets

- Theoretically, the receiver should buffer the entire packet first, before giving any bits to the CPU.
    - Only once the end of packet has been sent, the receiver start giving bits to CPU.

![Untitled 8 85.png](attachments/Untitled%208%2085.png)

- We’re not concerned with buffer overflow because the sender rate is slower than the CPU rate.

![Untitled 9 81.png](attachments/Untitled%209%2081.png)

- However, this is very expensive to make. It’s super hard to make a queue where you can push at 100 GHz and pop at 100.1 GHz.
    - We would probably need a very large buffer size.

## Bound packet size

- To constrain the size of the buffer, we need to know the size of the packet.
    - We want to make sure that the CPU is asking for the end of the packet just when the end of the packet arrives at the buffer.
    - This means the size of a packet must be bounded. This is why the maximum TCP segment payload is ~1400 bytes.
        - The longer the packet, the more buffer we will need to store.

## **Maximum transmission unit** (MTU)

- The MTU is the maximum size of a single packet
- Recall that our situation is:
    - `Sender (arrival) data rate` is 10 Mbit/s
    - `Maximum clock skew` is 1000 ppm → `clock (drain) data rate` is 10.01 Mbit/s
- Our goal is to get a MTU = `Max packet length` of 10000 bits, which we control using the buffer size.

## Underflow

- Happens when the arrival rate is too low compared to the drain rate, and buffer does not have enough data in it.
- The drain asks for information, but the information is not in the buffer yet, so it is forced to report idle in the middle of the packet (very bad).
- If the MTU goes up, past the maximum packet size of the buffer, then it can underflow because the buffer will be emptied before the full packet is received.

## Big buffer size

- Suppose the `buffer size` of 100 bits. This means the receiver doesn’t allow the bits to depart until we have 100 bits in the queue.
- We graph the occupancy of the queue at the receiver network interface.
    - First, it will take 10 microseconds to get 100 bits in the queue.
        - `(1s / 10 Mbit) * (100 bit) = 1e2 / 1e7 s = 1e-5 s = 10 microseconds`
    - Then, it will start draining from the queue because `drain rate > arrival rate`.
        - The drain rate is `10.01 Mbit/s - 10 Mbit/s = 0.01 Mbit/s = 10 kbit/s`
    - It will keep draining until the queue is empty.
        - `(1s / 10 kbit) * (100 bit) = 1e2 / 1e4 s = 1e-2 s = 10 milliseconds`
    - This means the buffer **will not underflow** for `10.01 ms`.
        - The total number of bits that arrived from the sender is:
            - `10 Mbit/s * 10.01ms = 1e7 * (1e-2 + 1e-5) s = 1e5 + 1e2 s = 100,100 bits`
        - This total number of bits is the maximum packet size before we have an issue, where the issue is buffer underflow.

![Untitled 10 76.png](attachments/Untitled%2010%2076.png)

## Smallest buffer size

- The smallest number of bits that the receiver buffer has to save based on the MTU, and relates to the maximum packet size.
- If we have a buffer size of `10 bits` , the maximum packet size is `10,010 bits`
    - First, it will take 1 microseconds to get 10 bits in the queue.
        - `(1s / 10 Mbit) * (10 bit) = 1e1 / 1e7 s = 1e-6 s = 1 microseconds`
    - Then, it will start draining from the queue because `drain rate > arrival rate`.
        - The drain rate is `10.01 Mbit/s - 10 Mbit/s = 0.01 Mbit/s = 10 kbit/s`
    - It will keep draining until the queue is empty.
        - `(1s / 10 kbit) * (10 bit) = 1e1 / 1e4 s = 1e-3 s = 1 milliseconds`
    - This means the buffer **will not underflow** for `1.001 ms`.
        - The total number of bits that arrived from the sender is:
            - `10 Mbit/s * 1.001ms = 1e7 * (1e-3 + 1e-6) s = 1e4 + 1e1 s = 10,010 bits`
        - This total number of bits is the maximum packet size before we have an issue, where the issue is buffer underflow.

## Relationships

![Untitled 11 74.png](attachments/Untitled%2011%2074.png)

- If the tolerance for the clock skew decreses, we can get a smaller buffer size for the same MTU.
- If the clocks were perfectly in sync, we basically would not need a buffer.
    - Save money on buffer, spend more money on clocks with smaller tolerances.
- To maximize cheapness:
    - We want to minimize the buffer size → number of bits in queue
    - We want to use a large clock tolerance → so we can buy cheap clocks
- We want the MTU to be large so that we can send more data.

# Result

- The elasticity buffer allows the sender of an Ethernet packet to send without caring about the receiver’s clock.
- This is similar to ByteStream: writer and reader of the ByteStream can operate at a different speed, and **no synchronization is needed** between the writer side and the reader side.