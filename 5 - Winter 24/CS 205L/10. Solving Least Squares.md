---
Week: Week 5
---
# Solution 1: Normal Equations

- Since $A$﻿ is tall and has full column rank, $A^T A$﻿ is SPD. This makes it easy to solve.

![[attachments/Untitled 52.png|Untitled 52.png]]

- There is a unique solution $c$﻿ that minimizes the residual $r$﻿.
    - This does not mean there is a unique solution to $Ac = b$﻿. That is only true if $r = 0$﻿.
    - If $A$﻿ is square and full rank, then $b$﻿ is always in the range of $A$﻿, meaning we $c$﻿ is the unique solution for $Ac = b$﻿.

![[attachments/Untitled 1 19.png|Untitled 1 19.png]]

## Issues with Condition Number

- The normal equations theoretically work. However, it’s not preferred since the condition number of $A^T A$﻿ is the square of the condition number of $A$﻿.
    - Suppose the old condition number was $10^4$﻿. This means you would need 4 significant digits in order to solve it.
    - The new condition number is now $10^8$﻿. You would now need 8 significant digits to solve the system using $A^T A$﻿.

![[attachments/Untitled 2 19.png|Untitled 2 19.png]]

# Understanding Least Squares

- Since $A$﻿ has full column rank, there are several rows of zeros in $\Sigma$﻿. This makes a lot of things disappear in our equation.

![[attachments/Untitled 3 19.png|Untitled 3 19.png]]

## SVD Simplification

- Recall from explanations of SVD that:
    - The columns of $U$﻿ corresponding to “nonzero” singular values form an orthonormal basis for the range (column space) of $A$﻿
    - The remaining columns of $U$﻿ form an orthonormal basis for the (unattainable) space perpendicular to the range of $A$﻿

![[attachments/Untitled 4 18.png|Untitled 4 18.png]]

# Solution 2: SVD

- Recall that orthogonal matrices have **orthonormal columns**, and their transpose is their inverse. They also **preserve inner products**.
    
    ![[attachments/Untitled 5 18.png|Untitled 5 18.png]]
    
- Using this fact, we can further simplify our least squares solution.
    
    ![[attachments/Untitled 6 17.png|Untitled 6 17.png]]
    
- We can now use $Ac = b \to U \Sigma V^T c = b \to \Sigma V^Tc = U^Tb$﻿. This means $\hat{c} = V^T c$﻿.
    
    ![[attachments/Untitled 7 17.png|Untitled 7 17.png]]
    
- If we take the norm of the residual, we’ll find that $\hat{b}_r - \hat{\Sigma} \hat{c} = 0$﻿, because $\hat{c} = V^T c = \hat{\Sigma}^{-1} \hat{b}_r$﻿
    
    ![[attachments/Untitled 8 17.png|Untitled 8 17.png]]
    
- These results tell us that the SVD approach **gives the same minimum residual least squares solution.**

# Gram-Schmidt for QR

- An important observation is that if each $s^{\hat{q}}$﻿ have A-weighted unit norms, the denominator can be dropped, simplifying our equation.

![[attachments/Untitled 9 17.png|Untitled 9 17.png]]

## QR Factorization

- For each column of a matrix $A$﻿, use it to build orthonormal columns in the matrix $Q$﻿.
    - To make it orthonormal, we’ll likely have to divide it by the norm.
    - To offset this, put that norm inside the $R$﻿ matrix, so that the unit norm operation is essentially reversed once you multiply $Q$﻿ and $R$﻿.

![[attachments/Untitled 10 17.png|Untitled 10 17.png]]

## Example

![[attachments/Untitled 11 17.png|Untitled 11 17.png]]

## Issue with Gram-Schmidt

![[attachments/Untitled 12 17.png|Untitled 12 17.png]]

# Solution 3: QR Factorization

- If we have a QR factorization, we can also use it to solve the minimum residual least squares.
    
    ![[attachments/Untitled 13 16.png|Untitled 13 16.png]]
    
- We augment the $Q$﻿ matrix with $\tilde{Q}$﻿, which contains $m- n$﻿ orthonormal columns that span the space orthogonal to the range of $Q$﻿.
    
    - This means that the matrix $\hat{Q} = (Q \quad \tilde{Q})$﻿ is orthogonal → all columns orthonormal
    
    ![[attachments/Untitled 14 15.png|Untitled 14 15.png]]
    
- Since $c$﻿ only appears in the top block, we can minimize this residual by minimizing the top block.
    
    ![[attachments/Untitled 15 15.png|Untitled 15 15.png]]
    

## Steps

- The steps are as follows:
    1. Run Gram-Schmidt (or another method on the columns of $A$﻿ to get $A = QR$﻿
    2. Then calculate $Q^T b$﻿
    3. Then solve the system $Rc = Q^T b$﻿ with back-substitution.
    4. This $c$﻿ that you find is your solution.

# Householder Transform

- **This is the preferred method for solving the least-squares problem.**
- The Housholder Transform $H$﻿ performs a reflection across a plane.
    - The norm and everything is preserved. You’ve just reflected it across the plane.
    - Generally, you don’t calculate $H$﻿. You simply use $\hat{v}$﻿ and do the dot products.

![[attachments/Untitled 16 15.png|Untitled 16 15.png]]

- We essentially do what we did with Gaussian elimination.
    - We want to use $H$﻿ to zero out elements in our column vectors.

![[attachments/Untitled 17 13.png|Untitled 17 13.png]]

## Examples

- In this first example, we do it with $a_1$﻿, so none of the entries are ignored.

![[attachments/Untitled 18 11.png|Untitled 18 11.png]]

- In this example, we do it with $a_2$﻿, so the first entry (6) gets ignored and kept.

![[attachments/Untitled 19 10.png|Untitled 19 10.png]]

## Using Householder for QR

- We progressively apply Householder transforms to zero out entries below the diagonal for each column of $A$﻿.
    - $H_1$﻿ will make it so that the first column will have one entry at the top, and zeros all below the diagonal.
    - $H_2$﻿ will do the same now. It won’t touch the first column. It’ll just zero out entries below the diagonal for the second column, and mess up the rest.
    - We then repeat this until we get an upper triangular matrix.

![[attachments/Untitled 20 10.png|Untitled 20 10.png]]

- We can relate this to QR by combining all of the Housholder transforms.
    - Recall that each Householder transform is orthogonal bc it’s just a reflection.
    - Doing a bunch of reflections is still an orthogonal process.

$A = \hat{Q} \begin{pmatrix} R \\0\end{pmatrix} = \begin{pmatrix} Q & \tilde{Q}\end{pmatrix}\begin{pmatrix} R \\0\end{pmatrix} = QR$

$\hat{Q}^T b = \begin{pmatrix}\hat{b}_Q \\ \hat{b}_{\tilde{Q}}\end{pmatrix}$

![[attachments/Untitled 21 8.png|Untitled 21 8.png]]