---
Week: Week 5
---
# Solution 1: Normal Equations

- Since $A$﻿ is tall and has full column rank, $A^T A$﻿ is SPD. This makes it easy to solve.

![Untitled 52.png](../../attachments/Untitled%2052.png)

- There is a unique solution $c$﻿ that minimizes the residual $r$﻿.
    - This does not mean there is a unique solution to $Ac = b$﻿. That is only true if $r = 0$﻿.
    - If $A$﻿ is square and full rank, then $b$﻿ is always in the range of $A$﻿, meaning we $c$﻿ is the unique solution for $Ac = b$﻿.

![Untitled 1 19.png](../../attachments/Untitled%201%2019.png)

## Issues with Condition Number

- The normal equations theoretically work. However, it’s not preferred since the condition number of $A^T A$﻿ is the square of the condition number of $A$﻿.
    - Suppose the old condition number was $10^4$﻿. This means you would need 4 significant digits in order to solve it.
    - The new condition number is now $10^8$﻿. You would now need 8 significant digits to solve the system using $A^T A$﻿.

![Untitled 2 19.png](../../attachments/Untitled%202%2019.png)

# Understanding Least Squares

- Since $A$﻿ has full column rank, there are several rows of zeros in $\Sigma$﻿. This makes a lot of things disappear in our equation.

![Untitled 3 19.png](../../attachments/Untitled%203%2019.png)

## SVD Simplification

- Recall from explanations of SVD that:
    - The columns of $U$﻿ corresponding to “nonzero” singular values form an orthonormal basis for the range (column space) of $A$﻿
    - The remaining columns of $U$﻿ form an orthonormal basis for the (unattainable) space perpendicular to the range of $A$﻿

![Untitled 4 18.png](../../attachments/Untitled%204%2018.png)

# Solution 2: SVD

- Recall that orthogonal matrices have **orthonormal columns**, and their transpose is their inverse. They also **preserve inner products**.
    
    ![Untitled 5 18.png](../../attachments/Untitled%205%2018.png)
    
- Using this fact, we can further simplify our least squares solution.
    
    ![Untitled 6 17.png](../../attachments/Untitled%206%2017.png)
    
- We can now use $Ac = b \to U \Sigma V^T c = b \to \Sigma V^Tc = U^Tb$﻿. This means $\hat{c} = V^T c$﻿.
    
    ![Untitled 7 17.png](../../attachments/Untitled%207%2017.png)
    
- If we take the norm of the residual, we’ll find that $\hat{b}_r - \hat{\Sigma} \hat{c} = 0$﻿, because $\hat{c} = V^T c = \hat{\Sigma}^{-1} \hat{b}_r$﻿
    
    ![Untitled 8 17.png](../../attachments/Untitled%208%2017.png)
    
- These results tell us that the SVD approach **gives the same minimum residual least squares solution.**

# Gram-Schmidt for QR

- An important observation is that if each $s^{\hat{q}}$﻿ have A-weighted unit norms, the denominator can be dropped, simplifying our equation.

![Untitled 9 17.png](../../attachments/Untitled%209%2017.png)

## QR Factorization

- For each column of a matrix $A$﻿, use it to build orthonormal columns in the matrix $Q$﻿.
    - To make it orthonormal, we’ll likely have to divide it by the norm.
    - To offset this, put that norm inside the $R$﻿ matrix, so that the unit norm operation is essentially reversed once you multiply $Q$﻿ and $R$﻿.

![Untitled 10 17.png](../../attachments/Untitled%2010%2017.png)

## Example

![Untitled 11 17.png](../../attachments/Untitled%2011%2017.png)

## Issue with Gram-Schmidt

![Untitled 12 17.png](../../attachments/Untitled%2012%2017.png)

# Solution 3: QR Factorization

- If we have a QR factorization, we can also use it to solve the minimum residual least squares.
    
    ![Untitled 13 16.png](../../attachments/Untitled%2013%2016.png)
    
- We augment the $Q$﻿ matrix with $\tilde{Q}$﻿, which contains $m- n$﻿ orthonormal columns that span the space orthogonal to the range of $Q$﻿.
    
    - This means that the matrix $\hat{Q} = (Q \quad \tilde{Q})$﻿ is orthogonal → all columns orthonormal
    
    ![Untitled 14 15.png](../../attachments/Untitled%2014%2015.png)
    
- Since $c$﻿ only appears in the top block, we can minimize this residual by minimizing the top block.
    
    ![Untitled 15 15.png](../../attachments/Untitled%2015%2015.png)
    

## Steps

- The steps are as follows:
    1. Run Gram-Schmidt (or another method on the columns of $A$﻿ to get $A = QR$﻿
    2. Then calculate $Q^T b$﻿
    3. Then solve the system $Rc = Q^T b$﻿ with back-substitution.
    4. This $c$﻿ that you find is your solution.

# Householder Transform

- **This is the preferred method for solving the least-squares problem.**
- The Housholder Transform $H$﻿ performs a reflection across a plane.
    - The norm and everything is preserved. You’ve just reflected it across the plane.
    - Generally, you don’t calculate $H$﻿. You simply use $\hat{v}$﻿ and do the dot products.

![Untitled 16 15.png](../../attachments/Untitled%2016%2015.png)

- We essentially do what we did with Gaussian elimination.
    - We want to use $H$﻿ to zero out elements in our column vectors.

![Untitled 17 13.png](../../attachments/Untitled%2017%2013.png)

## Examples

- In this first example, we do it with $a_1$﻿, so none of the entries are ignored.

![Untitled 18 11.png](../../attachments/Untitled%2018%2011.png)

- In this example, we do it with $a_2$﻿, so the first entry (6) gets ignored and kept.

![Untitled 19 10.png](../../attachments/Untitled%2019%2010.png)

## Using Householder for QR

- We progressively apply Householder transforms to zero out entries below the diagonal for each column of $A$﻿.
    - $H_1$﻿ will make it so that the first column will have one entry at the top, and zeros all below the diagonal.
    - $H_2$﻿ will do the same now. It won’t touch the first column. It’ll just zero out entries below the diagonal for the second column, and mess up the rest.
    - We then repeat this until we get an upper triangular matrix.

![Untitled 20 10.png](../../attachments/Untitled%2020%2010.png)

- We can relate this to QR by combining all of the Housholder transforms.
    - Recall that each Householder transform is orthogonal bc it’s just a reflection.
    - Doing a bunch of reflections is still an orthogonal process.

$A = \hat{Q} \begin{pmatrix} R \\0\end{pmatrix} = \begin{pmatrix} Q & \tilde{Q}\end{pmatrix}\begin{pmatrix} R \\0\end{pmatrix} = QR$

$\hat{Q}^T b = \begin{pmatrix}\hat{b}_Q \\ \hat{b}_{\tilde{Q}}\end{pmatrix}$

![Untitled 21 8.png](../../attachments/Untitled%2021%208.png)