---
Week: Week 5
---
# Recall

## Polynomial Interpolation

![[attachments/Untitled 66.png|Untitled 66.png]]

## Basis Functions

![[attachments/Untitled 1 33.png|Untitled 1 33.png]]

- Monomial basis is poorly conditioned, and the others are better.

## Overfitting

![[attachments/Untitled 2 33.png|Untitled 2 33.png]]

## Regularization

![[attachments/Untitled 3 33.png|Untitled 3 33.png]]

![[attachments/Untitled 4 32.png|Untitled 4 32.png]]

## Underfitting

![[attachments/Untitled 5 32.png|Untitled 5 32.png]]

- Suppose the matrix of basis functions is 5 x 5, where the rows are the datapoints and the columns are the basis functions up to $x^4$﻿.
    - A 5x5 matrix (uses $x^4$﻿) is likely overfit.
    - A 5x3 matrix (uses $x^2$﻿) is regularized, probably good.
    - A 5x2 matrix (uses $x$﻿) is likely unferfit.

# Eliminating Basis Functions

- Each entry $ik$﻿ (row $i$﻿, col $k$﻿) of $A$﻿ is $\phi_k(x_i)$﻿
- Each row applies all the basis functions to a single data point.
- Each column applies a single basis function to all data points.

![[attachments/Untitled 6 31.png|Untitled 6 31.png]]

- Regularization means reducing the number of basis functions, so reducing the number of columns of $A$﻿.
    - Let’s say an overfitted matrix is 100 x 100. Regularization means you use something like 100 x 10.
    - Your columns are still in $\R^{100}$﻿, but now there are less of them.

![[attachments/Untitled 7 31.png|Untitled 7 31.png]]

- Full column rank means it fully covers $\R^n$﻿, where $n$﻿ is the number of columns.
    - If a 100 x 100 matrix fully covers $\R^2$﻿, then kicking out columns to get a 100 x 2 matrix will likely still fully cover $\R^2$﻿
- We don’t want to throw away too many columns, since it can underfit.

# Tall (Full Rank) Matrices

- If you take any system, even if its a wide matrix, you can turn it into a square or tall full rank matrix by just throwing out columns until it becomes full rank.
    - **Every matrix can become tall and full rank by throwing away columns that don’t matter.**
- This kind of matrix is very common, since the rows are the data and the columns are the features.
    - There are more data entries than there are features.

![[attachments/Untitled 8 31.png|Untitled 8 31.png]]

- If you have a 100 x 5 matrix, you only need 5 rows out of those 100 to capture all of the information necessary.
    - There are $m - n$﻿ rows of extra data.
    - This is just the matrix itself. The right hand side might contain information that is useful (captures the errors in the system)
- If a matrix is tall but not full column rank, simply kick out columns until it is full rank.

![[attachments/Untitled 9 30.png|Untitled 9 30.png]]

# Solving Tall (Full Rank) Linear Systems

![[attachments/Untitled 10 29.png|Untitled 10 29.png]]

- The last row in the matrix is essentially ignored.
- The last column of $U$﻿ is unused because $\hat{b}_3$﻿ must equal zero.

# Solutions

- When the right hand side is in the range of the matrix on the left, there’s a solution.
- In the example below, the equation that we get is false because (3, 4) is not in the range of (1, 1)
    
    ![[attachments/Untitled 11 29.png|Untitled 11 29.png]]
    
- However, in this example, it’s good because its a different data point.
    
    ![[attachments/Untitled 12 29.png|Untitled 12 29.png]]
    

# Residual

![[attachments/Untitled 13 28.png|Untitled 13 28.png]]

- For least squares, we should be minimizing the residual.

## Norm Matters

- If there is a unique solution, meaning $b$﻿ is in the range of $A$﻿, then the **norm does not matter** because we can get $r = 0$﻿.
    - The norm of $r = 0$﻿ will just be zero, regardless of what norm.
- If there is no unique solution, we can’t get to $r = 0$﻿. Therefore, the **norm matters a lot**.
    - This means depending on which norm we are trying to minimize, there are different values of $c$﻿ that solve it.
    - This is because the norm defines the “minimum” of the residual.
- In the example, the line we are using to fit is $y = c_1$﻿.
    - We’re given three data points that obviously do not lie on the same line.
    - Therefore, there is no solution to the system.
    - Therefore, the norm matters a lot. Notice that depending on the norm, $c_1$﻿ changes.

![[attachments/Untitled 14 27.png|Untitled 14 27.png]]

## Row Operations Matter

- Any row operation changes the solution, unless there is a unique solution.
    - In other words, when the residual is zero, row operations do not matter.

![[attachments/Untitled 15 26.png|Untitled 15 26.png]]

![[attachments/Untitled 16 26.png|Untitled 16 26.png]]

![[attachments/Untitled 17 23.png|Untitled 17 23.png]]

## Weighted Minimization

- We can **assign different weights to different equations** by scaling some entries of the residual, meaning we scale the rows of $A$﻿ and $b$﻿ before computing the residual.
- We should embrace the fact that row scaling changes the solution to do weighting and get us a better solution.

![[attachments/Untitled 18 21.png|Untitled 18 21.png]]

- Column scaling does not affect the residual, so we can use it to preserve symmetry.
    - This is because to column scale the matrix $A$﻿, the vector $c$﻿ also has to be scaled, but in the opposite direction. The inverse is also true.
        
        ![[attachments/Untitled 19 18.png|Untitled 19 18.png]]
        
    - In the example below, we are scaling each column of matrix $A$﻿ using $\hat{D}^{-1}$﻿. Therefore, we need to corresponding scale the vector $c$﻿ with $\hat{D}$﻿ in order to keep the system equal to the original.

![[attachments/Untitled 20 18.png|Untitled 20 18.png]]

- $\hat{D}$﻿ does not affect $Dr$﻿, hence why it does not affect the residual.

# Least Squares

- Minimizing the two-norm $||r||_2$﻿ is called least squares.

![[attachments/Untitled 21 15.png|Untitled 21 15.png]]

- Minimizing the weighted two-norm $||Dr||_2$﻿ is referred to as weighted least squares.

## Least Squares Solution

![[attachments/Untitled 22 11.png|Untitled 22 11.png]]