---
Week: Week 5
---
# Recall

## Polynomial Interpolation

![Untitled 66.png](../../attachments/Untitled%2066.png)

## Basis Functions

![Untitled 1 33.png](../../attachments/Untitled%201%2033.png)

- Monomial basis is poorly conditioned, and the others are better.

## Overfitting

![Untitled 2 33.png](../../attachments/Untitled%202%2033.png)

## Regularization

![Untitled 3 33.png](../../attachments/Untitled%203%2033.png)

![Untitled 4 32.png](../../attachments/Untitled%204%2032.png)

## Underfitting

![Untitled 5 32.png](../../attachments/Untitled%205%2032.png)

- Suppose the matrix of basis functions is 5 x 5, where the rows are the datapoints and the columns are the basis functions up to $x^4$﻿.
    - A 5x5 matrix (uses $x^4$﻿) is likely overfit.
    - A 5x3 matrix (uses $x^2$﻿) is regularized, probably good.
    - A 5x2 matrix (uses $x$﻿) is likely unferfit.

# Eliminating Basis Functions

- Each entry $ik$﻿ (row $i$﻿, col $k$﻿) of $A$﻿ is $\phi_k(x_i)$﻿
- Each row applies all the basis functions to a single data point.
- Each column applies a single basis function to all data points.

![Untitled 6 31.png](../../attachments/Untitled%206%2031.png)

- Regularization means reducing the number of basis functions, so reducing the number of columns of $A$﻿.
    - Let’s say an overfitted matrix is 100 x 100. Regularization means you use something like 100 x 10.
    - Your columns are still in $\R^{100}$﻿, but now there are less of them.

![Untitled 7 31.png](../../attachments/Untitled%207%2031.png)

- Full column rank means it fully covers $\R^n$﻿, where $n$﻿ is the number of columns.
    - If a 100 x 100 matrix fully covers $\R^2$﻿, then kicking out columns to get a 100 x 2 matrix will likely still fully cover $\R^2$﻿
- We don’t want to throw away too many columns, since it can underfit.

# Tall (Full Rank) Matrices

- If you take any system, even if its a wide matrix, you can turn it into a square or tall full rank matrix by just throwing out columns until it becomes full rank.
    - **Every matrix can become tall and full rank by throwing away columns that don’t matter.**
- This kind of matrix is very common, since the rows are the data and the columns are the features.
    - There are more data entries than there are features.

![Untitled 8 31.png](../../attachments/Untitled%208%2031.png)

- If you have a 100 x 5 matrix, you only need 5 rows out of those 100 to capture all of the information necessary.
    - There are $m - n$﻿ rows of extra data.
    - This is just the matrix itself. The right hand side might contain information that is useful (captures the errors in the system)
- If a matrix is tall but not full column rank, simply kick out columns until it is full rank.

![Untitled 9 30.png](../../attachments/Untitled%209%2030.png)

# Solving Tall (Full Rank) Linear Systems

![Untitled 10 29.png](../../attachments/Untitled%2010%2029.png)

- The last row in the matrix is essentially ignored.
- The last column of $U$﻿ is unused because $\hat{b}_3$﻿ must equal zero.

# Solutions

- When the right hand side is in the range of the matrix on the left, there’s a solution.
- In the example below, the equation that we get is false because (3, 4) is not in the range of (1, 1)
    
    ![Untitled 11 29.png](../../attachments/Untitled%2011%2029.png)
    
- However, in this example, it’s good because its a different data point.
    
    ![Untitled 12 29.png](../../attachments/Untitled%2012%2029.png)
    

# Residual

![Untitled 13 28.png](../../attachments/Untitled%2013%2028.png)

- For least squares, we should be minimizing the residual.

## Norm Matters

- If there is a unique solution, meaning $b$﻿ is in the range of $A$﻿, then the **norm does not matter** because we can get $r = 0$﻿.
    - The norm of $r = 0$﻿ will just be zero, regardless of what norm.
- If there is no unique solution, we can’t get to $r = 0$﻿. Therefore, the **norm matters a lot**.
    - This means depending on which norm we are trying to minimize, there are different values of $c$﻿ that solve it.
    - This is because the norm defines the “minimum” of the residual.
- In the example, the line we are using to fit is $y = c_1$﻿.
    - We’re given three data points that obviously do not lie on the same line.
    - Therefore, there is no solution to the system.
    - Therefore, the norm matters a lot. Notice that depending on the norm, $c_1$﻿ changes.

![Untitled 14 27.png](../../attachments/Untitled%2014%2027.png)

## Row Operations Matter

- Any row operation changes the solution, unless there is a unique solution.
    - In other words, when the residual is zero, row operations do not matter.

![Untitled 15 26.png](../../attachments/Untitled%2015%2026.png)

![Untitled 16 26.png](../../attachments/Untitled%2016%2026.png)

![Untitled 17 23.png](../../attachments/Untitled%2017%2023.png)

## Weighted Minimization

- We can **assign different weights to different equations** by scaling some entries of the residual, meaning we scale the rows of $A$﻿ and $b$﻿ before computing the residual.
- We should embrace the fact that row scaling changes the solution to do weighting and get us a better solution.

![Untitled 18 21.png](../../attachments/Untitled%2018%2021.png)

- Column scaling does not affect the residual, so we can use it to preserve symmetry.
    - This is because to column scale the matrix $A$﻿, the vector $c$﻿ also has to be scaled, but in the opposite direction. The inverse is also true.
        
        ![Untitled 19 18.png](../../attachments/Untitled%2019%2018.png)
        
    - In the example below, we are scaling each column of matrix $A$﻿ using $\hat{D}^{-1}$﻿. Therefore, we need to corresponding scale the vector $c$﻿ with $\hat{D}$﻿ in order to keep the system equal to the original.

![Untitled 20 18.png](../../attachments/Untitled%2020%2018.png)

- $\hat{D}$﻿ does not affect $Dr$﻿, hence why it does not affect the residual.

# Least Squares

- Minimizing the two-norm $||r||_2$﻿ is called least squares.

![Untitled 21 15.png](../../attachments/Untitled%2021%2015.png)

- Minimizing the weighted two-norm $||Dr||_2$﻿ is referred to as weighted least squares.

## Least Squares Solution

![Untitled 22 11.png](../../attachments/Untitled%2022%2011.png)