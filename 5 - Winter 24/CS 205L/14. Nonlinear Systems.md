---
Week: Week 7
---
# Roadmap

![Untitled 60.png](../../attachments/Untitled%2060.png)

# Recap: Jacobian

![Untitled 1 27.png](../../attachments/Untitled%201%2027.png)

# Lineralization

- Solving a linear system is difficult, so we usually linearize it.
- The first method is to take the Taylor expansion of the system.
    
    ![Untitled 2 27.png](../../attachments/Untitled%202%2027.png)
    
    - In the example above, $F$﻿ is a column vector. $F'$﻿ is the Jacobian, a matrix.
- The second method is to use the chain rule with differential forms.
    
    - This is the total derivative.
    
    ![Untitled 3 27.png](../../attachments/Untitled%203%2027.png)
    
    - We approximate $dF(c)$﻿ with a small actual change in $F$﻿, and $dc$﻿ with a small actual change in $c$﻿.
- The red highlighted terms is the approximated linearization. This is basically the best linear approximation of the nonlinear function $F(c)$﻿.
- Newton’s Method uses two terms of the Taylor expansion, giving it quadratic convergence rate.
- GD uses one term of the Taylor expansion, giving it linear convergence.

# Recap: Solving Linear Systems

![Untitled 4 26.png](../../attachments/Untitled%204%2026.png)

# Newton’s Method

- An iterative method that uses linearization to solve the problem.
    - Everything is an approximation here. Does not find actual answer.

![Untitled 5 26.png](../../attachments/Untitled%205%2026.png)

- The goal is to make $F(c^{q+1})$﻿ smaller than $F(c^q)$﻿. We use $\beta$﻿ to do that.
    - We can also think of $\beta - 1$﻿ as $D - I$﻿, where $D$﻿ is a diagonal matrix that tells us how much each equation of $F(c^q)$﻿ should shrink.
- In this case, $\Delta c^q$﻿ is the search direction.
    - We’re solving the system $F'(c^q) \Delta c^q = (\beta - 1)F (c^q)$﻿ to find $\Delta c^q$﻿
        - Notice that the above is reminiscent of solving $Ac = b$﻿, because $F'$﻿ is a matrix and $F$﻿ is a vector.

![Untitled 6 25.png](../../attachments/Untitled%206%2025.png)

## Line Search

- Since there are inherent linearization errors, we can treat the solved $\Delta c^q$﻿ as a search direction, and treat it like a line search problem.
    - We just figure out how much we move in the direction of $\Delta c^q$﻿. This reduces our search problem into a series of 1D problems.

![Untitled 7 25.png](../../attachments/Untitled%207%2025.png)

- The original problem of finding $F(c^{q+1}) = 0$﻿ was a problem in $\R^n$﻿.
- The new problem $F(c^{q+1}(\alpha)) = 0$﻿ is now a problem in $\R^1$﻿, because $\alpha$﻿ is our only parameter.
- To find the roots of $F$﻿, we can minimize $F^T F$﻿, and this is minimized when $g(\alpha) = 0$﻿
    - If we find $g(\alpha) = 0$﻿, we’ve found the roots $F = 0$﻿.
    - If we can’t find it, since we minimized $g(\alpha)$﻿, we’ve at least gotten closer to the root.

![Untitled 8 25.png](../../attachments/Untitled%208%2025.png)

# Optimization Problems

- Optimization problems are very similar to non-linear systems.

![Untitled 9 24.png](../../attachments/Untitled%209%2024.png)

- The first two options are the same as before.
- The last option is:
    - Once we know our search direction $c^{q+1}(\alpha)$﻿, we can just directly minimize the cost function with this search direction.
    - This works because $\hat{f}$﻿ is already a scalar-valued function.