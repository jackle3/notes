---
Week: Week 7
---
# Recap

- When solving optimization problems, we tend to have:
    - A cost function $\hat{f}(c)$﻿
    - Seting the gradient to zero gives us a system of nonlinear equations: $\nabla\hat{f}(c) = 0$﻿
    - Solving that system of linear equations gives us a critical point, which can be max or min.
        - To solve, we typically linearize this problem. We take the Jacobian of $\nabla\hat{f}(c)$﻿ to get the Hessian $H_{\hat{f}}$﻿.
        - We then solve Hessian $\times$﻿ search direction = right hand side
            
            ![[attachments/Untitled 59.png|Untitled 59.png]]
            
    - We can think of the search direction $\Delta c^q$﻿ as a line in high-dimensional space.
        - On that line, we want to find a root of the original non-linear system of equations.
        - That root is either a solution to the non-linear system or its a critical point.
        - Alternatively, we also want to minimize and get towards the root.

# Fixed Point Iteration

- We take some initial guess for $t^q$﻿. Then, pass it through the function $\hat{g}(t)$﻿ to update it.
    - In the case of line search, this is $c^{q+1} = c^q + t^q \Delta c^q$﻿. The variable $t$﻿ is how far you go in your search direction $\Delta c_q$﻿.

![[attachments/Untitled 1 26.png|Untitled 1 26.png]]

- Once we converge, meaning it satisfies $t^* = \hat{g}^*$﻿, then we know $g(t^*) = 0$﻿, as desired.
    - This $t^*$﻿ is the fixed point. Passing it into $\hat{g}(t)$﻿ keeps it fixed.

![[attachments/Untitled 2 26.png|Untitled 2 26.png]]

- It will converge when the absolute value of the derivative at the converged point is less than one.
    - The error at $q+1$﻿ is $e^{q+1} = t^{q+1} - t^*$﻿.
    - We know $t^{q+1} = \hat{g}(t^q)$﻿ and $t* = \hat{g}(t^*)$﻿

## Convergence Rate

![[attachments/Untitled 3 26.png|Untitled 3 26.png]]

# 1D Newton’s Method

![[attachments/Untitled 4 25.png|Untitled 4 25.png]]

- When we have repeated roots (ie. more than one root at the same position), $g'(t*)$﻿ is zero.
    - Consider the function $(x - 2)^2 = x^2 - 4x + 4$﻿
        - This function has one root at 2 (quadratic, but both roots are here)
        - If we take the derivative, we have $2(x - 2)$﻿.
            - The derivative is zero at 2 → hard to converge to here.

![[attachments/Untitled 5 25.png|Untitled 5 25.png]]

- The derivative of $g$﻿ at $t^q$﻿ is the change in $g$﻿ divided by the change in $t$﻿.
    - Our goal is to get $g$﻿ to be zero. That’s why $\Delta g = g(t^q) - 0$﻿.
    - $t^{q+1}$﻿ is wherever the slope line intersects zero.
- We then keep repeating until we converge.

# Secant Method

- Same as Newtons, but instead of calculating the derivative, estimate it using a secant line.

![[attachments/Untitled 6 24.png|Untitled 6 24.png]]

- Takes more iterations to converge than Newton. However, each iteration takes less time because because we don’t need the derivative.

![[attachments/Untitled 7 24.png|Untitled 7 24.png]]

- Notice that instead of calculating the tangent at that point, we just get a secant line between $t^q$﻿ and $t^{q+1}$﻿.

# Bisection Method

- The last two methods are good **if** it’s able to converge. Generally, we might not be able to converge.
- Bisection guarantees convergence to a root if there is a sign change in the interval.
    - Sign change means one part of interval is positive, and one part is negative.

![[attachments/Untitled 8 24.png|Untitled 8 24.png]]

- We find two points where $g(t_L) \times g(t_R) < 0$﻿, meaning that there is a sign change between $t_L$﻿ and $t_R$﻿.
- From here, search for root within the interval, kinda like **binary search.**
    - Find the midpoint between $t_L$﻿ and $t_R$﻿.
    - If it has opposite signs as $t_L$﻿, meaning $g(t_L) \times g(t_M) < 0$﻿, throw away the right half of the interval since the sign change is in the left half.
- This is linear convergence rate, since the interval size halves each iteration.
    - This is pretty slow compared to the others.

![[attachments/Untitled 9 23.png|Untitled 9 23.png]]

- In the above, we see $t_L$﻿ and $t_M$﻿ has opposite signs, meaning the sign change in between them, so shrink the interval and set $t_R = t_m$﻿.

# Mixed Methods

- We can combine Newton/Secant and the Bisection method.
    - This is how the problem is generally solved.

![[attachments/Untitled 10 23.png|Untitled 10 23.png]]

# Function/Derivative Requirements

![[attachments/Untitled 11 23.png|Untitled 11 23.png]]

## Useful Derivatives

- These are the derivatives for nonlinear systems.
    
    ![[attachments/Untitled 12 23.png|Untitled 12 23.png]]
    
    - In the second, it’s just the chain rule.
        - Partial of F is Jacobian of F times the partial of the inside.
- These are the derivatives for optimization.
    
    ![[attachments/Untitled 13 22.png|Untitled 13 22.png]]
    
    - This uses the second derivatives, taking the partial of the Jacobian to get the Hessian.

# Recall: Line Search

![[attachments/Untitled 14 21.png|Untitled 14 21.png]]

![[attachments/Untitled 15 20.png|Untitled 15 20.png]]

## Nonlinear Systems Problems

![[attachments/Untitled 16 20.png|Untitled 16 20.png]]

- In option 1, we consider the vector-valued function and find roots for all i, one at a time.
- In option 2, we use the scalar-valued function and find the roots of that, which are also the roots of $F$﻿

# Recall: Optimization Problems

![[attachments/Untitled 17 18.png|Untitled 17 18.png]]

## Optimization Problems

![[attachments/Untitled 18 16.png|Untitled 18 16.png]]

- Option 2 can be solved using iterative methods like gradient descent, since its a scalar-valued function.