---
Week: Week 7
---
# Recap

- When solving optimization problems, we tend to have:
    - A cost function $\hat{f}(c)$﻿
    - Seting the gradient to zero gives us a system of nonlinear equations: $\nabla\hat{f}(c) = 0$﻿
    - Solving that system of linear equations gives us a critical point, which can be max or min.
        - To solve, we typically linearize this problem. We take the Jacobian of $\nabla\hat{f}(c)$﻿ to get the Hessian $H_{\hat{f}}$﻿.
        - We then solve Hessian $\times$﻿ search direction = right hand side
            
            ![Untitled 59.png](attachments/Untitled%2059.png)
            
    - We can think of the search direction $\Delta c^q$﻿ as a line in high-dimensional space.
        - On that line, we want to find a root of the original non-linear system of equations.
        - That root is either a solution to the non-linear system or its a critical point.
        - Alternatively, we also want to minimize and get towards the root.

# Fixed Point Iteration

- We take some initial guess for $t^q$﻿. Then, pass it through the function $\hat{g}(t)$﻿ to update it.
    - In the case of line search, this is $c^{q+1} = c^q + t^q \Delta c^q$﻿. The variable $t$﻿ is how far you go in your search direction $\Delta c_q$﻿.

![Untitled 1 26.png](attachments/Untitled%201%2026.png)

- Once we converge, meaning it satisfies $t^* = \hat{g}^*$﻿, then we know $g(t^*) = 0$﻿, as desired.
    - This $t^*$﻿ is the fixed point. Passing it into $\hat{g}(t)$﻿ keeps it fixed.

![Untitled 2 26.png](attachments/Untitled%202%2026.png)

- It will converge when the absolute value of the derivative at the converged point is less than one.
    - The error at $q+1$﻿ is $e^{q+1} = t^{q+1} - t^*$﻿.
    - We know $t^{q+1} = \hat{g}(t^q)$﻿ and $t* = \hat{g}(t^*)$﻿

## Convergence Rate

![Untitled 3 26.png](attachments/Untitled%203%2026.png)

# 1D Newton’s Method

![Untitled 4 25.png](attachments/Untitled%204%2025.png)

- When we have repeated roots (ie. more than one root at the same position), $g'(t*)$﻿ is zero.
    - Consider the function $(x - 2)^2 = x^2 - 4x + 4$﻿
        - This function has one root at 2 (quadratic, but both roots are here)
        - If we take the derivative, we have $2(x - 2)$﻿.
            - The derivative is zero at 2 → hard to converge to here.

![Untitled 5 25.png](attachments/Untitled%205%2025.png)

- The derivative of $g$﻿ at $t^q$﻿ is the change in $g$﻿ divided by the change in $t$﻿.
    - Our goal is to get $g$﻿ to be zero. That’s why $\Delta g = g(t^q) - 0$﻿.
    - $t^{q+1}$﻿ is wherever the slope line intersects zero.
- We then keep repeating until we converge.

# Secant Method

- Same as Newtons, but instead of calculating the derivative, estimate it using a secant line.

![Untitled 6 24.png](attachments/Untitled%206%2024.png)

- Takes more iterations to converge than Newton. However, each iteration takes less time because because we don’t need the derivative.

![Untitled 7 24.png](attachments/Untitled%207%2024.png)

- Notice that instead of calculating the tangent at that point, we just get a secant line between $t^q$﻿ and $t^{q+1}$﻿.

# Bisection Method

- The last two methods are good **if** it’s able to converge. Generally, we might not be able to converge.
- Bisection guarantees convergence to a root if there is a sign change in the interval.
    - Sign change means one part of interval is positive, and one part is negative.

![Untitled 8 24.png](attachments/Untitled%208%2024.png)

- We find two points where $g(t_L) \times g(t_R) < 0$﻿, meaning that there is a sign change between $t_L$﻿ and $t_R$﻿.
- From here, search for root within the interval, kinda like **binary search.**
    - Find the midpoint between $t_L$﻿ and $t_R$﻿.
    - If it has opposite signs as $t_L$﻿, meaning $g(t_L) \times g(t_M) < 0$﻿, throw away the right half of the interval since the sign change is in the left half.
- This is linear convergence rate, since the interval size halves each iteration.
    - This is pretty slow compared to the others.

![Untitled 9 23.png](attachments/Untitled%209%2023.png)

- In the above, we see $t_L$﻿ and $t_M$﻿ has opposite signs, meaning the sign change in between them, so shrink the interval and set $t_R = t_m$﻿.

# Mixed Methods

- We can combine Newton/Secant and the Bisection method.
    - This is how the problem is generally solved.

![Untitled 10 23.png](attachments/Untitled%2010%2023.png)

# Function/Derivative Requirements

![Untitled 11 23.png](attachments/Untitled%2011%2023.png)

## Useful Derivatives

- These are the derivatives for nonlinear systems.
    
    ![Untitled 12 23.png](attachments/Untitled%2012%2023.png)
    
    - In the second, it’s just the chain rule.
        - Partial of F is Jacobian of F times the partial of the inside.
- These are the derivatives for optimization.
    
    ![Untitled 13 22.png](attachments/Untitled%2013%2022.png)
    
    - This uses the second derivatives, taking the partial of the Jacobian to get the Hessian.

# Recall: Line Search

![Untitled 14 21.png](attachments/Untitled%2014%2021.png)

![Untitled 15 20.png](attachments/Untitled%2015%2020.png)

## Nonlinear Systems Problems

![Untitled 16 20.png](attachments/Untitled%2016%2020.png)

- In option 1, we consider the vector-valued function and find roots for all i, one at a time.
- In option 2, we use the scalar-valued function and find the roots of that, which are also the roots of $F$﻿

# Recall: Optimization Problems

![Untitled 17 18.png](attachments/Untitled%2017%2018.png)

## Optimization Problems

![Untitled 18 16.png](attachments/Untitled%2018%2016.png)

- Option 2 can be solved using iterative methods like gradient descent, since its a scalar-valued function.