---
Week: Week 6
---
# Adding the identity

- We’ll still use the matrix $A$﻿, even if it has some zero singular values.

![[attachments/Untitled 68.png|Untitled 68.png]]

- The matrix $\begin{pmatrix} A \\ I\end{pmatrix}$﻿ is now a tall and full rank matrix (bc $I$﻿ is square).
    - This effectively forces the lower components of $c$﻿ (that correspond to small singular values) to zero.

![[attachments/Untitled 1 35.png|Untitled 1 35.png]]

- Recall that $\hat{b} = U^T b$﻿ and $\hat{c} = V^T c$﻿.
- By including the identity, it makes sure that all the $\hat{c}_z$﻿ gets set to zero. This is what we want.

$\begin{pmatrix} \hat{\Sigma^2} + 1 &0 \\0&1\end{pmatrix} \begin{pmatrix} \hat{c}_r \\ \hat{c}_z\end{pmatrix} = \begin{pmatrix} \hat{\Sigma}\hat{b}_r\\0\end{pmatrix} $

## Perturbation

- For the large singular values (the most important ones), this perturbation is negligible, which is good.
- For small singular values, it drives it towards zero, which is also what we want.

![[attachments/Untitled 2 35.png|Untitled 2 35.png]]

# Regularization

- To make the previous example more generalizable, change the $1$﻿ to some $\epsilon$﻿ that you pick.
    - $\epsilon$﻿ is effectively the size of the singular value that you want to ignore.
        - For singular values larger than $\epsilon$﻿, its effect is negligible.
        - For singualr values smaller than $\epsilon$﻿, it pushes the associated $\hat{c}_k$﻿ to zero.

![[attachments/Untitled 3 35.png|Untitled 3 35.png]]

# Nonzero Initial Guess - Prior

![[attachments/Untitled 4 34.png|Untitled 4 34.png]]

- For very big $\sigma_k$﻿, you get $\hat{c}_k \approx \hat{b}_k / \sigma_k$﻿, which is the regular solution.
- For very small $\sigma_k$﻿, you get $\hat{c}_k \approx \hat{c}_k^*$﻿

## Convex Combination

![[attachments/Untitled 5 34.png|Untitled 5 34.png]]

# Iterative Approach with Prior

- The red is the prior, which was the result of the last iteration.

![[attachments/Untitled 6 33.png|Untitled 6 33.png]]

- Notice that the coefficient is a geometric series, multiplied by the result of $\hat{c}_k$﻿ when the prior was $\epsilon I c = 0$﻿

## Convergence

- Repeating this iterative approach converges to the desired SVD solution, for every singular value.
- It also converges to the minimum norm solution → $\hat{c}_k = 0$﻿ when $\sigma_k = 0$﻿

![[attachments/Untitled 7 33.png|Untitled 7 33.png]]

## Convergence Rate

![[attachments/Untitled 8 33.png|Untitled 8 33.png]]

# Comparison to PCA

- PCA uses the largest $q$﻿ singular values exactly. Smaller ones are all set to zero.

![[attachments/Untitled 9 32.png|Untitled 9 32.png]]

- In this method, after $q$﻿ operations, it gets closer to using the full rank-one approximations, but not the full thing.
    - All components are contributing.
        - Components that you want to regularize (e.g. smaller $\sigma_k$﻿) are contributing less.
        - Components that you want to keep are contributing more.
    - The fall-off is smooth for smaller $\sigma_k$﻿’s

![[attachments/Untitled 10 31.png|Untitled 10 31.png]]

# Adding a diagonal matrix

- Some components of $c$﻿ should be more regularized than others.
    - When we add the identity, we are regularizing them all equally.
    - We can use a diagonal to effectively do some weighting for regularization.

![[attachments/Untitled 11 31.png|Untitled 11 31.png]]

- Adding $D$﻿ straight up leads to some issues. Instead, modify $A$﻿ and $c$﻿, allowing us to treat the problem in the traditional way.
    - $\tilde{A} = AD^{-1}$﻿ and $\tilde{c} = Dc$﻿

# Geometric Approaches

![[attachments/Untitled 12 31.png|Untitled 12 31.png]]

## Example

![[attachments/Untitled 13 30.png|Untitled 13 30.png]]

## Overshooting

- In this case, $A$﻿ is full rank, so it has a unique solution.
    - However, the unique solution is pretty bad because it significantly overshoots then backtracks.
    - This is because $A$﻿ is ill conditioned, meaning the columns were almost parallel.

![[attachments/Untitled 14 29.png|Untitled 14 29.png]]

## Regularization/Damping

- Notice that adding regularization in this case damps it so much that the components can’t quite get to the actual solution.

![[attachments/Untitled 15 27.png|Untitled 15 27.png]]

## Smarter Regularization

- In this case, we add a diagonal matrix instead of just $Ic = 0$﻿. This matrix only damps the second component.
    - As we can see, it primarily uses $c_1a_1$﻿ to get us closer. It only uses a tiny bit of $c_2a_2$﻿

![[attachments/Untitled 16 27.png|Untitled 16 27.png]]

## Coordinate Descent

![[attachments/Untitled 17 24.png|Untitled 17 24.png]]

## Summary

![[attachments/Untitled 18 22.png|Untitled 18 22.png]]

## Correlation vs. Gains

![[attachments/Untitled 19 19.png|Untitled 19 19.png]]

- To maximize gains, we want to minimize actions. This means we basically look at which column makes the most progress, preferencing towards a smaller $c_k$﻿

# Facial Tracking Example

- We draw blue curves on a 2D image, and red curves on the 3D geometry. To do facial tracking, we want the projection of the red curves onto the 2D image plane to overlap the blue curves.
    - This can be interpreted as an optimization problem.

![[attachments/Untitled 20 19.png|Untitled 20 19.png]]

## No Regularization

- This mathces the curves exactly, but the geometry is very bad.

![[attachments/Untitled 21 16.png|Untitled 21 16.png]]

## L2 Regularization

- This looks a lot better. However, notice that it now doesn’t really match the curves exactly.
    - Notice in the graph that all the values are really damp too. Adding $Ic = 0$﻿ is pushing all of the parameters down.

![[attachments/Untitled 22 12.png|Untitled 22 12.png]]

## Soft L1 Regularization

- This looks a bit better. Also, notice that the $\theta$﻿s are very sparse, which is better. We basically activate less things to get the same result.
    - However, it’s still overly damped.

![[attachments/Untitled 23 11.png|Untitled 23 11.png]]

## Column Space Search

- Instead of doing any explicit regularization and solving, this one is just using the geometric approach shown above.

![[attachments/Untitled 24 10.png|Untitled 24 10.png]]