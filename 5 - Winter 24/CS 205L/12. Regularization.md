---
Week: Week 6
---
# Adding the identity

- We’ll still use the matrix $A$﻿, even if it has some zero singular values.

![Untitled 68.png](../../attachments/Untitled%2068.png)

- The matrix $\begin{pmatrix} A \\ I\end{pmatrix}$﻿ is now a tall and full rank matrix (bc $I$﻿ is square).
    - This effectively forces the lower components of $c$﻿ (that correspond to small singular values) to zero.

![Untitled 1 35.png](../../attachments/Untitled%201%2035.png)

- Recall that $\hat{b} = U^T b$﻿ and $\hat{c} = V^T c$﻿.
- By including the identity, it makes sure that all the $\hat{c}_z$﻿ gets set to zero. This is what we want.

$\begin{pmatrix} \hat{\Sigma^2} + 1 &0 \\0&1\end{pmatrix} \begin{pmatrix} \hat{c}_r \\ \hat{c}_z\end{pmatrix} = \begin{pmatrix} \hat{\Sigma}\hat{b}_r\\0\end{pmatrix} $

## Perturbation

- For the large singular values (the most important ones), this perturbation is negligible, which is good.
- For small singular values, it drives it towards zero, which is also what we want.

![Untitled 2 35.png](../../attachments/Untitled%202%2035.png)

# Regularization

- To make the previous example more generalizable, change the $1$﻿ to some $\epsilon$﻿ that you pick.
    - $\epsilon$﻿ is effectively the size of the singular value that you want to ignore.
        - For singular values larger than $\epsilon$﻿, its effect is negligible.
        - For singualr values smaller than $\epsilon$﻿, it pushes the associated $\hat{c}_k$﻿ to zero.

![Untitled 3 35.png](../../attachments/Untitled%203%2035.png)

# Nonzero Initial Guess - Prior

![Untitled 4 34.png](../../attachments/Untitled%204%2034.png)

- For very big $\sigma_k$﻿, you get $\hat{c}_k \approx \hat{b}_k / \sigma_k$﻿, which is the regular solution.
- For very small $\sigma_k$﻿, you get $\hat{c}_k \approx \hat{c}_k^*$﻿

## Convex Combination

![Untitled 5 34.png](../../attachments/Untitled%205%2034.png)

# Iterative Approach with Prior

- The red is the prior, which was the result of the last iteration.

![Untitled 6 33.png](../../attachments/Untitled%206%2033.png)

- Notice that the coefficient is a geometric series, multiplied by the result of $\hat{c}_k$﻿ when the prior was $\epsilon I c = 0$﻿

## Convergence

- Repeating this iterative approach converges to the desired SVD solution, for every singular value.
- It also converges to the minimum norm solution → $\hat{c}_k = 0$﻿ when $\sigma_k = 0$﻿

![Untitled 7 33.png](../../attachments/Untitled%207%2033.png)

## Convergence Rate

![Untitled 8 33.png](../../attachments/Untitled%208%2033.png)

# Comparison to PCA

- PCA uses the largest $q$﻿ singular values exactly. Smaller ones are all set to zero.

![Untitled 9 32.png](../../attachments/Untitled%209%2032.png)

- In this method, after $q$﻿ operations, it gets closer to using the full rank-one approximations, but not the full thing.
    - All components are contributing.
        - Components that you want to regularize (e.g. smaller $\sigma_k$﻿) are contributing less.
        - Components that you want to keep are contributing more.
    - The fall-off is smooth for smaller $\sigma_k$﻿’s

![Untitled 10 31.png](../../attachments/Untitled%2010%2031.png)

# Adding a diagonal matrix

- Some components of $c$﻿ should be more regularized than others.
    - When we add the identity, we are regularizing them all equally.
    - We can use a diagonal to effectively do some weighting for regularization.

![Untitled 11 31.png](../../attachments/Untitled%2011%2031.png)

- Adding $D$﻿ straight up leads to some issues. Instead, modify $A$﻿ and $c$﻿, allowing us to treat the problem in the traditional way.
    - $\tilde{A} = AD^{-1}$﻿ and $\tilde{c} = Dc$﻿

# Geometric Approaches

![Untitled 12 31.png](../../attachments/Untitled%2012%2031.png)

## Example

![Untitled 13 30.png](../../attachments/Untitled%2013%2030.png)

## Overshooting

- In this case, $A$﻿ is full rank, so it has a unique solution.
    - However, the unique solution is pretty bad because it significantly overshoots then backtracks.
    - This is because $A$﻿ is ill conditioned, meaning the columns were almost parallel.

![Untitled 14 29.png](../../attachments/Untitled%2014%2029.png)

## Regularization/Damping

- Notice that adding regularization in this case damps it so much that the components can’t quite get to the actual solution.

![Untitled 15 27.png](../../attachments/Untitled%2015%2027.png)

## Smarter Regularization

- In this case, we add a diagonal matrix instead of just $Ic = 0$﻿. This matrix only damps the second component.
    - As we can see, it primarily uses $c_1a_1$﻿ to get us closer. It only uses a tiny bit of $c_2a_2$﻿

![Untitled 16 27.png](../../attachments/Untitled%2016%2027.png)

## Coordinate Descent

![Untitled 17 24.png](../../attachments/Untitled%2017%2024.png)

## Summary

![Untitled 18 22.png](../../attachments/Untitled%2018%2022.png)

## Correlation vs. Gains

![Untitled 19 19.png](../../attachments/Untitled%2019%2019.png)

- To maximize gains, we want to minimize actions. This means we basically look at which column makes the most progress, preferencing towards a smaller $c_k$﻿

# Facial Tracking Example

- We draw blue curves on a 2D image, and red curves on the 3D geometry. To do facial tracking, we want the projection of the red curves onto the 2D image plane to overlap the blue curves.
    - This can be interpreted as an optimization problem.

![Untitled 20 19.png](../../attachments/Untitled%2020%2019.png)

## No Regularization

- This mathces the curves exactly, but the geometry is very bad.

![Untitled 21 16.png](../../attachments/Untitled%2021%2016.png)

## L2 Regularization

- This looks a lot better. However, notice that it now doesn’t really match the curves exactly.
    - Notice in the graph that all the values are really damp too. Adding $Ic = 0$﻿ is pushing all of the parameters down.

![Untitled 22 12.png](../../attachments/Untitled%2022%2012.png)

## Soft L1 Regularization

- This looks a bit better. Also, notice that the $\theta$﻿s are very sparse, which is better. We basically activate less things to get the same result.
    - However, it’s still overly damped.

![Untitled 23 11.png](../../attachments/Untitled%2023%2011.png)

## Column Space Search

- Instead of doing any explicit regularization and solving, this one is just using the geometric approach shown above.

![Untitled 24 10.png](../../attachments/Untitled%2024%2010.png)