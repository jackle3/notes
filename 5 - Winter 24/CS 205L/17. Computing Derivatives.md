---
Week: Week 8
---
# Recap of Optimization

- Given some cost function $f$﻿, we want to minimize it
- Take the Jacobian of the cost function $f$﻿ and set it equal to zero.
    - This gives us a set of nonlinear equations.
- Linearize the nonlinear equations using Newton’s Method, and solve to get search directions.
- Use the 1D search directions to make progress towards the root of the Jacobian nonlinear system.

# Smoothness

- To compute a derivative, the function needs to be differentiable → smooth function.
    - If the problem fits some special class, we can use specialty methods to solve them more easily.
    - In general, neural networks will be nonlinear systems, so we can’t really use specialty approaches.

![Untitled 50.png](../../attachments/Untitled%2050.png)

## Biological Neurons

- There are some issues if we use biologically inspired methods → almost all of them are non-differentiable and not smooth.
- The increases frequency approach will have kinks in the derivatives.
    - This can be like putting your hand on a stove → the neuron response will fire with more frequency the hotter it is.
    - However, these changes in frequency are kinks, it’s not usually a smooth curve.

![Untitled 1 17.png](../../attachments/Untitled%201%2017.png)

# Activation Functions

## Heaviside Function

- It’s not really possible to optimize for this function, since it’s zero derivative everywhere. We don’t know what direction to go.

![Untitled 2 17.png](../../attachments/Untitled%202%2017.png)

## Sigmoid Function

- The sigmoid is a variant of Heaviside that is smoothed. Now, we actually know how to move to minimize the functions.

![Untitled 3 17.png](../../attachments/Untitled%203%2017.png)

## Rectifier Functions (ReLU)

- For this, we know how to minimize it to get the function to equal zero. As long as we are positive, we know to move left.
- Issues with this function:
    - We can possibly overshoot the zero point.
    - If you are negative, you don’t know that you should be moving right ot turn on the activation function.
- Basically, you know which way to turn off the function (move left), but you don’t know which way to turn it on.

![Untitled 4 16.png](../../attachments/Untitled%204%2016.png)

## Softplus Function

![Untitled 5 16.png](../../attachments/Untitled%205%2016.png)

## Leaky Rectifier Function

![Untitled 6 15.png](../../attachments/Untitled%206%2015.png)

## Arg/Soft Max

- Arg-max corresponds to a Heaviside decision. Soft-max is the smoothed version.

![Untitled 7 15.png](../../attachments/Untitled%207%2015.png)

- Softmax is differentiable, while argmax is not.

# Binary Classification (SVM)

![Untitled 8 15.png](../../attachments/Untitled%208%2015.png)

- This is the intuition between support vector machines.
    - In a decision boundary, the closest $x_i$﻿, the support vectors, are the borderline cases.
- This basically means that when you slide the plane back and forth, the first datapoint you hit are the support vectors.
    - You can place the plane equidistant between the support vectors.
- To get rid of the $\epsilon$﻿ parameter, divide it out so that we get a normal vector with different magnitude.
- We want the plane to make a good separation of the data, so we maximize the margin $\epsilon$﻿

![Untitled 9 15.png](../../attachments/Untitled%209%2015.png)

- There are constraints that we have to incorporate into our cost function. We can fold these constraints into unconstrained optimization using the Heaviside function.
    - $\hat{g}(c) = y_i(c^T x_i - b) \geq 1$﻿ are the inequality constraints on the optimization problem.

## Constrained Optimization with Inequalities

- We create the penalty term using the Heaviside function $H$﻿. Recall that $H$﻿ turns on when the thing inside is positive, and turns off when its negative.
    - This means $H(-g)$﻿ is zero when $g$﻿ is positive, and one when $g$﻿ is negative.

![Untitled 10 15.png](../../attachments/Untitled%2010%2015.png)

# Symbolic Differentiation

- Given the symbols, calculate the derivatives → this is really hard for computers to do.
    - We can do it by hand though; this is how we’re taught to compute derivatives.

![Untitled 11 15.png](../../attachments/Untitled%2011%2015.png)

- Functions whose derivatives can’t be approximated (such as a vertical line) can dramatically break ML libraries
    - When there are indeterminant results (division by zero), people often hack it by adding a small epsilon to the denominator.

![Untitled 12 15.png](../../attachments/Untitled%2012%2015.png)

- In the above, if we actually evaluate $h(2)$﻿ using the results, we’ll get $h(2) = \frac{f(2)}{g(2)} = 0$﻿
- However, since we know the symbols, we can see that

$h(t) = \frac{f(t)}{g(t)} = \frac{(t-2)(t+2)}{(t-2)} = t+2 \quad \to \quad h(2) = 4$

- Basically, ML libraries **do not know L’Hopital’s rule**.
    - If you program this example in Pytorch, you’ll find that $h'(2)$﻿ actually blows up to infinity.

## Code

- To prevent issues like this, if we have known derivatives, we should include that inside the part
    - For instance, replace $h(t)$﻿ and $h’(t)$﻿ with the known derivatives, instead of letting the code do it itself.

![Untitled 13 14.png](../../attachments/Untitled%2013%2014.png)

## Differentiate the right thing

- Instead of trying to invert or trying to differentiate the linear system, use what we know with PCA or the Power method to directly calculate it.
    - Instead of trying to differentiate the forward pass, analytically figure out the derivative in backpropagation and use that → will likely be faster and more efficient too.

![Untitled 14 13.png](../../attachments/Untitled%2014%2013.png)

# Finite Differences

- We can approximate derivatives similar to the secant method. We can just basically do algebra with the taylor expansions.

![Untitled 15 13.png](../../attachments/Untitled%2015%2013.png)

## Drawbacks

- The biggest issue with this method is that it’s still an approximation. It also contains truncation errors (in the form of the $O(h)$﻿ terms), whose effects may be unclear.
- Also, it requires you to discretize the space in order to figure out what a good value for $h$﻿ is.

![Untitled 16 13.png](../../attachments/Untitled%2016%2013.png)

# Automatic Differentiation (Backprop)

- This is what a lot of ML libraries do.

![Untitled 17 11.png](../../attachments/Untitled%2017%2011.png)

- The forward step is evaluation $F(c)$﻿, the backwards step is evaluating $\frac{\partial{F}}{\partial{c}}(c)$﻿.

![Untitled 18 9.png](../../attachments/Untitled%2018%209.png)

- There are still roundoff errors, which can accumulate here. This can lead to issues like vanishing gradients.

## Second Derivatives

- If the input is a vector for a vector-valued function, the Jacobian is a matrix. The Hessian is then a tensor that basically pushes the Jacobian into the page.

![Untitled 19 8.png](../../attachments/Untitled%2019%208.png)

- This can be quite intractible. The Hessian is $O(n^2)$﻿ in dimension, which is huge.

# Dropout

- Dropout is adding some stochasticity to the search directions. Makes the network robust to errors in the derivatives.

![Untitled 20 8.png](../../attachments/Untitled%2020%208.png)

# Function Layers

![Untitled 21 6.png](../../attachments/Untitled%2021%206.png)

## Classical Optimization

- In classical optimization, we lock the parameters, and our goal is to move $X_1$﻿ around so that we produce $X_4$﻿ that matches the $X_{target}$﻿.

![Untitled 22 5.png](../../attachments/Untitled%2022%205.png)

## Network Training

- In machine learning, we lock $X_1$﻿, and we perturb the parameters to make the function do the right thing.

![Untitled 23 5.png](../../attachments/Untitled%2023%205.png)

- We can think of this as multiple successive feed-forward neural networks.
    - We train each network $f_i$﻿ to minimize our cost function.
- We can optimize this by only tuning the parts that can be influenced by our parameters.
    - In this case, we are only really learning how to produce a good $X_3$﻿ given a $X_2$﻿. This good $X_3$﻿ should produce a good $X_4$﻿.

![Untitled 24 5.png](../../attachments/Untitled%2024%205.png)