---
Week: Week 8
---
# Recap of Optimization

- Given some cost function $f$﻿, we want to minimize it
- Take the Jacobian of the cost function $f$﻿ and set it equal to zero.
    - This gives us a set of nonlinear equations.
- Linearize the nonlinear equations using Newton’s Method, and solve to get search directions.
- Use the 1D search directions to make progress towards the root of the Jacobian nonlinear system.

# Smoothness

- To compute a derivative, the function needs to be differentiable → smooth function.
    - If the problem fits some special class, we can use specialty methods to solve them more easily.
    - In general, neural networks will be nonlinear systems, so we can’t really use specialty approaches.

![[attachments/Untitled 50.png|Untitled 50.png]]

## Biological Neurons

- There are some issues if we use biologically inspired methods → almost all of them are non-differentiable and not smooth.
- The increases frequency approach will have kinks in the derivatives.
    - This can be like putting your hand on a stove → the neuron response will fire with more frequency the hotter it is.
    - However, these changes in frequency are kinks, it’s not usually a smooth curve.

![[attachments/Untitled 1 17.png|Untitled 1 17.png]]

# Activation Functions

## Heaviside Function

- It’s not really possible to optimize for this function, since it’s zero derivative everywhere. We don’t know what direction to go.

![[attachments/Untitled 2 17.png|Untitled 2 17.png]]

## Sigmoid Function

- The sigmoid is a variant of Heaviside that is smoothed. Now, we actually know how to move to minimize the functions.

![[attachments/Untitled 3 17.png|Untitled 3 17.png]]

## Rectifier Functions (ReLU)

- For this, we know how to minimize it to get the function to equal zero. As long as we are positive, we know to move left.
- Issues with this function:
    - We can possibly overshoot the zero point.
    - If you are negative, you don’t know that you should be moving right ot turn on the activation function.
- Basically, you know which way to turn off the function (move left), but you don’t know which way to turn it on.

![[attachments/Untitled 4 16.png|Untitled 4 16.png]]

## Softplus Function

![[attachments/Untitled 5 16.png|Untitled 5 16.png]]

## Leaky Rectifier Function

![[attachments/Untitled 6 15.png|Untitled 6 15.png]]

## Arg/Soft Max

- Arg-max corresponds to a Heaviside decision. Soft-max is the smoothed version.

![[attachments/Untitled 7 15.png|Untitled 7 15.png]]

- Softmax is differentiable, while argmax is not.

# Binary Classification (SVM)

![[attachments/Untitled 8 15.png|Untitled 8 15.png]]

- This is the intuition between support vector machines.
    - In a decision boundary, the closest $x_i$﻿, the support vectors, are the borderline cases.
- This basically means that when you slide the plane back and forth, the first datapoint you hit are the support vectors.
    - You can place the plane equidistant between the support vectors.
- To get rid of the $\epsilon$﻿ parameter, divide it out so that we get a normal vector with different magnitude.
- We want the plane to make a good separation of the data, so we maximize the margin $\epsilon$﻿

![[attachments/Untitled 9 15.png|Untitled 9 15.png]]

- There are constraints that we have to incorporate into our cost function. We can fold these constraints into unconstrained optimization using the Heaviside function.
    - $\hat{g}(c) = y_i(c^T x_i - b) \geq 1$﻿ are the inequality constraints on the optimization problem.

## Constrained Optimization with Inequalities

- We create the penalty term using the Heaviside function $H$﻿. Recall that $H$﻿ turns on when the thing inside is positive, and turns off when its negative.
    - This means $H(-g)$﻿ is zero when $g$﻿ is positive, and one when $g$﻿ is negative.

![[attachments/Untitled 10 15.png|Untitled 10 15.png]]

# Symbolic Differentiation

- Given the symbols, calculate the derivatives → this is really hard for computers to do.
    - We can do it by hand though; this is how we’re taught to compute derivatives.

![[attachments/Untitled 11 15.png|Untitled 11 15.png]]

- Functions whose derivatives can’t be approximated (such as a vertical line) can dramatically break ML libraries
    - When there are indeterminant results (division by zero), people often hack it by adding a small epsilon to the denominator.

![[attachments/Untitled 12 15.png|Untitled 12 15.png]]

- In the above, if we actually evaluate $h(2)$﻿ using the results, we’ll get $h(2) = \frac{f(2)}{g(2)} = 0$﻿
- However, since we know the symbols, we can see that

$h(t) = \frac{f(t)}{g(t)} = \frac{(t-2)(t+2)}{(t-2)} = t+2 \quad \to \quad h(2) = 4$

- Basically, ML libraries **do not know L’Hopital’s rule**.
    - If you program this example in Pytorch, you’ll find that $h'(2)$﻿ actually blows up to infinity.

## Code

- To prevent issues like this, if we have known derivatives, we should include that inside the part
    - For instance, replace $h(t)$﻿ and $h’(t)$﻿ with the known derivatives, instead of letting the code do it itself.

![[attachments/Untitled 13 14.png|Untitled 13 14.png]]

## Differentiate the right thing

- Instead of trying to invert or trying to differentiate the linear system, use what we know with PCA or the Power method to directly calculate it.
    - Instead of trying to differentiate the forward pass, analytically figure out the derivative in backpropagation and use that → will likely be faster and more efficient too.

![[attachments/Untitled 14 13.png|Untitled 14 13.png]]

# Finite Differences

- We can approximate derivatives similar to the secant method. We can just basically do algebra with the taylor expansions.

![[attachments/Untitled 15 13.png|Untitled 15 13.png]]

## Drawbacks

- The biggest issue with this method is that it’s still an approximation. It also contains truncation errors (in the form of the $O(h)$﻿ terms), whose effects may be unclear.
- Also, it requires you to discretize the space in order to figure out what a good value for $h$﻿ is.

![[attachments/Untitled 16 13.png|Untitled 16 13.png]]

# Automatic Differentiation (Backprop)

- This is what a lot of ML libraries do.

![[attachments/Untitled 17 11.png|Untitled 17 11.png]]

- The forward step is evaluation $F(c)$﻿, the backwards step is evaluating $\frac{\partial{F}}{\partial{c}}(c)$﻿.

![[attachments/Untitled 18 9.png|Untitled 18 9.png]]

- There are still roundoff errors, which can accumulate here. This can lead to issues like vanishing gradients.

## Second Derivatives

- If the input is a vector for a vector-valued function, the Jacobian is a matrix. The Hessian is then a tensor that basically pushes the Jacobian into the page.

![[attachments/Untitled 19 8.png|Untitled 19 8.png]]

- This can be quite intractible. The Hessian is $O(n^2)$﻿ in dimension, which is huge.

# Dropout

- Dropout is adding some stochasticity to the search directions. Makes the network robust to errors in the derivatives.

![[attachments/Untitled 20 8.png|Untitled 20 8.png]]

# Function Layers

![[attachments/Untitled 21 6.png|Untitled 21 6.png]]

## Classical Optimization

- In classical optimization, we lock the parameters, and our goal is to move $X_1$﻿ around so that we produce $X_4$﻿ that matches the $X_{target}$﻿.

![[attachments/Untitled 22 5.png|Untitled 22 5.png]]

## Network Training

- In machine learning, we lock $X_1$﻿, and we perturb the parameters to make the function do the right thing.

![[attachments/Untitled 23 5.png|Untitled 23 5.png]]

- We can think of this as multiple successive feed-forward neural networks.
    - We train each network $f_i$﻿ to minimize our cost function.
- We can optimize this by only tuning the parts that can be influenced by our parameters.
    - In this case, we are only really learning how to produce a good $X_3$﻿ given a $X_2$﻿. This good $X_3$﻿ should produce a good $X_4$﻿.

![[attachments/Untitled 24 5.png|Untitled 24 5.png]]