---
Week: Week 1
---
# Knowledge Based Systems

![Untitled 51.png](../../attachments/Untitled%2051.png)

- Have a rule for each scenario (e.g. for addition of negatives, decimals, fractions, etc)
- It has **no inherent error**

# Machine Learning

![Untitled 1 18.png](../../attachments/Untitled%201%2018.png)

- We learn the rules based on training examples. This will have **inherent approximation error**
- Given just three examples, we can find the plane $z = x + y$﻿ and figure out how to do addition for every type of number.
- **Extrapolation:** away from the training data, the model function $z = f(x, y)$﻿ is likely to be inaccurate.

# KBs vs ML

![Untitled 2 18.png](../../attachments/Untitled%202%2018.png)

- KB fails when **we don’t know the rules**
    - E.g. if we have an unknown operation x # y, and we have a few examples, we can only use ML to learn it.

# Nearest Neighbor

![Untitled 3 18.png](../../attachments/Untitled%203%2018.png)

# Interpolation

![Untitled 4 17.png](../../attachments/Untitled%204%2017.png)

![Untitled 5 17.png](../../attachments/Untitled%205%2017.png)

![Untitled 6 16.png](../../attachments/Untitled%206%2016.png)

## Overfitting

![Untitled 7 16.png](../../attachments/Untitled%207%2016.png)

- In the case above, the data points actually look like they somewhat follow a quadratic. However, since we used too high of a degree, it overfit.

## Regularization

![Untitled 8 16.png](../../attachments/Untitled%208%2016.png)

![Untitled 9 16.png](../../attachments/Untitled%209%2016.png)

## Underfitting

![Untitled 10 16.png](../../attachments/Untitled%2010%2016.png)

## Nearest Neighbor

![Untitled 11 16.png](../../attachments/Untitled%2011%2016.png)

- In this example, imagine that the input is just 1D. Where we land on the number line is the point that we choose to return.

# Noise vs. Features

- It can be hard to differentiate between noise and features.

![Untitled 12 16.png](../../attachments/Untitled%2012%2016.png)

![Untitled 13 15.png](../../attachments/Untitled%2013%2015.png)

## Train-test-split

![Untitled 14 14.png](../../attachments/Untitled%2014%2014.png)

# Errors

![Untitled 15 14.png](../../attachments/Untitled%2015%2014.png)

![Untitled 16 14.png](../../attachments/Untitled%2016%2014.png)

![Untitled 17 12.png](../../attachments/Untitled%2017%2012.png)

# Robust Computational Approach

- Make sure the problem you’re solving is well-posed before you solve it with a computer. The error could grow exponentially.

![Untitled 18 10.png](../../attachments/Untitled%2018%2010.png)

- You also want a well-conditioned approach. You don’t want the errors to explode.

![Untitled 19 9.png](../../attachments/Untitled%2019%209.png)

- Algorithms also need to be stable and accurate.

![Untitled 20 9.png](../../attachments/Untitled%2020%209.png)

![Untitled 21 7.png](../../attachments/Untitled%2021%207.png)

## Ex: Vector Norms

![Untitled 22 6.png](../../attachments/Untitled%2022%206.png)

- Doing it the regular way can lead to overflow. To fix this, find the largest magnitude $x_i$﻿ and then divide the whole sum by that $z$﻿.

![Untitled 23 6.png](../../attachments/Untitled%2023%206.png)

# Learning Polynomial Interrpolation

![Untitled 24 6.png](../../attachments/Untitled%2024%206.png)

## Issue with matrix method

![Untitled 25 5.png](../../attachments/Untitled%2025%205.png)

- To see this, think of each column of the matrix as a vector. The right hand side is also a vector.

![Untitled 26 4.png](../../attachments/Untitled%2026%204.png)

![Untitled 27 3.png](../../attachments/Untitled%2027%203.png)

# Singular Matrices

![Untitled 28 2.png](../../attachments/Untitled%2028%202.png)

![Untitled 29 2.png](../../attachments/Untitled%2029%202.png)

## Near Singular Matrix

![Untitled 30 2.png](../../attachments/Untitled%2030%202.png)

# Changing basis for polynomial

- Using the monomial basis (just powers) is bad because it can become near-singular.

![Untitled 31 2.png](../../attachments/Untitled%2031%202.png)

- Note that each of the basis functions are non-linear. However, they are being combined in a linear way, which is why we can get matrices
- If you have a linear combination of potentially non-linear basis functions, the partial derivative with respect to the coefficients $c_i$﻿ is the non-linear basis functions $\phi_i$﻿

## Lagrange Basis

- Using a lagrange basis, we can solve for the polynoxmial without inverting a matrix.

![Untitled 32 2.png](../../attachments/Untitled%2032%202.png)

![Untitled 33 2.png](../../attachments/Untitled%2033%202.png)

- There’s a basis function for each data point. They’re 1 at the point and zero at all the other data points.

## Newton Basis

![Untitled 34 2.png](../../attachments/Untitled%2034%202.png)

- We can easily invert the matrix A because it’s a lower triangular.

## Summary

![Untitled 35 2.png](../../attachments/Untitled%2035%202.png)