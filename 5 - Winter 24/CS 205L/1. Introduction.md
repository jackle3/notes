---
Week: Week 1
---
# Knowledge Based Systems

![[attachments/Untitled 51.png|Untitled 51.png]]

- Have a rule for each scenario (e.g. for addition of negatives, decimals, fractions, etc)
- It has **no inherent error**

# Machine Learning

![[attachments/Untitled 1 18.png|Untitled 1 18.png]]

- We learn the rules based on training examples. This will have **inherent approximation error**
- Given just three examples, we can find the plane $z = x + y$﻿ and figure out how to do addition for every type of number.
- **Extrapolation:** away from the training data, the model function $z = f(x, y)$﻿ is likely to be inaccurate.

# KBs vs ML

![[attachments/Untitled 2 18.png|Untitled 2 18.png]]

- KB fails when **we don’t know the rules**
    - E.g. if we have an unknown operation x # y, and we have a few examples, we can only use ML to learn it.

# Nearest Neighbor

![[attachments/Untitled 3 18.png|Untitled 3 18.png]]

# Interpolation

![[attachments/Untitled 4 17.png|Untitled 4 17.png]]

![[attachments/Untitled 5 17.png|Untitled 5 17.png]]

![[attachments/Untitled 6 16.png|Untitled 6 16.png]]

## Overfitting

![[attachments/Untitled 7 16.png|Untitled 7 16.png]]

- In the case above, the data points actually look like they somewhat follow a quadratic. However, since we used too high of a degree, it overfit.

## Regularization

![[attachments/Untitled 8 16.png|Untitled 8 16.png]]

![[attachments/Untitled 9 16.png|Untitled 9 16.png]]

## Underfitting

![[attachments/Untitled 10 16.png|Untitled 10 16.png]]

## Nearest Neighbor

![[attachments/Untitled 11 16.png|Untitled 11 16.png]]

- In this example, imagine that the input is just 1D. Where we land on the number line is the point that we choose to return.

# Noise vs. Features

- It can be hard to differentiate between noise and features.

![[attachments/Untitled 12 16.png|Untitled 12 16.png]]

![[attachments/Untitled 13 15.png|Untitled 13 15.png]]

## Train-test-split

![[attachments/Untitled 14 14.png|Untitled 14 14.png]]

# Errors

![[attachments/Untitled 15 14.png|Untitled 15 14.png]]

![[attachments/Untitled 16 14.png|Untitled 16 14.png]]

![[attachments/Untitled 17 12.png|Untitled 17 12.png]]

# Robust Computational Approach

- Make sure the problem you’re solving is well-posed before you solve it with a computer. The error could grow exponentially.

![[attachments/Untitled 18 10.png|Untitled 18 10.png]]

- You also want a well-conditioned approach. You don’t want the errors to explode.

![[attachments/Untitled 19 9.png|Untitled 19 9.png]]

- Algorithms also need to be stable and accurate.

![[attachments/Untitled 20 9.png|Untitled 20 9.png]]

![[attachments/Untitled 21 7.png|Untitled 21 7.png]]

## Ex: Vector Norms

![[attachments/Untitled 22 6.png|Untitled 22 6.png]]

- Doing it the regular way can lead to overflow. To fix this, find the largest magnitude $x_i$﻿ and then divide the whole sum by that $z$﻿.

![[attachments/Untitled 23 6.png|Untitled 23 6.png]]

# Learning Polynomial Interrpolation

![[attachments/Untitled 24 6.png|Untitled 24 6.png]]

## Issue with matrix method

![[attachments/Untitled 25 5.png|Untitled 25 5.png]]

- To see this, think of each column of the matrix as a vector. The right hand side is also a vector.

![[attachments/Untitled 26 4.png|Untitled 26 4.png]]

![[attachments/Untitled 27 3.png|Untitled 27 3.png]]

# Singular Matrices

![[attachments/Untitled 28 2.png|Untitled 28 2.png]]

![[attachments/Untitled 29 2.png|Untitled 29 2.png]]

## Near Singular Matrix

![[attachments/Untitled 30 2.png|Untitled 30 2.png]]

# Changing basis for polynomial

- Using the monomial basis (just powers) is bad because it can become near-singular.

![[attachments/Untitled 31 2.png|Untitled 31 2.png]]

- Note that each of the basis functions are non-linear. However, they are being combined in a linear way, which is why we can get matrices
- If you have a linear combination of potentially non-linear basis functions, the partial derivative with respect to the coefficients $c_i$﻿ is the non-linear basis functions $\phi_i$﻿

## Lagrange Basis

- Using a lagrange basis, we can solve for the polynoxmial without inverting a matrix.

![[attachments/Untitled 32 2.png|Untitled 32 2.png]]

![[attachments/Untitled 33 2.png|Untitled 33 2.png]]

- There’s a basis function for each data point. They’re 1 at the point and zero at all the other data points.

## Newton Basis

![[attachments/Untitled 34 2.png|Untitled 34 2.png]]

- We can easily invert the matrix A because it’s a lower triangular.

## Summary

![[attachments/Untitled 35 2.png|Untitled 35 2.png]]