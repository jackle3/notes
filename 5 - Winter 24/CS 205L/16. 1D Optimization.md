---
Week: Week 7
---
- Instead of looking for roots in 1D, we are now looking for minima in 1D.

# Leveraging Root Finding

- We can find the roots of the derivative $g'(t)$﻿ to find the minima of $g(t)$﻿
    - We can use the Newton, Secant, and Bisection methods of root finding, etc.

![[attachments/Untitled 67.png|Untitled 67.png]]

- This is a pretty ill-conditioned, since we need to calculate the derivatives and even the second derivatives constantly.

# Unimodal

![[attachments/Untitled 1 34.png|Untitled 1 34.png]]

# Successive Parabolic Interpolation

- You can find a root by just finding where it goes from positive to negative.
- For minima, you need to find a parabola, where it curves down then curves up.

![[attachments/Untitled 2 34.png|Untitled 2 34.png]]

- Given two points, find the midpoint, and fit a parabola through those points.
    - When its unimodal, the minima will be less than the left and right. This parabola is concave up.

![[attachments/Untitled 3 34.png|Untitled 3 34.png]]

- On the left, we draw a parabola through $t_L$﻿, $t_M$﻿, and $t_R$﻿.
    - We find the minima of the parabola as an estimate for the actual minima, call that $t_{\min}$﻿
    - We then evaluate $g(t_{\min})$﻿, as seen in the red point.
    - Notice that $g(t_{\min}) \leq g(t_M)$﻿, so we shrink our interval and draw a new parabola.

## Discarding Intervals

![[attachments/Untitled 4 33.png|Untitled 4 33.png]]

# Golden Section Search

- We’re trying to solve for $\lambda$﻿, the fraction between of $\delta$﻿ of $t_{M1}$﻿

![[attachments/Untitled 5 33.png|Untitled 5 33.png]]

![[attachments/Untitled 6 32.png|Untitled 6 32.png]]

- Below is a visualization of this method:
    
    - Since $g(t_{M2})$﻿ is smaller, we discard the interval the first interval and shift our search.
    
    ![[attachments/Untitled 7 32.png|Untitled 7 32.png]]
    

# Mixed Methods

![[attachments/Untitled 8 32.png|Untitled 8 32.png]]

# Function/Derivative Requirements

![[attachments/Untitled 9 31.png|Untitled 9 31.png]]

## More Useful Derivatives

- The Jacobian is a matrix. Taking the partial of that now gives you a three dimensional Hessian, called a rank 3 tensor.
    - Suppose you have a vector $v$﻿.
    - When you take the derivative, you get the Jacobian, which extends each row of $v$﻿ with each derivative.
    - When you take the derivative again, you get the Hessian, which extends each entry of the Jacobian into three dimensions (ie. pushing it into the board).
        - This creates a 3D block of derivatives/numbers.

![[attachments/Untitled 10 30.png|Untitled 10 30.png]]

# Recall: Nonlinear Systems (Roots)

![[attachments/Untitled 11 30.png|Untitled 11 30.png]]

## Nonlinear Systems (Minima)

![[attachments/Untitled 12 30.png|Untitled 12 30.png]]

# Recall: Optimization (Roots)

![[attachments/Untitled 13 29.png|Untitled 13 29.png]]

## Optimization (Minima)

- Notice that for the second derivative, we now need to find the rank-3 tensor of third derivatives.

![[attachments/Untitled 14 28.png|Untitled 14 28.png]]

- We can now do option 3, where we minimize $g(t) = \hat{f}$﻿ directly.
    - The first derivative is just the Jacobian.
    - The second derivative is just the Hessian.
- For the last option, recall that $J_{\hat{f}}^T$﻿ is the same as $F$﻿, and $H_{\hat{f}}^T$﻿ is the same as $J_F$﻿.