---
Week: Week 7
---
- Instead of looking for roots in 1D, we are now looking for minima in 1D.

# Leveraging Root Finding

- We can find the roots of the derivative $g'(t)$﻿ to find the minima of $g(t)$﻿
    - We can use the Newton, Secant, and Bisection methods of root finding, etc.

![Untitled 67.png](attachments/Untitled%2067.png)

- This is a pretty ill-conditioned, since we need to calculate the derivatives and even the second derivatives constantly.

# Unimodal

![Untitled 1 34.png](attachments/Untitled%201%2034.png)

# Successive Parabolic Interpolation

- You can find a root by just finding where it goes from positive to negative.
- For minima, you need to find a parabola, where it curves down then curves up.

![Untitled 2 34.png](attachments/Untitled%202%2034.png)

- Given two points, find the midpoint, and fit a parabola through those points.
    - When its unimodal, the minima will be less than the left and right. This parabola is concave up.

![Untitled 3 34.png](attachments/Untitled%203%2034.png)

- On the left, we draw a parabola through $t_L$﻿, $t_M$﻿, and $t_R$﻿.
    - We find the minima of the parabola as an estimate for the actual minima, call that $t_{\min}$﻿
    - We then evaluate $g(t_{\min})$﻿, as seen in the red point.
    - Notice that $g(t_{\min}) \leq g(t_M)$﻿, so we shrink our interval and draw a new parabola.

## Discarding Intervals

![Untitled 4 33.png](attachments/Untitled%204%2033.png)

# Golden Section Search

- We’re trying to solve for $\lambda$﻿, the fraction between of $\delta$﻿ of $t_{M1}$﻿

![Untitled 5 33.png](attachments/Untitled%205%2033.png)

![Untitled 6 32.png](attachments/Untitled%206%2032.png)

- Below is a visualization of this method:
    
    - Since $g(t_{M2})$﻿ is smaller, we discard the interval the first interval and shift our search.
    
    ![Untitled 7 32.png](attachments/Untitled%207%2032.png)
    

# Mixed Methods

![Untitled 8 32.png](attachments/Untitled%208%2032.png)

# Function/Derivative Requirements

![Untitled 9 31.png](attachments/Untitled%209%2031.png)

## More Useful Derivatives

- The Jacobian is a matrix. Taking the partial of that now gives you a three dimensional Hessian, called a rank 3 tensor.
    - Suppose you have a vector $v$﻿.
    - When you take the derivative, you get the Jacobian, which extends each row of $v$﻿ with each derivative.
    - When you take the derivative again, you get the Hessian, which extends each entry of the Jacobian into three dimensions (ie. pushing it into the board).
        - This creates a 3D block of derivatives/numbers.

![Untitled 10 30.png](attachments/Untitled%2010%2030.png)

# Recall: Nonlinear Systems (Roots)

![Untitled 11 30.png](attachments/Untitled%2011%2030.png)

## Nonlinear Systems (Minima)

![Untitled 12 30.png](attachments/Untitled%2012%2030.png)

# Recall: Optimization (Roots)

![Untitled 13 29.png](attachments/Untitled%2013%2029.png)

## Optimization (Minima)

- Notice that for the second derivative, we now need to find the rank-3 tensor of third derivatives.

![Untitled 14 28.png](attachments/Untitled%2014%2028.png)

- We can now do option 3, where we minimize $g(t) = \hat{f}$﻿ directly.
    - The first derivative is just the Jacobian.
    - The second derivative is just the Hessian.
- For the last option, recall that $J_{\hat{f}}^T$﻿ is the same as $F$﻿, and $H_{\hat{f}}^T$﻿ is the same as $J_F$﻿.