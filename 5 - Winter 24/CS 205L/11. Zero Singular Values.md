---
Week: Week 6
---
# Underdetermined Systems

- Co-located means the points are identical.

![Untitled 53.png](attachments/Untitled%2053.png)

- Co-linear means the points lie on the same line.
    - In other words, it lies on the span of the two columns of the Vandemonde matrix
        - In range of matrix A, where columns of A are basis function.
    - When it’s not colinear, there is no unique solution. However, **there is a least squares**.

![Untitled 1 20.png](attachments/Untitled%201%2020.png)

- In the example above, the least squares solution would be the flat line through the point.

# Variable Classification

![Untitled 2 20.png](attachments/Untitled%202%2020.png)

- Instead of using unique, no, infinite solutions → focus on what you can do with each specific variables of the system.
    - You can try to reconcile the disagreement in $c_1$﻿ using something like least squares.
    - You can unique solve $c_2$﻿.
    - You can do anything for $c_3$﻿.

# Understanding Underdetermined Systems

- Underdetermined systems are systems that have singular values equal to zero.

![Untitled 3 20.png](attachments/Untitled%203%2020.png)

- Recall that $V^T c = \hat{c}$﻿. Therefore, $c = V\hat{c}$﻿. As such, each $\hat{c}_k$﻿ tells us how much to scale and include column $k$﻿ of $V$﻿ in the final solution.
- The purpose of **regularization** is to make small $\sigma_k$﻿ behave nicely as if it were zero $\sigma_k$﻿
- In tall matrices, $\Sigma \hat{c}$﻿ might have rows that are all zeros. This tells us that there is a nonzero residual.
- For wide matrices, it can have columns of zeros, which leaves some $\hat{c}_k$﻿ underdetermined.
    - It’s not possible for wide matrices to be full rank.
    - However, you can collect more data (e.g. more rows) to make the wide matrix square or tall.

![Untitled 4 19.png](attachments/Untitled%204%2019.png)

- The most general form of the SVD is the one highlighted in red. It’s a diagonal and full rank, and potentially padded with zeros.

## Minimum Norm Solution

- This solution is where we set the undetermined $\hat{c}_z$﻿ to zero, which minimizes the norm.
    - When a variable is undetermined, it has infinite solutions. Minimum norm is when we set that variable to zero.

![Untitled 5 19.png](attachments/Untitled%205%2019.png)

- This basically means → if a variable does not matter for our system, we just ignore it and don’t report any information about it.

![Untitled 6 18.png](attachments/Untitled%206%2018.png)

## Pseudo-Inverse

- The equation above showed us that $c = A^+ b$﻿

![Untitled 7 18.png](attachments/Untitled%207%2018.png)

- If the matrix is invertible, meaning its square and full rank, then $A^+ = A^{-1}$﻿
- For any matrix, we can get a pseudo-inverse which tells us $c = A^+ b$﻿

# Sum of Rank One Matrices

$A = U \Sigma V^T = \sum_k \sigma_k u_k v_k^T= \sum_{\sigma_k \neq 0} \sigma_k u_k v_k^T$

![Untitled 8 18.png](attachments/Untitled%208%2018.png)

- An outer product $uv^T$﻿ builds a matrix where each column of the matrix is the column $u$﻿, scaled by the entries of $v$﻿. The resulting matrix is rank 1 (all columns are dependent).
- $Ac$﻿ projects $c$﻿ onto the basis vectors in $V$﻿, scales by the associated singular values, and uses those results as weights on the basis vectors in $U$﻿

# Matrix Approximation

- The first singular value is much bigger than the second, and so represents the vast  
    majority of what  
    $A$﻿ does (note, the vectors in $U$﻿ and $V$﻿ are unit length)
- Thus, one could approximate $A$﻿ quite well by only using the terms associated with the largest singular values.

![Untitled 9 18.png](attachments/Untitled%209%2018.png)

- Dropping the smaller singular values is a form of regularization to make $A^+$﻿ better conditioned.
    - We’re essentially throwing away variables that we don’t have good measurements for.

## Rank-One Updates (PCA)

- We can iterative add one term at a time until we run out of time. This would basically give us the best approximation we can get given the time that we have.

![Untitled 10 18.png](attachments/Untitled%2010%2018.png)

- This is the basic structure of principal component analysis in real time.

# Computing the SVD

- We never compute and use the full SVD.
- Recall that this is the eigenvalue problems that we have to compute to solve for $V$﻿, $U$﻿, and $\Sigma$﻿
    
    ![Untitled 11 18.png](attachments/Untitled%2011%2018.png)
    
- For efficiency, we should use whichever has smaller dimensions between $A^TA$﻿ and $AA^T$﻿
    
    ![Untitled 12 18.png](attachments/Untitled%2012%2018.png)
    

## Finding Eigenvectors from Eigenvalues

![Untitled 13 17.png](attachments/Untitled%2013%2017.png)

## Condition Number of Eigenproblems

- The condition number for finding an eigenvalue is different from the condition number for a solving linear system.
    - For a linear system, that’s the condition number for inverting the matrix (bc u divide by the $\sigma$﻿’s during inversion)
    - For eigenproblems, it’s the value for finding eigenvalues and eigenvectors.

![Untitled 14 16.png](attachments/Untitled%2014%2016.png)

- Note that eigenvalues and eigenvectors are **only defined for square matrices.**
    - Right eigenvectors are $Av = \lambda v$﻿ → this is the normal one
    - Left eigenvectors are $v^TA = \lambda v^T$﻿
    - Both have dimension $n \times 1$﻿, and are both associated with the same eigenvalues of the matrix $A$﻿
- If the **normalized** left and right eigenvectors point in the same direction, $v_L^T v_R = 1$﻿, so the condition number is $1$﻿.
    - If they are orthogonal, then $v_L^T v_R = 0$﻿ so the condition number is infinite, and it’s infinitely hard to solve the problem.
- The matrices $AA^T$﻿ and $A^TA$﻿ are symmetric, so they have perfect condition numbers.

# Similarity Transforms

![Untitled 15 16.png](attachments/Untitled%2015%2016.png)

- For the real and symmetric $A^TA$﻿, the transform is $T = V$﻿
- For $AA^T$﻿, the transform is $T = U$﻿

![Untitled 16 16.png](attachments/Untitled%2016%2016.png)

## Using QR Iteration

![Untitled 17 14.png](attachments/Untitled%2017%2014.png)

# Power Method

- The power method works by repeated left-multiplying the vector by $\hat{A}$﻿.

![Untitled 18 12.png](attachments/Untitled%2018%2012.png)

- For **rank-one updates** or approximations of A
    - Use power method to calculate the largest eigenvalue, and calculate that component of the sum.
    - Then, do deflation to remove the largest eigenvalue, then repeat the power method again for the next largest eigenvalue value.
- We can define $c_o$﻿ in terms of the basis vectors $v_k$﻿, which are also the eigenvectors of $\hat{A}$﻿
- As $q \to \infty$﻿, the only term of the sum that survives is $\alpha_{\max} (\frac{\lambda_{\max}}{\lambda_{\max}})^q v_{\max} = \alpha_{\max} v_{\max}$﻿
- In the notes below, $c^q = v$﻿ and $c^{q+1} = w$﻿
    
    ![Untitled 19 11.png](attachments/Untitled%2019%2011.png)
    

![Untitled 20 11.png](attachments/Untitled%2020%2011.png)

- For inverse iteration, instead of simply multiplying $\hat{A}$﻿, we have to solve the linear system.