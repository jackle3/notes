---
Week: Week 8
---
# Recap

- **1D Root Finding:**
    
    ![Untitled 64.png](attachments/Untitled%2064.png)
    
- **1D Optimization:**
    
    ![Untitled 1 31.png](attachments/Untitled%201%2031.png)
    
- **Nonlinear Systems:**
    
    ![Untitled 2 31.png](attachments/Untitled%202%2031.png)
    

# Quasi-Newton Methods

- These methods get more aggressive at approximation the Jacobian.

![Untitled 3 31.png](attachments/Untitled%203%2031.png)

- Basically hack the equation to avoid putting a real Jacobian.

## Broyden’s Method

- A classical method is to simply set $\beta = 0$﻿. It’s essentially a secant method for nonlinear systems.
    - It’s going to compute a rank-one secant to approximate the Jacobian.
- The Jacobian is not necessarily square here.

![Untitled 4 30.png](attachments/Untitled%204%2030.png)

- This iterative method allows you to never have to actually compute a derivative to get the Jacobian.
    - First, set $J^0 = I$﻿. Then, solve the linear system to get search direction.
    - Using the search direction, find some $c^{q+1}$﻿. Then, overwrite the search direction with that.
    - Using that, update the Jacobian guess using a rank-one update.
        - The term $(F(c^{q+1})) - F(c^q) - J^q \Delta c^q)(\Delta c^q)^T$﻿ is rank-one.

# More Recap

- **Optimization:** this now requires both the Jacobian and the Hessian. We can get the Jacobian easily with Tensorflow and PyTorch, but getting the Hessian is very hard.
    
    ![Untitled 5 30.png](attachments/Untitled%205%2030.png)
    

# Quasi-Newton for Optimization

![Untitled 6 29.png](attachments/Untitled%206%2029.png)

- Notice that it goes one step further and approximates the inverse Hessian. This way, we don’t even have to solve the linear system to find $\Delta c^q$﻿
- One important thing is that $H_f$﻿ is always square. This makes it easier to work with.

## Broyden’s Method for Optimization

- Instead of computing the Hessian and second derivatives, we use Quasi-Newton to approximate the Hessian.
- Instead of using $\Delta F$﻿ for our secant, we use $\Delta J_f$﻿

![Untitled 7 29.png](attachments/Untitled%207%2029.png)

- We can follow similar steps to approximate the inverse Hessian.
    - The only thing you change is the update formula and the secant equation.

![Untitled 8 29.png](attachments/Untitled%208%2029.png)

- This allows us to fully skip the linear algebra and solving the linear system. Simply multiply the approximated inverse Hessian by the change in the Jacobian.

## Symmetric Rank 1

- The only thing that changes from Broyden’s here is the update formula.

![Untitled 9 28.png](attachments/Untitled%209%2028.png)

## DFP

- Again, the only thing that changes is the update formula.

![Untitled 10 27.png](attachments/Untitled%2010%2027.png)

## BFGS

- This is the one that is mostly used.

![Untitled 11 27.png](attachments/Untitled%2011%2027.png)

## L-BFGS

- We can store the vectors that make up the matrix, storing just the vectors instead of the full matrix.
    - Simply need to store the vectors $\Delta c$﻿ and $\Delta J$﻿, and we can use these to apply the inverse Hessian instead of building the full inverse Hessian.
- To make this even more efficient, we can discard older search directions. In practice, we keep the last 10 search directions to build a rank-10 approximation of the Hessian inverse.

![Untitled 12 27.png](attachments/Untitled%2012%2027.png)

# Gradient/Steepest Descent

- Gradient descent essentially repeatedly applies the first step of all the aforementioned methods. It does not update or approximate $H$﻿, just approximates it as $H = I$﻿.

![Untitled 13 26.png](attachments/Untitled%2013%2026.png)

## Coordinate Descent

![Untitled 14 25.png](attachments/Untitled%2014%2025.png)

# Nonlinear Least Squares

- The data term is typically a nonlinear least square term. This data term is one of the terms in the objective function. There are also regularization terms, constraints terms, etc.

![Untitled 15 24.png](attachments/Untitled%2015%2024.png)

- $f$﻿ can be a vector valued function. To easily minimze this, use $f^Tf$﻿ to get a scalar-valued.

![Untitled 16 24.png](attachments/Untitled%2016%2024.png)

- Example, where $m = 100$﻿ and $\hat{m} = 3$﻿.
    - $f$﻿ is a function that takes in 100 datapoints, each of which outputs something in $\R^3$﻿.
    - We stack the outputs in $\R^3$﻿ of each of the 100 datapoints to make $\tilde{f}$﻿.
        - $\tilde{f}$﻿ will be outputting something in $\R^{300}$﻿
- This essentially allows us to remove the summation notation. Before, we had to sum over all the datapoints. Now, we already stacked all the datapoints.

![Untitled 17 21.png](attachments/Untitled%2017%2021.png)

- Note that $\tilde{f}$﻿ produces a column vector. Therefore, the Jacobian is a matrix.
    - The Jacobian of $\hat{f}$﻿, the thing you want to minimize, is just equal to $J_{\tilde{f}}^T \tilde{f}$﻿, the original equation.

## Gauss Newton

- A special approach that works for nonlinear least squares.
    - We replace $H_{\hat{f}}$﻿ with $J_{\tilde{f}}^T J_{\tilde{f}}$﻿.
- It takes $\tilde{f}(c)$﻿ and does a Taylor expansion about $c^q$﻿. It uses the first order expansion.

![Untitled 18 19.png](attachments/Untitled%2018%2019.png)

- This allows us to get rid of the second derivatives, approximation it using the Jacobian.

![Untitled 19 17.png](attachments/Untitled%2019%2017.png)

- With the QR approach, we minimize the residual and solve for the red equation.

## Weighted Gauss Newton

- Similar to regular least squares, you can add a diagonal matrix to row scale, giving greater importance to different equations.
    - Notice the equation si the same as the normal equations for weighted least squares.

![Untitled 20 17.png](attachments/Untitled%2020%2017.png)

## Regularization Gauss Newton (Levenberg-Marquardt)

- If $J$﻿ is not full rank, we can add regularization underneath $J$﻿ and iterate that.

![Untitled 21 14.png](attachments/Untitled%2021%2014.png)