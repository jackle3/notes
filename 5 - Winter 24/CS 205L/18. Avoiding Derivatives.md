---
Week: Week 8
---
# Recap

- **1D Root Finding:**
    
    ![[attachments/Untitled 64.png|Untitled 64.png]]
    
- **1D Optimization:**
    
    ![[attachments/Untitled 1 31.png|Untitled 1 31.png]]
    
- **Nonlinear Systems:**
    
    ![[attachments/Untitled 2 31.png|Untitled 2 31.png]]
    

# Quasi-Newton Methods

- These methods get more aggressive at approximation the Jacobian.

![[attachments/Untitled 3 31.png|Untitled 3 31.png]]

- Basically hack the equation to avoid putting a real Jacobian.

## Broyden’s Method

- A classical method is to simply set $\beta = 0$﻿. It’s essentially a secant method for nonlinear systems.
    - It’s going to compute a rank-one secant to approximate the Jacobian.
- The Jacobian is not necessarily square here.

![[attachments/Untitled 4 30.png|Untitled 4 30.png]]

- This iterative method allows you to never have to actually compute a derivative to get the Jacobian.
    - First, set $J^0 = I$﻿. Then, solve the linear system to get search direction.
    - Using the search direction, find some $c^{q+1}$﻿. Then, overwrite the search direction with that.
    - Using that, update the Jacobian guess using a rank-one update.
        - The term $(F(c^{q+1})) - F(c^q) - J^q \Delta c^q)(\Delta c^q)^T$﻿ is rank-one.

# More Recap

- **Optimization:** this now requires both the Jacobian and the Hessian. We can get the Jacobian easily with Tensorflow and PyTorch, but getting the Hessian is very hard.
    
    ![[attachments/Untitled 5 30.png|Untitled 5 30.png]]
    

# Quasi-Newton for Optimization

![[attachments/Untitled 6 29.png|Untitled 6 29.png]]

- Notice that it goes one step further and approximates the inverse Hessian. This way, we don’t even have to solve the linear system to find $\Delta c^q$﻿
- One important thing is that $H_f$﻿ is always square. This makes it easier to work with.

## Broyden’s Method for Optimization

- Instead of computing the Hessian and second derivatives, we use Quasi-Newton to approximate the Hessian.
- Instead of using $\Delta F$﻿ for our secant, we use $\Delta J_f$﻿

![[attachments/Untitled 7 29.png|Untitled 7 29.png]]

- We can follow similar steps to approximate the inverse Hessian.
    - The only thing you change is the update formula and the secant equation.

![[attachments/Untitled 8 29.png|Untitled 8 29.png]]

- This allows us to fully skip the linear algebra and solving the linear system. Simply multiply the approximated inverse Hessian by the change in the Jacobian.

## Symmetric Rank 1

- The only thing that changes from Broyden’s here is the update formula.

![[attachments/Untitled 9 28.png|Untitled 9 28.png]]

## DFP

- Again, the only thing that changes is the update formula.

![[attachments/Untitled 10 27.png|Untitled 10 27.png]]

## BFGS

- This is the one that is mostly used.

![[attachments/Untitled 11 27.png|Untitled 11 27.png]]

## L-BFGS

- We can store the vectors that make up the matrix, storing just the vectors instead of the full matrix.
    - Simply need to store the vectors $\Delta c$﻿ and $\Delta J$﻿, and we can use these to apply the inverse Hessian instead of building the full inverse Hessian.
- To make this even more efficient, we can discard older search directions. In practice, we keep the last 10 search directions to build a rank-10 approximation of the Hessian inverse.

![[attachments/Untitled 12 27.png|Untitled 12 27.png]]

# Gradient/Steepest Descent

- Gradient descent essentially repeatedly applies the first step of all the aforementioned methods. It does not update or approximate $H$﻿, just approximates it as $H = I$﻿.

![[attachments/Untitled 13 26.png|Untitled 13 26.png]]

## Coordinate Descent

![[attachments/Untitled 14 25.png|Untitled 14 25.png]]

# Nonlinear Least Squares

- The data term is typically a nonlinear least square term. This data term is one of the terms in the objective function. There are also regularization terms, constraints terms, etc.

![[attachments/Untitled 15 24.png|Untitled 15 24.png]]

- $f$﻿ can be a vector valued function. To easily minimze this, use $f^Tf$﻿ to get a scalar-valued.

![[attachments/Untitled 16 24.png|Untitled 16 24.png]]

- Example, where $m = 100$﻿ and $\hat{m} = 3$﻿.
    - $f$﻿ is a function that takes in 100 datapoints, each of which outputs something in $\R^3$﻿.
    - We stack the outputs in $\R^3$﻿ of each of the 100 datapoints to make $\tilde{f}$﻿.
        - $\tilde{f}$﻿ will be outputting something in $\R^{300}$﻿
- This essentially allows us to remove the summation notation. Before, we had to sum over all the datapoints. Now, we already stacked all the datapoints.

![[attachments/Untitled 17 21.png|Untitled 17 21.png]]

- Note that $\tilde{f}$﻿ produces a column vector. Therefore, the Jacobian is a matrix.
    - The Jacobian of $\hat{f}$﻿, the thing you want to minimize, is just equal to $J_{\tilde{f}}^T \tilde{f}$﻿, the original equation.

## Gauss Newton

- A special approach that works for nonlinear least squares.
    - We replace $H_{\hat{f}}$﻿ with $J_{\tilde{f}}^T J_{\tilde{f}}$﻿.
- It takes $\tilde{f}(c)$﻿ and does a Taylor expansion about $c^q$﻿. It uses the first order expansion.

![[attachments/Untitled 18 19.png|Untitled 18 19.png]]

- This allows us to get rid of the second derivatives, approximation it using the Jacobian.

![[attachments/Untitled 19 17.png|Untitled 19 17.png]]

- With the QR approach, we minimize the residual and solve for the red equation.

## Weighted Gauss Newton

- Similar to regular least squares, you can add a diagonal matrix to row scale, giving greater importance to different equations.
    - Notice the equation si the same as the normal equations for weighted least squares.

![[attachments/Untitled 20 17.png|Untitled 20 17.png]]

## Regularization Gauss Newton (Levenberg-Marquardt)

- If $J$﻿ is not full rank, we can add regularization underneath $J$﻿ and iterate that.

![[attachments/Untitled 21 14.png|Untitled 21 14.png]]