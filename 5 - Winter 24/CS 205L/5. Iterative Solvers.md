---
Week: Week 3
---
# Iterative vs. Direct Solvers

- The words solver and method are used interchangeably.

![Untitled 63.png](../../attachments/Untitled%2063.png)

- Iterative solvers basically refer to things like gradient descent, where we solve things iteratively.

## Issues with Direct Methods

![Untitled 1 30.png](../../attachments/Untitled%201%2030.png)

# Residual and Solution Error

- The residual tells you the error in the **equations**
    - This is the error involving $A$﻿, between the LHS with our guess and the RHS.

![Untitled 2 30.png](../../attachments/Untitled%202%2030.png)

- The actual error would be the difference btween the current guess and the solution.

![Untitled 3 30.png](../../attachments/Untitled%203%2030.png)

- In other words, the residual is the solution error transformed in to the space that $b$﻿ lives in
    - $A$﻿ maps the solution error into the space the residual lives in (the range of A)

## 1D example

- Since the residual is the error in another space:
    - You can have small error with big residual
        - This means big error in the equations, but little error in the solutions.
    - You can have big error with small resdiual
        - This means little error in the equations, but big error in the solutions.

![Untitled 4 29.png](../../attachments/Untitled%204%2029.png)

- In the left example, $a = 2$﻿. In the right example, $a = 1/5$﻿.

## Diagonalizing the Residual/Error Equation

![Untitled 5 29.png](../../attachments/Untitled%205%2029.png)

- In the above, $\hat{r}_k^q = U_k \cdot r^q$﻿, the dot product of the $k$﻿th column of $U$﻿ with $r^q$﻿.
- Similarly, $\hat{e}^q_k = V_k \cdot e^q$﻿
- When $\sigma_k$﻿ is small, the residual error $\hat{r}_k^q$﻿ can be small while the solution error $\hat{e}_k^q$﻿ can be big.

# Line Search

- Gradient descent methods basically use line search.
    
    ![Untitled 6 28.png](../../attachments/Untitled%206%2028.png)
    
    - $\alpha^q$﻿ is the learning rate → how much you move in the search direction $s^q$﻿
    - You can do some modifications to see **how the error updates** and **how the residual updates** during the local search.
- The optimal solution (though impossible) is to just directly move to minimize the error.
    
    ![Untitled 7 28.png](../../attachments/Untitled%207%2028.png)
    
    - If we knew the error, we’d be able to directly solve for the solution.
- Instead, we will try to minimize the residual.
    
    ![Untitled 8 28.png](../../attachments/Untitled%208%2028.png)
    
    - This tells you how much to move in the $s^q$﻿ direction.

## Steepest Descent

- The direction of the steepest descent is just the direction of the residual.

![Untitled 9 27.png](../../attachments/Untitled%209%2027.png)

- Iteratively move in that direction until the residual is small.

![Untitled 10 26.png](../../attachments/Untitled%2010%2026.png)

- The alternate equation above is just the equation for the residual defined in line search.
- The drawback with this is that it will go in the same direction a lot of times, which can be inefficient.

# Conjugate Gradients

- This is a very efficient and robust iterative method for **SPD systems**.
- At its core, it’s just a smarter way to do steepest descent search.
    - Instead of searching in direction $s^q = r^q$﻿, instead choose A-orthogonal search directions.

![Untitled 11 26.png](../../attachments/Untitled%2011%2026.png)

- Since we search each direction successively, the most ideal is to search in orthogonal search directions, so that we don’t have repeated work.
- Instead of finding the actual orthogonal (which is hard), find the A-orthogonal directions.
    - This is the inner product weighted on A, an SPD matrix → $<s^q, s^{\hat{q}}>_A = s^{q^\top}As^{\hat{q}}$﻿
- Intuitively, the reason why the conjugate gradient method can converge much faster than the steepest descent method lies in how it chooses the search directions.
    - While the steepest descent method moves directly towards the minimum in the direction of the negative gradient, it can lead to a zig-zagging path, especially in narrow valleys of the objective function.
    - The conjugate gradient method, on the other hand, selects search directions that are conjugate to each other with respect to the matrix defining the system.
        - This ensures that progress made in one direction is not undone when making progress in another direction.
        - As a result, the conjugate gradient method often takes more direct paths towards the minimum, achieving faster convergence compared to the steepest descent method, especially for large systems.

## Error Analysis for CG

- We can make a A-orthonal basis of $\R^n$﻿, consisting of our search directions. Note that these directions are not necessarily orthogonal → they are orthogonal when weighted on A.
    
    ![Untitled 12 26.png](../../attachments/Untitled%2012%2026.png)
    
- The initial error is just a linear combination of the basis vectors $s^{\hat{q}}$﻿ with coefficients $\beta^{\hat{q}}$﻿
    - If we take any of the search directions and take the A-weighted inner product with the error, we get a nice result.
    - Since the error is made up of orthogonal basis vectors, when you dot product it with a basis vector, only that one basis vector will survive. All the other ones will go to zero.
- Recall that $e^{q + 1} = e^q + \alpha^{q} s^{q}$﻿
    
    ![Untitled 13 25.png](../../attachments/Untitled%2013%2025.png)
    
    - Notice that for the inner product, the sum gets dropped to zero because $s^q$﻿ does not appear in the sum.
- Now recall that $r^{q + 1} = r^q - \alpha^{q} As^{q}$﻿. Using what was shown previously when the residual is orthogonal to $s^q$﻿, we have:
    
    ![Untitled 14 24.png](../../attachments/Untitled%2014%2024.png)
    
    - Also recall that $r = -Ae$﻿, so we can further simplify this into the A-weighted inner product.
    - This tells you that the learning rate is the negative of the basis coefficient for the search direction.

![Untitled 15 23.png](../../attachments/Untitled%2015%2023.png)

# Gram-Schmidt

- We can use Gram-Schmidt to get an A-orthogonal basis. Just do weighted inner products instead of regular inner products.

![Untitled 16 23.png](../../attachments/Untitled%2016%2023.png)

- For every $q$﻿, we want to get a search direction $s^q$﻿ that is A-orthogonal to all the prior search directions $s^1$﻿ to $s^{q-1}$﻿.
    - We are turning the red $\bar{S}^q$﻿, which is not A-orthogonal, into the $s^q$﻿, which is A-orthogonal

## Gram-Schmidt for CG

- First, the candidate search direction will just be the residual $r^q$﻿.
    
    ![Untitled 17 20.png](../../attachments/Untitled%2017%2020.png)
    
- Then, we make that candidate search direction A-orthogonal to the previous search directions to get $s^q$﻿.
    
    ![Untitled 18 18.png](../../attachments/Untitled%2018%2018.png)
    
    - However, notice that this first equation still has the full summation, which can be an issue with computation limits.
- We can optimize this → we don’t need the full Gram-Schmidt formula to get the answer
    - When calculating for CG, all the terms in the sum go away except for the very last term.
    - Gram-Schmidt doesn’t require going through all of the prior directions → just need to look at the the last term $q-1$﻿
    - We avoid computing all of those intermediate values, to prevent the smaller errors that may happen from intermediate steps.
- To see the optimization, take the entire formula and dot product it with an arbitrary $r^{\tilde{q}}$﻿
    
    ![Untitled 19 16.png](../../attachments/Untitled%2019%2016.png)
    
    - Recall that $r^{\tilde{q}}$﻿ is orthogonal to all prior search directions. If $r^{\tilde{q}}$﻿ is bigger than $q$﻿, the $s^q \cdot r^{\tilde{q}}$﻿ just zeroes out, and gets removed.
    - This basically just tells you that $s^q \cdot r^q = r^q \cdot r^q$﻿
- Then, we use the standard recursion formula from the residual, as defined previous
    
    ![Untitled 20 16.png](../../attachments/Untitled%2020%2016.png)
    
    - Now recall that $r^{\tilde{q}}$﻿ is orthogonal to all prior residuals. If $r^{\tilde{q}}$﻿ is bigger than $q$﻿, the $r^{\tilde{q}} \cdot r^q$﻿ just zeroes out, and gets removed.
        - This tells you that $\langle r^{\tilde{q}}, s^{q-1}\rangle_A$﻿ is zero as long as $\tilde{q} > q$﻿
        - This also tells you that only the last term in the summation of the original equation is nonzero, since $\tilde{q} > q$﻿ for everything except the last term.
- We now substitute in the results that we’ve found so far to simplify the equation
    
    - All of the dot products except the last one in the summation is gone.
    - The blue part of the summation (the numerator) is exactly the blue part of the picture above, so we can just substitute that in.
        
        $\langle r^q, s^{q-1}\rangle_A = \frac{r^q\cdot r^q}{-\alpha^{q-1}}$
        
    - Finally, we can substitute the denominator using what we found earlier
        
        $\langle s^{q-1}, s^{q-1}\rangle_A = \frac{r^{q-1} \cdot r^{q-1}}{\alpha^{q-1}}$
        
    
    ![Untitled 21 13.png](../../attachments/Untitled%2021%2013.png)
    
- The new search direction is just the new residual added with the dot product of the new residual divided by the dot product of the old residual, multiplied by the old search direction.

## Method

![Untitled 22 10.png](../../attachments/Untitled%2022%2010.png)

# Non-Symmetric and/or Indefinite

![Untitled 23 10.png](../../attachments/Untitled%2023%2010.png)

# Summary

- We’ve learned how to iteratively solve using Line Search.
- There are some issues with Line Search, such as repeating work due to repeating search directions.
- We picked how to search
    - We can line search using steepest descent.
    - We can also line search using conjugate gradients.
        - This just means make all of our search directions A-orthogonal for SPD matrices, which would help with repeating search directions.
        - We make search directions A-orthogonal using Gram Schmidt.