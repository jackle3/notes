---
Week: Week 2
---
# Rows vs. Columns

![[attachments/Untitled 61.png|Untitled 61.png]]

- You can have more rows (ie. more data) without changing how the problem works, since the columns are what define the number of variables.
- Once the columns become full rank, adding data no longer helps, because a unique solution already exists at that point.

# Eigensystems

![[attachments/Untitled 1 28.png|Untitled 1 28.png]]

- A diagonal matrix has the basis vectors as eigenvectors
    - Notice that the upper/lower triangular matrices do not have $\hat{e}_k$﻿ as eigenvectors

# Complex Numbers

![[attachments/Untitled 2 28.png|Untitled 2 28.png]]

# Vector Deformation

![[attachments/Untitled 3 28.png|Untitled 3 28.png]]

- The eigenvectors $v_1$﻿ and $v_2$﻿ (in red) form a basis for $\R^2$﻿. Therefore, any vector $c$﻿ can be written as a linear combination of those basis vectors.
- When we take the matrix $A$﻿ and multiply it by the vector $c$﻿, the $A$﻿ distributes inside the sum, getting us a linear combination of our basis eigenvectors with the eigenvalues.
    - In the final equation, the eigenvalues multiplies each of the coefficients.
- Multiplying by $A$﻿ deforms the vector $c$﻿ to be closer to eigenvectors with larger eigenvalues.

# Spatial Deformation

- $A$﻿ transforms all of $\R^2$﻿, stretching and rotating all the vectors in that vector space.
    - It squashes vectors in the direction of smaller eigenvalues, and stretches them in the direction of large eigenvalues.

![[attachments/Untitled 4 27.png|Untitled 4 27.png]]

# Perturbation

- Since the space is stretched, linear systems also correspondingly gets stretched.

![[attachments/Untitled 5 27.png|Untitled 5 27.png]]

- If the stretch eigenvalue is very large, the lines $Ac = b$﻿ and $A\hat{c} = \hat{b}$﻿ can become almost parallel, which leads to the same algorithmic issues as before.
- Small changes from $b$﻿ to $\hat{b}$﻿ (even if they are nearly parallel) would lead to big errors in $c$﻿ and $\hat{c}$﻿

## Conditioning

- A matrix is ill-conditioned if the ratio of the largest to the smallest eigenvalue is very big. It would be very bad to invert this matrix computationally since any impreciseness will lead to big errors
- For example, the Vandemonde matrix for the monomial basis has a very large ratio between eigenvalues, so it is poor conditioned

## Preconditioning

- Make intermediate matrices with better conditioning to make it easier to solve, because the ratio of the eigenvalues are better

![[attachments/Untitled 6 26.png|Untitled 6 26.png]]

# Rank of Rectangular Matrices

![[attachments/Untitled 7 26.png|Untitled 7 26.png]]

- A $1 \times 5$﻿ matrix has at most a rank of 1.
- In real world appliations, matrices tend to be very narrow. There is a lot of data so there is a lot of rows, and there is not that many unknowns in comparison.

# Singular Value Decomposition

![[attachments/Untitled 8 26.png|Untitled 8 26.png]]

- $U$﻿ is an orthonormal basis for $\R^m$﻿, and $V$﻿ is an orthonormal basis for $\R^n$﻿
- All the singular values $\sigma_k$﻿ is non-negative and in descending order

![[attachments/Untitled 9 25.png|Untitled 9 25.png]]

- Since $\Sigma^T \Sigma$﻿ is a diagonal matrix, the eigenvalues are on the diagonal and the eigenvectors are the standard basis vectors.
    - We found that $v = V\hat{e}_k =$﻿ the k-th column of $V$﻿. Therefore, the columns of $V$﻿ are the eigenvectors of $A^T A$﻿

![[attachments/Untitled 10 24.png|Untitled 10 24.png]]

- A similar result is true for that columns of $U$﻿

![[attachments/Untitled 11 24.png|Untitled 11 24.png]]

- Positive semi-definite means that their eigenvalues are positive of zero.

## Understanding Ac

- The $m \times n$﻿ matrix $A$﻿ maps from $\R^n$﻿ to $\R^m$﻿
- $Ac = U\Sigma V^T c$﻿ first projects vector $c \in R^n$﻿ onto the n basis vectors in $V$﻿.
- Then, the associated singular values (diagonally) scale the results
- Lastly, those scaled results are used as weights on the basis vectors in $U$﻿.

![[attachments/Untitled 12 24.png|Untitled 12 24.png]]

- V is being transposed because of how the equation is structured. Since $\Sigma$﻿ is left-multiplying $V^T$﻿, it’s going to be affecting the rows of $V^T$﻿. We’re focused on the columns of $V$﻿ and the columns of $U$﻿, so by transposing $V$﻿, we make it so that the $\Sigma$﻿ matrix affects the columns of $V$﻿ by left-multiply $V^T$﻿, and affecting the columns of $U$﻿ by right-multiply.

## Example (tall matrix)

![[attachments/Untitled 13 23.png|Untitled 13 23.png]]

![[attachments/Untitled 14 22.png|Untitled 14 22.png]]

- The eigenvalue $25.5$﻿ goes with the first column of $U$﻿ (eigenvector) when working with $AA^T$﻿
- The eigenvalue $25.5$﻿ goes with the first row of $V^T$﻿ (eigenvector) when working with $A^TA$﻿

![[attachments/Untitled 15 21.png|Untitled 15 21.png]]

$Ac = u_1\sigma_1v_1^Tc +u_2\sigma_2v_2^Tc+u_3\sigma_3v_3^Tc+u_40$

![[attachments/Untitled 16 21.png|Untitled 16 21.png]]

- This outer product term at the bottom progressively gives a higher rank and better approximation of $A$﻿ as you add more terms to it

## Truncating the SVD

- You can remove columns of $U$﻿ representing unreachable dimensions
    
    ![[attachments/Untitled 17 19.png|Untitled 17 19.png]]
    
- You can also remove columns corresponding with zero singular values.
    
    ![[attachments/Untitled 18 17.png|Untitled 18 17.png]]
    
- You can also optionally truncate singular values that are super small, giving you a good lower rank approximation. This is the idea behind PCA.
    
    ![[attachments/Untitled 19 15.png|Untitled 19 15.png]]
    

## Example (Wide Matrix)

![[attachments/Untitled 20 15.png|Untitled 20 15.png]]

![[attachments/Untitled 21 12.png|Untitled 21 12.png]]

## Summary

![[attachments/Untitled 22 9.png|Untitled 22 9.png]]

# Caveat with SVD

- It should always be true that $A v_k = \sigma_k u_k$﻿, where $v_k$﻿ is a column of $V$﻿
- In addition, the columns of $U$﻿ and $V$﻿ can be flipped in whichever direction, and it would be fine.
    - We can, if necessary, flip the columns by multiplying by $-1$﻿ to ensure that $A v_k = \sigma_k u_k$﻿ hold for all the columns

![[attachments/Untitled 23 9.png|Untitled 23 9.png]]

![[attachments/Untitled 24 9.png|Untitled 24 9.png]]

# Solving Linear Systems

- The SVD simply rotates your unknowns and right hand side to make your problem much easier to solve.

![[attachments/Untitled 25 7.png|Untitled 25 7.png]]

- Notice that $U^T b$﻿ dot products each row of $U^T$﻿ with $b$﻿. In other words, $\hat{b}$﻿ is the dot product of $b$﻿ with each column of $U$﻿, thereby projecting it into a new basis.

![[attachments/Untitled 26 6.png|Untitled 26 6.png]]

- In the first case of the square matrix, if $ $﻿ then there is no solution for $\hat{c}_2$﻿
    - This is because the equation is $\sigma_2 \hat{c}_2 = \hat{b}_2$﻿, and the left side is zero.
    - Note that $\hat{c}_1$﻿ is still well defined. As such, you can still get a good enough answer for your original coefficients, which were $c = V \hat{c} = \hat{c}_1 v_1 + \dots$﻿
        - You simply just throw away the other parts of the sum, keeping only what is well defined.
- In the second case, the final row gives the equation $0\hat{c}_1 + 0 \hat{c}_2 = \hat{b}_3$﻿
- In the last case, $\hat{c}_3$﻿ is a free variable, so there are infinite solutions for it.

## Understanding Variables

- This is a better way to think of solutions than the traditional no solution, infinitely many solutions, etc way of thinking

![[attachments/Untitled 27 5.png|Untitled 27 5.png]]

# Condition Number

![[attachments/Untitled 28 4.png|Untitled 28 4.png]]

# Norms

![[attachments/Untitled 29 4.png|Untitled 29 4.png]]

- In the example above, it might seem okay because the average norm is quite low. However, it’s cleary not okay because one person has a super high temperature.
    - In this case, it would be better to use the infinity norm (which uses the maximum of the data)

## Matrix Norms

![[attachments/Untitled 30 4.png|Untitled 30 4.png]]