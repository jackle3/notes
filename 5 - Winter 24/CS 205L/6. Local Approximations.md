---
Week: Week 4
---
# Sampling

![[attachments/Untitled 55.png|Untitled 55.png]]

- When we don’t have enough data, we can say that our approximation was **under-resolved** or **under-sampled**.
- Once we get enough data and we have a better approximation, we say it’s resolved better with more data.

# Taylor Expansion

- Takes a bunch of derivatives and pieces them together in order to approximate a function.

![[attachments/Untitled 1 22.png|Untitled 1 22.png]]

- If we were to truncate the Taylor series, we would have some error in the form of $O(h^{(\hat{p} + 1)})$﻿
    - **Key Detail**: when the derivatives of $f$﻿ are big, then $h$﻿ must be small in order for the error term $O(h^k)$﻿ to be safely dropped and ignored.
- This helps us find information about the function **around x.**

# Well-Resolved Functions

![[attachments/Untitled 2 22.png|Untitled 2 22.png]]

- Well-resolved means that we have enough sample points such that the distance between points makes the $h$﻿ small enough to make the Taylor expansion good for the derivatives near those points.

![[attachments/Untitled 3 22.png|Untitled 3 22.png]]

- **You need enough data to understand the function.**
    - In deep neural networks where functions are very non-linear, we need to think about chunks of the function locally.

# Piecewise Approximation

- We think of the data as disjoint subsets, fitting simpler models to each subset.
    - In the left example, we attempt to fit the whole data with one function, which is quite complex.
    - In the right example, we fit the subsets of the data with two piecewise functions, which is a lot simpler.

![[attachments/Untitled 4 21.png|Untitled 4 21.png]]

## Piecewise Constant Interpolation

![[attachments/Untitled 5 21.png|Untitled 5 21.png]]

- In this example, we basically just make a numberline, and go to the nearest availlable data point.
    - The rightmost picture shows the actual function and the constant interpolation, showing the errors.
- The function here is not continous, so no derivative exists at boundaries.

## Piecewise Linear Interpolation

- In this case, we approximate it with locally linear functions.

![[attachments/Untitled 6 20.png|Untitled 6 20.png]]

- The errors in this case are already super good → much less error than constant
- Since it’s locally linear functions, there are sharp turns in the derivatives
    - No continous derivative at each data point

## Higher Order Piecewise Interpolation

![[attachments/Untitled 7 20.png|Untitled 7 20.png]]

## Piecewise Cubic Interpolation (B-Splines)

- This is an improvement over linear because there are derivatives across data points (boundaries)

![[attachments/Untitled 8 20.png|Untitled 8 20.png]]

- We recursively use lower order polynomials and make higher order polynomials.

# Image Segmentation

![[attachments/Untitled 9 20.png|Untitled 9 20.png]]

- This is a good example for neural networks because humans can do it trivially → leverage that humans can do it well to train networks to do it.

## Bool Output Labels

![[attachments/Untitled 10 20.png|Untitled 10 20.png]]

## Integer Output Labels

![[attachments/Untitled 11 20.png|Untitled 11 20.png]]

## Real Number Output Labels

![[attachments/Untitled 12 20.png|Untitled 12 20.png]]

- For problems like this, it might be really hard to get training data since you have to manually label it.

# Segmenting Botanical Trees

- Image segmentation is not too difficult if the task is easy. However, segmenting trees is hard.

![[attachments/Untitled 13 19.png|Untitled 13 19.png]]

## Construction Training Data

- To make training data, you have to label it by hand and figure out the occlusions caused by branches.

![[attachments/Untitled 14 18.png|Untitled 14 18.png]]

- Once we have some data, we can artifically create more data to train with

![[attachments/Untitled 15 18.png|Untitled 15 18.png]]

- Break off the main image into smaller subsets of the image. This has a few advantages:
    - We can make many more training data by just taking windows of the original mask.
    - It helps with downsampling → networks have to work with low-res images, so taking subsets gives us low-res images without down-sampling the data.

## Training the Neural network

![[attachments/Untitled 16 18.png|Untitled 16 18.png]]

- Notice that the network outputs have some **regularization**, which also accounts for the occlusion since we know that our training data labels are somewhat wrong (we didn’t label every part of the tree)

## Network Inference

![[attachments/Untitled 17 16.png|Untitled 17 16.png]]

- Because of the regularization that we did, it can generalize a lot better and better figure out occlusions.

# Local Approximations

- Notice that our training data seems to fall into two different types.

![[attachments/Untitled 18 14.png|Untitled 18 14.png]]

- We can apply local approximation by training two different networks for each of the types.
    - We use k-means to figure out which training image belongs to each group/cluster

![[attachments/Untitled 19 13.png|Untitled 19 13.png]]

- We can then inference on both networks, and combine the outputs.
    - If the pixel is closer to the result of one network than the other, choose that one.
    - We essentially take the weighted average based on distance to the cluster center.

![[attachments/Untitled 20 13.png|Untitled 20 13.png]]

- Below is an example of this.
    - Network 2 does really well on the part of the image that has trees on grass
    - Network 1 does well on the branches occluding each other.
    - The middle is kind of even between the two.
- The interpolating can be seen in the combine image, where the color shows which network we take predictions from.

![[attachments/Untitled 21 10.png|Untitled 21 10.png]]