---
Week: Week 4
---
# Sampling

![Untitled 55.png](attachments/Untitled%2055.png)

- When we don’t have enough data, we can say that our approximation was **under-resolved** or **under-sampled**.
- Once we get enough data and we have a better approximation, we say it’s resolved better with more data.

# Taylor Expansion

- Takes a bunch of derivatives and pieces them together in order to approximate a function.

![Untitled 1 22.png](attachments/Untitled%201%2022.png)

- If we were to truncate the Taylor series, we would have some error in the form of $O(h^{(\hat{p} + 1)})$﻿
    - **Key Detail**: when the derivatives of $f$﻿ are big, then $h$﻿ must be small in order for the error term $O(h^k)$﻿ to be safely dropped and ignored.
- This helps us find information about the function **around x.**

# Well-Resolved Functions

![Untitled 2 22.png](attachments/Untitled%202%2022.png)

- Well-resolved means that we have enough sample points such that the distance between points makes the $h$﻿ small enough to make the Taylor expansion good for the derivatives near those points.

![Untitled 3 22.png](attachments/Untitled%203%2022.png)

- **You need enough data to understand the function.**
    - In deep neural networks where functions are very non-linear, we need to think about chunks of the function locally.

# Piecewise Approximation

- We think of the data as disjoint subsets, fitting simpler models to each subset.
    - In the left example, we attempt to fit the whole data with one function, which is quite complex.
    - In the right example, we fit the subsets of the data with two piecewise functions, which is a lot simpler.

![Untitled 4 21.png](attachments/Untitled%204%2021.png)

## Piecewise Constant Interpolation

![Untitled 5 21.png](attachments/Untitled%205%2021.png)

- In this example, we basically just make a numberline, and go to the nearest availlable data point.
    - The rightmost picture shows the actual function and the constant interpolation, showing the errors.
- The function here is not continous, so no derivative exists at boundaries.

## Piecewise Linear Interpolation

- In this case, we approximate it with locally linear functions.

![Untitled 6 20.png](attachments/Untitled%206%2020.png)

- The errors in this case are already super good → much less error than constant
- Since it’s locally linear functions, there are sharp turns in the derivatives
    - No continous derivative at each data point

## Higher Order Piecewise Interpolation

![Untitled 7 20.png](attachments/Untitled%207%2020.png)

## Piecewise Cubic Interpolation (B-Splines)

- This is an improvement over linear because there are derivatives across data points (boundaries)

![Untitled 8 20.png](attachments/Untitled%208%2020.png)

- We recursively use lower order polynomials and make higher order polynomials.

# Image Segmentation

![Untitled 9 20.png](attachments/Untitled%209%2020.png)

- This is a good example for neural networks because humans can do it trivially → leverage that humans can do it well to train networks to do it.

## Bool Output Labels

![Untitled 10 20.png](attachments/Untitled%2010%2020.png)

## Integer Output Labels

![Untitled 11 20.png](attachments/Untitled%2011%2020.png)

## Real Number Output Labels

![Untitled 12 20.png](attachments/Untitled%2012%2020.png)

- For problems like this, it might be really hard to get training data since you have to manually label it.

# Segmenting Botanical Trees

- Image segmentation is not too difficult if the task is easy. However, segmenting trees is hard.

![Untitled 13 19.png](attachments/Untitled%2013%2019.png)

## Construction Training Data

- To make training data, you have to label it by hand and figure out the occlusions caused by branches.

![Untitled 14 18.png](attachments/Untitled%2014%2018.png)

- Once we have some data, we can artifically create more data to train with

![Untitled 15 18.png](attachments/Untitled%2015%2018.png)

- Break off the main image into smaller subsets of the image. This has a few advantages:
    - We can make many more training data by just taking windows of the original mask.
    - It helps with downsampling → networks have to work with low-res images, so taking subsets gives us low-res images without down-sampling the data.

## Training the Neural network

![Untitled 16 18.png](attachments/Untitled%2016%2018.png)

- Notice that the network outputs have some **regularization**, which also accounts for the occlusion since we know that our training data labels are somewhat wrong (we didn’t label every part of the tree)

## Network Inference

![Untitled 17 16.png](attachments/Untitled%2017%2016.png)

- Because of the regularization that we did, it can generalize a lot better and better figure out occlusions.

# Local Approximations

- Notice that our training data seems to fall into two different types.

![Untitled 18 14.png](attachments/Untitled%2018%2014.png)

- We can apply local approximation by training two different networks for each of the types.
    - We use k-means to figure out which training image belongs to each group/cluster

![Untitled 19 13.png](attachments/Untitled%2019%2013.png)

- We can then inference on both networks, and combine the outputs.
    - If the pixel is closer to the result of one network than the other, choose that one.
    - We essentially take the weighted average based on distance to the cluster center.

![Untitled 20 13.png](attachments/Untitled%2020%2013.png)

- Below is an example of this.
    - Network 2 does really well on the part of the image that has trees on grass
    - Network 1 does well on the branches occluding each other.
    - The middle is kind of even between the two.
- The interpolating can be seen in the combine image, where the color shows which network we take predictions from.

![Untitled 21 10.png](attachments/Untitled%2021%2010.png)