---
Week: Week 5
---
# Jacobian

- $F(c)$﻿ is a vector-valued function. It takes in some $c$﻿ and outputs a vector.
- Each entry of the Jacobian is the partial of each function $F_i$﻿ with respect to the variable $c_k$﻿
    - Each row is for each function $F_i$﻿
    - Each column is for each component of the parameter $c_k$﻿
    - It’s the matrix of all derivatives.

![[attachments/Untitled 62.png|Untitled 62.png]]

- To build the Jacobian, basically just expand the original function to the right.

# Gradient

- Gradients are a subset of the Jacobian, for functions that are scalar valued (output is scalar)

![[attachments/Untitled 1 29.png|Untitled 1 29.png]]

- The gradient is the transpose of the Jacobian.

# Critical Points

- In order to do minimization, it must be a scalar-valued function.
    - This is why we minimize the norm of the residual in LS → the residual is a vector-valued function, while the norm is scalar-valued.

![[attachments/Untitled 2 29.png|Untitled 2 29.png]]

# Jacobian of the Gradient

- This is essentially the second partial derivative.
    - We take the gradient, and expand it to the right with all the partials.

![[attachments/Untitled 3 29.png|Untitled 3 29.png]]

- We need to work with the gradient because we can’t take the Jacobian of the Jacobian.
    - Jacobian is already expanded to the right. Can’t expand it further.
    - Gradient is Jacobian transposed, so it’s expanded vertically. We can expand this one.
- **The order of the partials matters.** The green is calculated first, then the blue is calculated.

# Hessian

- The Hessian is the transpose of the Jacobian of the Gradient

![[attachments/Untitled 4 28.png|Untitled 4 28.png]]

# Differential Forms

- The first equation is derived from:

$\frac{dF(c)}{dc} = J(F(c))$

- The second equation comes from the fact that gradient is Jacobian transposed.

$J(f(c))^T = \nabla f(c)$

![[attachments/Untitled 5 28.png|Untitled 5 28.png]]

# Classifying Critical Points

![[attachments/Untitled 6 27.png|Untitled 6 27.png]]

- Recall that to determine the definiteness, we will look at the eigenvalues of the Hessian.
    - If all eigenvalues are positive, its positive definite → local minimum
    - If all eigenvalues are negative, its negative definite → local maximum
    - If there is a mix of pos and neg, then it is indefinite → saddle point

## 1D Critical Points

- In one-dimension, the eigenvalue of the Hessian at $c^*$﻿ (and the only value in it) is the second derivative of the function at the critical point.

![[attachments/Untitled 7 27.png|Untitled 7 27.png]]

![[attachments/Untitled 8 27.png|Untitled 8 27.png]]

# Quadratic Form

- This is the standard quadratic equation in the form of a matrix.
    
    ![[attachments/Untitled 9 26.png|Untitled 9 26.png]]
    
    - $\tilde{c}$﻿ is a scalar. $\tilde{b}$﻿ is a vector. $\tilde{A}$﻿ is a matrix.
    - This means the quadratic form is a scalar-valued function.
- To minimize the quadratic form, you would use the gradient.
    
    ![[attachments/Untitled 10 25.png|Untitled 10 25.png]]
    
- The minimization becomes a lot easier when the matrix $\tilde{A}$﻿ is symmetric.
    
    - This is becauase $\frac{1}{2}\tilde{A} c + \frac{1}{2}\tilde{A}^T c = \tilde{A}c$﻿
    
    ![[attachments/Untitled 11 25.png|Untitled 11 25.png]]
    

## Connection to Hessian

![[attachments/Untitled 12 25.png|Untitled 12 25.png]]

# Solving Least Squares

- Recall the general solution to least squares.
    
    ![[attachments/Untitled 13 24.png|Untitled 13 24.png]]
    

## Normal Equations

- Notice that the **quadratic form** is equal to $f(c) = \frac{1}{2} c^T \tilde{A} c - \tilde{b}^T c + \tilde{c}$﻿
- Minimizing the qudratic form is the same as minimizing 2 times the quadratic form.
    
    $2f(c) = c^T \tilde{A} c - 2\tilde{b}^T c + \tilde{c}$
    
- Notice that the equation above is basically the same as the least squares minimization.
    
    $c^T \tilde{A} c = c^T( A^T D^2 A )c \quad \to\quad \tilde{A} = A^T D^2 A$
    
    $2\tilde{b}^T c = 2b^TD^2Ac \quad \to \quad \tilde{b}^T = b^TD^2A$
    
- Notice also that since $\tilde{A} = A^T D^2 A$﻿, it means that $\tilde{A}$﻿ is symmetric for least squares, making the solution even easier.

![[attachments/Untitled 14 23.png|Untitled 14 23.png]]

## Hessian for Least Squares

- Using the Hessian, we can show that the critical point that we get from the quadratic form (ie. the solution to least squares) is a minimum.

![[attachments/Untitled 15 22.png|Untitled 15 22.png]]

- For weighted least squares, we can essentially fold the $D$﻿ into $A$﻿ by multiplying it in.
    - First, we have to remember that if we row-scale $A$﻿ (e.g. multiply $DA$﻿), it does not change the (column) rank of the matrix $A$﻿.
- We can use the same proof as above. Instead of using the SVD of $A$﻿, we use the SVD of $DA$﻿.

![[attachments/Untitled 16 22.png|Untitled 16 22.png]]