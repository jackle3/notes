---
Week: Week 9
---
# Roadmap

- To avoid derivatives, we just hack it. We say that the Hessian is just the identity, and the Jacobian is mostly zero.

![Untitled 58.png](../../attachments/Untitled%2058.png)

- Note that this is how the Broyden’s methods begin. They start with setting $H \approx I$﻿, then do rank one updates to make that better.
    - The difference is that those methods improve the approximation for $H$﻿
    - This method does not improve $H$﻿
- This method also mostly ignores the Jacobian on the right hand side.
    - This is batch processing. They only use the Jacobian for a batch of the data, instead of the whole thing.

# Recall: Gradient

- The gradient is the transposed Jacobian.

![Untitled 1 25.png](../../attachments/Untitled%201%2025.png)

# Gradient/Steepest Descent

- Since we set the $H \approx I$﻿, this is how we use the negative gradient as the search direction.

![Untitled 2 25.png](../../attachments/Untitled%202%2025.png)

## Steepest Descent for Quadratic Forms

- The residual is the negative gradient of the quadratic form.

![Untitled 3 25.png](../../attachments/Untitled%203%2025.png)

- The update equation of steepest descent if $c^{q+1} = c^q + \alpha^q r^q$﻿
    - In English, this is next value = current value + step size * step direction
- Since the matrix is SPD, we can use conjugate gradients to converge in less steps.

![Untitled 4 24.png](../../attachments/Untitled%204%2024.png)

- With traditional steepest descent, you take the direction that is perpendicular to your current isocontour
    - You keep going in that direction until you are parallel to the next isocontour.
    - The minima in the next contour is the point where its tangent to the next isocontour.
    - Repeat.
- Conjugate gradients can do it in two steps in $\R^2$﻿
    - The first step is the same as steepest descent. It’s still the direction that is orthogonal to the current isocontour.
    - However, you keep going in that direction until you are A-orthogonal to the origin.
    - Once you’re there, you just take the second step and get straight to the origin.

# Recall: Nonlinear Least Squares

![Untitled 5 24.png](../../attachments/Untitled%205%2024.png)

![Untitled 6 23.png](../../attachments/Untitled%206%2023.png)

# Steepest Descent for Nonlinear Least Squares

![Untitled 7 23.png](../../attachments/Untitled%207%2023.png)

- The disadvantage of this is that if you have a lot of data, calculating each entry of the search direction can be very expensive.
- To fix this, we can ignore some of the data. This is where stochastic and mini-batch gradient descent comes in.
    
    - We consider less data at a time, approximating the gradient search direction.
    - This effectively means we zero out most of the terms that contribute to building the gradient and $J_{\hat{f}}$﻿, approximating it with less computational cost.
    
    ![Untitled 8 23.png](../../attachments/Untitled%208%2023.png)