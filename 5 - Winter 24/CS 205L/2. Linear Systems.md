---
Week: Week 2
---
# Motivation

- Matrices are bad, and vector spaces are good
    - Don’t think of matrices as a collection of numbers
    - Instead, think of the columns of the matrix as **vectors in a high dimensional space**

# Systems of Linear Equations

- We can represent linear systems as a matrix vector product

![Untitled 56.png](../../attachments/Untitled%2056.png)

- In this class, $x$﻿ is used for **data**, and $c$﻿ is used for unknowns (such as the unknown parameters of a neural network)

# Normalization

- There are major issues with the number zero on the computer due to precision.
    
    ![Untitled 1 23.png](../../attachments/Untitled%201%2023.png)
    
- By non-dimensionalize, we mean changing the units or changing the equation so that the units simply cancel out.

## Row/Column Scaling

![Untitled 2 23.png](../../attachments/Untitled%202%2023.png)

- The equations of the above are

$(3e6)c1 + (2e10)c2 = 5e10 \\$

- With row-scaling, we simply change the units of the first row, dividing both sides of the equation.
- After row-scaling, we can do column scaling by defining a new variable, kinda like factoring out like terms in both equations.

# Matrix Definitions

![Untitled 3 23.png](../../attachments/Untitled%203%2023.png)

- Symmetric matrices must be square.

## Solvability

![Untitled 4 22.png](../../attachments/Untitled%204%2022.png)

- The rank of a matrix tells you what dimension the matrix works in. Tells you the subset/subspace of the full vector space that the matrix spans.
    - A 3x3 matrix with rank 1 tells you all three columns lie in a line, a rank 2 tells you the columns lie in a plane, and rank 3 tells you the columns point in different directions

![Untitled 5 22.png](../../attachments/Untitled%205%2022.png)

## Linear Independence

![Untitled 6 21.png](../../attachments/Untitled%206%2021.png)

## Diagonal Matrix

![Untitled 7 21.png](../../attachments/Untitled%207%2021.png)

- In the example above, the first column tells you that there is a vector 5 units long in the x direction, and the second column is a vector 2 units long in the y direction.
    - These are the basis vectors, which are then multiplied with the coefficients to get the resulting vector.

## Upper Triangular Matrix

![Untitled 8 21.png](../../attachments/Untitled%208%2021.png)

## Lower Triangular Matrix

![Untitled 9 21.png](../../attachments/Untitled%209%2021.png)

# Gaussian Elimination

- This is a method to allow you to solve linear equations by elimination certain variables.

## Elimination Matrix

- First, pick any column $k$﻿ of the matrix and zero out everything below the diagonal by using the helper vector $m_{ik}$﻿ which zeros out the diagonal and everything above it.
- Then, take an outer product of $m_{ik} \hat{e_i}^T$﻿

![Untitled 10 21.png](../../attachments/Untitled%2010%2021.png)

- Below is an actual example of this

![Untitled 11 21.png](../../attachments/Untitled%2011%2021.png)

## Inverse of Elimination Matrix

- To get the inverse, simply change the minus to a plus

![Untitled 12 21.png](../../attachments/Untitled%2012%2021.png)

## Combining Elimination Matrices

![Untitled 13 20.png](../../attachments/Untitled%2013%2020.png)

## Example

![Untitled 14 19.png](../../attachments/Untitled%2014%2019.png)

- We deal with the first column first by creating the elimination matrix for it. This matrix is created by using $\frac{1}{2}[0, 4,-2]^T$﻿ and taking the outer product. The 2 comes from the fact that $a_{11} = 2$﻿, and the vector is what’s shown above.
- Then, we multiply $M_{11}$﻿ to both sides of the equation above.

![Untitled 15 19.png](../../attachments/Untitled%2015%2019.png)

- Now we deal with the second column, focusing on the second column of the resulting matrix $M_{11}A$﻿ from above. We focus on entry $(M_{11}A)_{22} = 1$﻿ to create $M_{22}$﻿. We also multiply it by the right hand side.

![Untitled 16 19.png](../../attachments/Untitled%2016%2019.png)

- At this point, we’ve created an upper triangular matrix, which we can solve easily.

![Untitled 17 17.png](../../attachments/Untitled%2017%2017.png)

# LU Factorization

![Untitled 18 15.png](../../attachments/Untitled%2018%2015.png)

- Gaussian Elimination gives us the upper triangular matrix $U$﻿
- We can use the inverses of each of the elmination matrices to cancel them all out and get us back to $A$﻿. Multiplying all these inverses together, we can get a lower triangular matrix $L$﻿.
- Not all matrices have an LU factorization, and we may need to permute rows/cols before finding one.

![Untitled 19 14.png](../../attachments/Untitled%2019%2014.png)

## Pivoting

![Untitled 20 14.png](../../attachments/Untitled%2020%2014.png)

- Changing the rows is kind of like changing the order of the equations you solve. It makes no difference.

## Permutation Matrix

![Untitled 21 11.png](../../attachments/Untitled%2021%2011.png)

- To swap rows, simplity left-multiply the permutation matrix and the right hand side
- To swap columns, right-multiply the matrix $A$﻿ and left-multiply the unknowns $c$﻿

## Full Pivoting

- You can do the pivoting first, then calculate the LU of the pivoted matrix.

![Untitled 22 8.png](../../attachments/Untitled%2022%208.png)

# Sparsity

![Untitled 23 8.png](../../attachments/Untitled%2023%208.png)

- A sparse matrix can have very few nonzero entries compared to its size.
- In the example above, for each grid point, the unknowns are just itself and its 6 immediate grid neighbors. Everything else is zero.
    - Even though there are $10^{12}$﻿ possible entries, only $7 \times 10^6$﻿ entries are nonzeros.
    - Despite that, the inverse is not necessarily space, and it can have all $10^{12}$﻿ nonzero entries, which is very inefficient for space.
        - As such, you typically never want to compute the inverse for a sparse matrix. Just use the existence of the inverse to do stuff.

# Computing Inverses

- You can still compute the inverse for small and dense matrices.

![Untitled 24 8.png](../../attachments/Untitled%2024%208.png)

- Since you know that $AA^{-1} = I$﻿, you can compute these inverses one column at a time.
    - Let $c_k$﻿ be some column of $A^{-1}$﻿. You then know that $Ac_k = e_k$﻿, where $e_k$﻿ is a column of $I$﻿.
- In the equations above, $c_k = A^{-1}e_k$﻿, which is why the solution is the k-th column of $A^{-1}$﻿
    - You can then solve for each column $c_k$﻿ to find the inverse.