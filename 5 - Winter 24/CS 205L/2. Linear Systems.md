---
Week: Week 2
---
# Motivation

- Matrices are bad, and vector spaces are good
    - Don’t think of matrices as a collection of numbers
    - Instead, think of the columns of the matrix as **vectors in a high dimensional space**

# Systems of Linear Equations

- We can represent linear systems as a matrix vector product

![[attachments/Untitled 56.png|Untitled 56.png]]

- In this class, $x$﻿ is used for **data**, and $c$﻿ is used for unknowns (such as the unknown parameters of a neural network)

# Normalization

- There are major issues with the number zero on the computer due to precision.
    
    ![[attachments/Untitled 1 23.png|Untitled 1 23.png]]
    
- By non-dimensionalize, we mean changing the units or changing the equation so that the units simply cancel out.

## Row/Column Scaling

![[attachments/Untitled 2 23.png|Untitled 2 23.png]]

- The equations of the above are

$(3e6)c1 + (2e10)c2 = 5e10 \\$

- With row-scaling, we simply change the units of the first row, dividing both sides of the equation.
- After row-scaling, we can do column scaling by defining a new variable, kinda like factoring out like terms in both equations.

# Matrix Definitions

![[attachments/Untitled 3 23.png|Untitled 3 23.png]]

- Symmetric matrices must be square.

## Solvability

![[attachments/Untitled 4 22.png|Untitled 4 22.png]]

- The rank of a matrix tells you what dimension the matrix works in. Tells you the subset/subspace of the full vector space that the matrix spans.
    - A 3x3 matrix with rank 1 tells you all three columns lie in a line, a rank 2 tells you the columns lie in a plane, and rank 3 tells you the columns point in different directions

![[attachments/Untitled 5 22.png|Untitled 5 22.png]]

## Linear Independence

![[attachments/Untitled 6 21.png|Untitled 6 21.png]]

## Diagonal Matrix

![[attachments/Untitled 7 21.png|Untitled 7 21.png]]

- In the example above, the first column tells you that there is a vector 5 units long in the x direction, and the second column is a vector 2 units long in the y direction.
    - These are the basis vectors, which are then multiplied with the coefficients to get the resulting vector.

## Upper Triangular Matrix

![[attachments/Untitled 8 21.png|Untitled 8 21.png]]

## Lower Triangular Matrix

![[attachments/Untitled 9 21.png|Untitled 9 21.png]]

# Gaussian Elimination

- This is a method to allow you to solve linear equations by elimination certain variables.

## Elimination Matrix

- First, pick any column $k$﻿ of the matrix and zero out everything below the diagonal by using the helper vector $m_{ik}$﻿ which zeros out the diagonal and everything above it.
- Then, take an outer product of $m_{ik} \hat{e_i}^T$﻿

![[attachments/Untitled 10 21.png|Untitled 10 21.png]]

- Below is an actual example of this

![[attachments/Untitled 11 21.png|Untitled 11 21.png]]

## Inverse of Elimination Matrix

- To get the inverse, simply change the minus to a plus

![[attachments/Untitled 12 21.png|Untitled 12 21.png]]

## Combining Elimination Matrices

![[attachments/Untitled 13 20.png|Untitled 13 20.png]]

## Example

![[attachments/Untitled 14 19.png|Untitled 14 19.png]]

- We deal with the first column first by creating the elimination matrix for it. This matrix is created by using $\frac{1}{2}[0, 4,-2]^T$﻿ and taking the outer product. The 2 comes from the fact that $a_{11} = 2$﻿, and the vector is what’s shown above.
- Then, we multiply $M_{11}$﻿ to both sides of the equation above.

![[attachments/Untitled 15 19.png|Untitled 15 19.png]]

- Now we deal with the second column, focusing on the second column of the resulting matrix $M_{11}A$﻿ from above. We focus on entry $(M_{11}A)_{22} = 1$﻿ to create $M_{22}$﻿. We also multiply it by the right hand side.

![[attachments/Untitled 16 19.png|Untitled 16 19.png]]

- At this point, we’ve created an upper triangular matrix, which we can solve easily.

![[attachments/Untitled 17 17.png|Untitled 17 17.png]]

# LU Factorization

![[attachments/Untitled 18 15.png|Untitled 18 15.png]]

- Gaussian Elimination gives us the upper triangular matrix $U$﻿
- We can use the inverses of each of the elmination matrices to cancel them all out and get us back to $A$﻿. Multiplying all these inverses together, we can get a lower triangular matrix $L$﻿.
- Not all matrices have an LU factorization, and we may need to permute rows/cols before finding one.

![[attachments/Untitled 19 14.png|Untitled 19 14.png]]

## Pivoting

![[attachments/Untitled 20 14.png|Untitled 20 14.png]]

- Changing the rows is kind of like changing the order of the equations you solve. It makes no difference.

## Permutation Matrix

![[attachments/Untitled 21 11.png|Untitled 21 11.png]]

- To swap rows, simplity left-multiply the permutation matrix and the right hand side
- To swap columns, right-multiply the matrix $A$﻿ and left-multiply the unknowns $c$﻿

## Full Pivoting

- You can do the pivoting first, then calculate the LU of the pivoted matrix.

![[attachments/Untitled 22 8.png|Untitled 22 8.png]]

# Sparsity

![[attachments/Untitled 23 8.png|Untitled 23 8.png]]

- A sparse matrix can have very few nonzero entries compared to its size.
- In the example above, for each grid point, the unknowns are just itself and its 6 immediate grid neighbors. Everything else is zero.
    - Even though there are $10^{12}$﻿ possible entries, only $7 \times 10^6$﻿ entries are nonzeros.
    - Despite that, the inverse is not necessarily space, and it can have all $10^{12}$﻿ nonzero entries, which is very inefficient for space.
        - As such, you typically never want to compute the inverse for a sparse matrix. Just use the existence of the inverse to do stuff.

# Computing Inverses

- You can still compute the inverse for small and dense matrices.

![[attachments/Untitled 24 8.png|Untitled 24 8.png]]

- Since you know that $AA^{-1} = I$﻿, you can compute these inverses one column at a time.
    - Let $c_k$﻿ be some column of $A^{-1}$﻿. You then know that $Ac_k = e_k$﻿, where $e_k$﻿ is a column of $I$﻿.
- In the equations above, $c_k = A^{-1}e_k$﻿, which is why the solution is the k-th column of $A^{-1}$﻿
    - You can then solve for each column $c_k$﻿ to find the inverse.