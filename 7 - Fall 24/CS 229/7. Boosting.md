
# Weak Classifiers
* These are good! They are usually very fast to learn.
	* Low variance (not much overfitting) but high bias (underfitting).
![Pasted image 20241028231024](../../attachments/Pasted%20image%2020241028231024.png)
![Pasted image 20241028231053](../../attachments/Pasted%20image%2020241028231053.png)
# Ensemble Classifier
* **Definition**: Ensemble learning in machine learning combines predictions from multiple models to improve stability and generalization.
* **Model Output**: For an ensemble of models $F_1, F_2, \dots, F_m$, the combined prediction is represented as $f(x) = \sum_{i=1}^{m} \beta_i F_i(x)$, where $\beta_i$ is a weight for each model's prediction.
* **Equal Weighting**: Typically, all weights $\beta_i = \frac{1}{m}$ to avoid favoring any individual model, thus balancing their influence.
* **Benefits**: Ensemble learning decreases variance, helping models generalize better across new data.
* **Drawback**: The computational cost increases as more models are trained and utilized.
* **Use in Regression and Classification**: In regression, ensemble learning averages predictions, while in classification, it aggregates predictions through a plurality vote among the models.

* Each weak classifier votes on the preidction.
	* Each new weak classifier should reduce the number of mistakes.
	* The weight $w_i$ should be higher if $f_i$ fills in old mistakes.
![600](../../attachments/Pasted%20image%2020241028231227.png)
![600](../../attachments/Pasted%20image%2020241028231347.png)

# Bagging
* Bootstraping is a technique where we simulate drawing samples from the true underlying distribution from which our training set is generated.
	* E.g. we want to find mean of unknown true distribution. draw a bunch of samples, calculate mean of sample, repeat. you now have a distribution of the mean.

* **Definition**: Bagging, or “bootstrap aggregating,” reduces model variance by creating multiple datasets (using bootstrapping) and training a separate model on each.
* **Bootstrapping**: To create diverse datasets, we repeatedly sample with replacement from the training data. Each "bootstrapped" dataset approximates the original but includes a random selection of points.
* **Training Process**: Each bootstrapped dataset is used to train a separate model, resulting in an ensemble of diverse models that capture various patterns from the original data.
* **Variance Reduction**: High-variance models, like decision trees, tend to overfit the data they’re trained on, capturing noise rather than general patterns
	* Bagging helps by introducing variations in the training data, encouraging each model to learn slightly different aspects, reducing the chance of overfitting to specific data points ⟶ reduces ensemble variance.
	* **Diversity Benefit**: Each model only sees a portion (around 63.2%) of the data on average, which introduces unique biases in each model, lowering the ensemble’s overall variance.
	* **Parallel Training**: Since each model is trained independently, bagging allows parallelization, making it computationally efficient.

## Random Forests
* **Extension of Bagging**: Random forests add an extra layer of randomness specifically for decision trees by selecting a random fixed size subset ($k < d$) of features at each node-splitting stage.
* **Feature Subsampling**: Instead of using all features, each split in the tree considers only a subset, chosen randomly at each node. This randomness increases variation between trees in the forest.
* **Effect**: By limiting the features considered at each node, random forests prevent individual trees from relying too heavily on any single feature, which decreases correlation among trees.
* **Outcome**: This method generally enhances performance, as it further reduces the ensemble’s variance without significantly increasing bias, making random forests effective for a range of tasks, from classification to regression.

# Boosting
* **Core Idea**: Boosting aims to create a stronger ensemble by sequentially training models, where each model tries to correct the errors of its predecessor.
	* **Greedy learning ensembles from data.**
* **Training Process**: Unlike bagging, which trains all models in parallel, boosting trains models one after another. Each new model $F_{i+1}$ is added to the ensemble based on the residual errors (areas where the ensemble is underperforming) of the ensemble model $f_i$.
$$
f_{i}(x) = \sum_{j=1}^i \beta_{j} F_{j}(x)
$$
* **Weak Learners to Strong Learner**: Boosting combines "weak learners"—models that perform only slightly better than random guessing—into a "strong learner" that can make highly accurate predictions.
* **Bias Reduction**: By focusing on correcting errors in previous models, boosting primarily reduces bias in the model (as opposed to variance reduction in bagging).

* **Key Idea:** combine weak learners to create strong learner. Each new learner focuses on hard points.
![Pasted image 20241028231451](../../attachments/Pasted%20image%2020241028231451.png)

## Number of Stumps
* If $x$ has length $|V|$, boosting can lead to a collection of $2|V|$ decision stumps.
	* Occurs when each feature in $x$ is used to create two distinct stumps: one for each possible threshold direction (e.g., $x_i > \theta$ and $x_i < \theta$ for a given feature $x_i$ and threshold $\theta$).
* Typically, for each iteration of boosting (e.g., AdaBoost), a weak learner (like a decision stump) is added based on the feature that best classifies the data under the current distribution.
	* Over multiple rounds, boosting may select multiple stumps across features, potentially leading to a total of $2|V|$ stumps if each feature's possible threshold direction is used in the process.

## Weighted Data
* After learning a model $f_i(x)$, we weight our dataset for the next model $f_{i+1}(x)$.
* Each datapoint $(x_i, y_i)$ is weighted by $\alpha_i$ ⟶ more important points have higher weight.
* When learning:
	* Data point $i$ counts as $\alpha_i$ copies of that datapoint (e.g. $\alpha_i = 2$ means two copies of $x_i$)
	$$
\text{Loss} = \sum_{i=1}^N \alpha_{i} \cdot \ell(x_{i}, y_{i}; \theta)
$$
	* E.g. for decision trees, a mistake on data point $i$ counts as $\alpha_i$ mistakes.
	* E.g. for gradient descent, weight the contribution of point $i$ to the gradient by $\alpha_i$.
* **Decision stump**
![Pasted image 20241028231824](../../attachments/Pasted%20image%2020241028231824.png)
![Pasted image 20241028231837](../../attachments/Pasted%20image%2020241028231837.png)

# AdaBoost
* This algorithm is greedy. Once you learn $w_t$ and $f_t$, you put it into your prediction $\hat{y}$ and never change it again. Each new $f_{t+1}$ fixes mistakes of prev model.
![Pasted image 20241028231855](../../attachments/Pasted%20image%2020241028231855.png)

## Computing the Coefficient
![Pasted image 20241028231914](../../attachments/Pasted%20image%2020241028231914.png)
![Pasted image 20241028232006](../../attachments/Pasted%20image%2020241028232006.png)

## Computing the Weights
![Pasted image 20241028232107](../../attachments/Pasted%20image%2020241028232107.png)
![Pasted image 20241028232125](../../attachments/Pasted%20image%2020241028232125.png)

## Normalizing Weights
![Pasted image 20241028232155](../../attachments/Pasted%20image%2020241028232155.png)

## Learning Summary
![Pasted image 20241028232209](../../attachments/Pasted%20image%2020241028232209.png)
* Increase $\alpha_i$ on datapoints where you've made mistakes so the next classifier can do better
* If weighted training error:
	* is small (less than 0.5), $w_t$ is positive. 
	* is random (equal to 0.5), $w_t$ is zero.
	* is big (greater than 0.5), $w_t$ is negative.

# Boosting Convergence
* In order to converge to zero error, we need:
	* At every $t$, we can find a weak learner $\text{weighted\_error}(f_{t}) < 0.5 - \delta$.
	* This is not always possible (e.g. when there are two classes on top of each other)
	* Nonetheless, boosting yields great training error.
![Pasted image 20241028232251](../../attachments/Pasted%20image%2020241028232251.png)
## Upper Bound
![Pasted image 20241028232501](../../attachments/Pasted%20image%2020241028232501.png)
![Pasted image 20241028232522](../../attachments/Pasted%20image%2020241028232522.png)
![Pasted image 20241028232532](../../attachments/Pasted%20image%2020241028232532.png)
![Pasted image 20241028232548](../../attachments/Pasted%20image%2020241028232548.png)

# Boosting Overfitting
* Boosting tends to be robust to overfitting.
![Pasted image 20241028232614](../../attachments/Pasted%20image%2020241028232614.png)
![Pasted image 20241028232640](../../attachments/Pasted%20image%2020241028232640.png)

## Stopping Boosting
![Pasted image 20241028232651](../../attachments/Pasted%20image%2020241028232651.png)

# Gradient Boosting
- **Mechanism**: Gradient boosting refines the boosting concept by using functional gradient descent. 
	- Each new model in the ensemble corrects the residual error of the prior model by following the gradient of the loss function.
	- Instead of learning parameters $\beta_{i}$ and $\theta_i$ to develop $F_{i}(\cdot;\theta_{i})$, we learn the entire function $F_i$.
$$
F_{i} = \arg\min_{F} \mathcal{L}(f_{i-1} + F) = \arg\min_{F} \sum_{j=1}^n \ell\left(y^{(j)}, \left(f_{i-1}(x^{(j)}) + F(x^{(j)})\right)\right)
$$
- **Optimization Process**:
	- At each step $i$, the model $F_i$ is chosen to minimize the residual loss, effectively reducing the ensemble’s error by fitting a new model to the negative gradient of the loss with respect to the current predictions.
	- This method allows each successive model to target the “weak points” in the ensemble’s predictions, iteratively improving accuracy.
- **Result**: Gradient boosting iteratively adds models that address the ensemble’s remaining errors, gradually increasing accuracy and creating a powerful predictive model.

## XGBoost
- **Enhancements Over Standard Gradient Boosting**:
	- **Loss Regularization**: Adds regularization terms to the loss function to prevent overfitting and control the complexity of each tree in the ensemble. The loss function becomes:
$$
L(f) = \sum_{i=1}^{n} \ell(y^{(i)}, f(x^{(i)})) + \gamma J + \frac{1}{2} \lambda \sum_{j=1}^{J} w_j^2
$$
    where $J$ is the number of leaves, $w_j$ the prediction at each leaf, and $\gamma$, $\lambda$ are hyperparameters.
	* **Feature Subsampling**: Similar to random forests, XGBoost uses feature subsampling to reduce correlation between trees.
	* **Second-Order Approximation**: Utilizes the Hessian matrix (second derivatives) of the loss function for a more refined optimization step, allowing faster convergence and better accuracy.
- **Real-World Applications**: Known for its speed and performance, XGBoost is widely used in industry settings and competition environments due to its robust handling of complex datasets.

# Summary
![Pasted image 20241028232724](../../attachments/Pasted%20image%2020241028232724.png)
