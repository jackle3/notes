# Definition
* A decision tree can be through of as a mapping from some $k$ regions of the input domain $\{R_{1}, R_{2}, \dots, R_{k}\}$ to some $K$ corresponding predictions $\{w_{1}, w_{2}, \dots, w_{k}\}$.
	* These regions partition the input domain, meaning there is no intersection between regions.
	* The union of all the regions recovers the entire input domain.
	* The prediction for any point in a certain region $R_j$ is $w_j$.
![Pasted image 20241028223135](attachments/Pasted%20image%2020241028223135.png)
* For classification, $w_j$ is typically whichever class is the most common in the training data in $R_j$.
* For regression, $w_j$ is commonly the average value of the labels of the training data in $R_j$.

* For example, suppose we split $\mathbb{R}^2$ into four quadrants:
		![Pasted image 20241028223444](attachments/Pasted%20image%2020241028223444.png)
	* The decision tree is as follows:![Pasted image 20241028223455](attachments/Pasted%20image%2020241028223455.png)

* A real world example might be taking a loan application and outputting whether its safe or risky.
		![500](attachments/Pasted%20image%2020241028223325.png)
		![500](attachments/Pasted%20image%2020241028223333.png)

# Finding the Best Tree
* Given training data $(X, y)$, we optimize a **quality metric** on the training data to create a tree $T(X)$.
* A good quality metric is the **classification error** ⟶ measures the fraction of mistakes:
$$
\text{Error rate} = \frac{\text{\# incorrect predictions}}{\text{\# examples}}
$$
* In general, we figure out splits via a gain function ⟶ what is the "gain" in our cost after the split?
![Pasted image 20241028230408](attachments/Pasted%20image%2020241028230408.png)
* We can't brute force learning ⟶ there is an **exponentially large number of trees**

## Greedy Decision Tree Learning
* At every step, take the decision that **minimizes the number of errors** after taking that step.

* At the start, we would've predicted based on majority ⟶ in this case, we have 18 mistakes.
	* This is the **root node**.
![500](attachments/Pasted%20image%2020241028224056.png)

* Decision stump: single level tree after **splitting on credit** ⟶ now we have 8 mistakes.
	* These blue nodes are the **intermediate nodes** ⟶ contains subsets of data with each label.
![500](attachments/Pasted%20image%2020241028224118.png)

* In order to learn a decision stump, we need to **find the best feature to split on**.

### Selecting Best Feature
* As before, choose the feature that minimizes the number of mistakes.
![500](attachments/Pasted%20image%2020241028224317.png)

### Classification Error
* An alternate way to see this is to calculate the classification error.
![500](attachments/Pasted%20image%2020241028224432.png)
### Stopping Condition
* We stop once **all points are in the majority class** (agree on prediction). Otherwise we recurse.
![600](attachments/Pasted%20image%2020241028224616.png)
* We also stop once we've **already split on all features** ⟶ in this case we have 3 errors.
![600](attachments/Pasted%20image%2020241028224651.png)
### Algorithm Summary
![500](attachments/Pasted%20image%2020241028224458.png)
![500](attachments/Pasted%20image%2020241028224743.png)
![Pasted image 20241028224831](attachments/Pasted%20image%2020241028224831.png)

### Runtime
* Notice that you can't have more leaves than you have data points.
* Further, every step is linear in the number of features (we pick a feature and move on).

## Loss Function
![Pasted image 20241029220924](attachments/Pasted%20image%2020241029220924.png)

# Real-Valued Decision Trees
* When we do not have discrete classes, how should we split it?

## Threshold Split
* We create a threshold on a feature and split based on that.
![500](attachments/Pasted%20image%2020241028225003.png)

* To find the best threshold, choose $t^*$ that minimizes the number of mistakes.
![600](attachments/Pasted%20image%2020241028225059.png)

* In terms of what threshold to consider, consider the midpoints between data points.
![600](attachments/Pasted%20image%2020241028225133.png)

* Notice that you can split on one feature multiple times using different thresholds.
![400](attachments/Pasted%20image%2020241028225210.png)

### Algorithm Summary
![600](attachments/Pasted%20image%2020241028225151.png)

# Decision Tree vs. Logistic Regression
* Decision trees can make complex decision boundaries via many splits.
* Logistic regression makes linear decision boundaries.
	* Linear relationship between input and output.
	* Note that you can make nonlinear boundaries by transforming input $x$
![Pasted image 20241028225341](attachments/Pasted%20image%2020241028225341.png)


# Overfitting
* As the depth of the decision tree increases, we are likely to also overfit.
	* To determine optimal depth, use cross validation.
* Alternatively, we can also **pick simpler trees**. There are two techniques:
	* Early stopping ⟶ stop learning before tree becomes too complex
		* This is not good: some branches might need to go deeper than others, hard to know when exactly to stop, and we may miss out on good splits.
	* Pruning ⟶ simplify tree after learning terminates
		* This is generally better.
## Pruning
* This allows trees with varying maximum depths, depending on what is necessary.
* In order to prune, go up the tree from the bottom up and consider which split is worth it given the validation set.
![600](attachments/Pasted%20image%2020241028225638.png)

* We want to balance (1) how well tree fits data and (2) complexity of tree.
	* Let $L(T)$ be the number of leaf nodes.
	* Define the total cost as $C(T) = \text{Error}(T) = \lambda L(T)$
		* If $\lambda = 0$ ⟶ maximal tree, no pruning
		* If $\lambda = \infty$ ⟶ root node, no splits (bc we want no leaf nodes)

## Pruning Algorithm
1. Consider a split.
2. Compute total cost of split.
	![500](attachments/Pasted%20image%2020241028225926.png)
	* Undo the split to make $T_{\text{smaller}}$. If it gives smaller cost, prune the split.
		![500](attachments/Pasted%20image%2020241028230032.png)
3. Repeat for every split (from the bottom up).

# Summary
**Positives**: Advantages of using trees include but are not limited to:
* Highly interpretable
* Robust to outliers
* Robust to mix of continuous and discrete features
* Robust to monotone transformations of input (transformations that don’t alter the output of our sorting strategy from above)
* Can usually achieve a “decent” fit relatively quickly, even on large datasets
**Negatives**: Disadvantages of using trees include but are not limited to:
* Tend to generalize poorly, even when regularized
* Highly unstable around boundaries
