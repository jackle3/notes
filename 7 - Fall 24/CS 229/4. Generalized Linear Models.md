
* Linear regression and logistic regression are a subset of the family of GLMs.
* Linear regression was:$$
y|x;\theta \sim \mathcal{N}(\mu, \sigma^2)
$$
* Logistic regression was:
$$
y|x;\theta \sim \text{Bernoulli}(\phi)
$$
# Exponential Family
* A distribution is in the exponential family if it can be written as:
$$
p(y;\eta) = b(y)\exp(\eta^T T(y) - a(\eta))
$$
* $\eta$ is the **natural parameter** (aka the canonical parameter).
* $T(y)$ is the **sufficient statistic** (often $T(y) = y)$
* $a(\eta)$ is the **log partition function**
	* This quantity makes it so that the distribution sums/integrates over $y$ to 1.

* A fixed choice of $b$, $T$, and $a$ defines a different distribution in the family.
![400](../../attachments/Pasted%20image%2020241011171654.png)


## Bernoulli
![400](../../attachments/Pasted%20image%2020241011171452.png)
The probability mass function for a Bernoulli random variable $Y$ with parameter $p$ is:
$$
p(y; \phi) = \phi^y (1 - \phi)^{1 - y}, \quad y \in \{0, 1\}
$$
Reparameterizing with $\eta = \log\left(\frac{\phi}{1 - \phi}\right)$, we get:
$$
p(y; \eta) = \exp\left(y \eta - \log(1 + e^{\eta})\right)
$$
Exponential Family Form
- **Natural parameter**: $\eta = \log\left(\frac{\phi}{1 - \phi}\right)$
- **Sufficient statistic**: $T(y) = y$
- **Log-partition function**: $a(\eta) = -\log(1 - \phi) = \log(1 + e^{\eta})$
- **Base measure**: $b(y) = 1$

Parameter Relationship
- $\phi = \frac{1}{1 + e^{-\eta}}$ (the sigmoid function)

![400](../../attachments/Pasted%20image%2020241011171912.png)

## Gaussian
* To simply the derivation, suppose $\sigma^2 = 1$
![Pasted image 20241011171458](../../attachments/Pasted%20image%2020241011171458.png)
The probability density function for a Gaussian random variable $Y$ with mean $\mu$ and variance $\sigma^2$ is:
$$
p(y; \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(y - \mu)^2}{2 \sigma^2}\right)
$$
To express this in exponential family form, we expand the exponent $-\frac{(y - \mu)^2}{2 \sigma^2}$:
$$
(y - \mu)^2 = y^2 - 2y\mu + \mu^2
$$
Substitute into the exponent:
$$
-\frac{(y - \mu)^2}{2 \sigma^2} = -\frac{y^2}{2 \sigma^2} + \frac{y \mu}{\sigma^2} - \frac{\mu^2}{2 \sigma^2}
$$
Now the density function becomes:
$$
p(y; \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{y^2}{2 \sigma^2} + \frac{y \mu}{\sigma^2} - \frac{\mu^2}{2 \sigma^2}\right)
$$
Set the natural parameter as:
$$
\eta_1 = \frac{\mu}{\sigma^2} \quad \text{and} \quad \eta_2 = -\frac{1}{2\sigma^2}
$$
This gives:
$$
\frac{y \mu}{\sigma^2} = \eta_1 y \quad \text{and} \quad -\frac{y^2}{2 \sigma^2} = \eta_2 y^2
$$
The density function now becomes:
$$
p(y; \eta) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(\eta_1 y + \eta_2 y^2 - \frac{\eta_1^2}{4 \eta_2}\right)
$$
Rewrite the density in the form:
$$
p(y; \eta) = b(y) \exp\left(T(y)^T \eta - a(\eta)\right)
$$
**Exponential Family Form**
* **Natural parameters**: $\eta = \begin{bmatrix} \eta_1 \\ \eta_2 \end{bmatrix} = \begin{bmatrix} \frac{\mu}{\sigma^2} \\ -\frac{1}{2\sigma^2} \end{bmatrix}$
* **Sufficient statistics**: $T(y) = \begin{bmatrix} y \\ y^2 \end{bmatrix}$
* **Log-partition function**:
$$
a(\eta) = \frac{\mu^2}{2 \sigma^2} + \log \sigma = -\frac{\eta_1^2}{4 \eta_2} - \frac{1}{2} \log(-2 \eta_2)
$$
* **Base measure**: $b(y) = \frac{1}{\sqrt{2 \pi}}$

**Parameter Relationships**
Using $\eta_1$ and $\eta_2$, we can express $\mu$ and $\sigma^2$ in terms of $\eta$:
* $\mu = -\frac{\eta_1}{2 \eta_2}$
* $\sigma^2 = -\frac{1}{2 \eta_2}$

## Binomial
The probability mass function for a Binomial random variable $Y$ with parameters $n$ and $p$ is:
$$
p(y; n, p) = \binom{n}{y} p^y (1 - p)^{n - y}
$$
With $\eta = \log\left(\frac{p}{1 - p}\right)$:
$$
p(y; \eta) = \binom{n}{y} \exp\left(y \eta - n \log(1 + e^{\eta})\right)
$$
Exponential Family Form
* **Natural parameter**: $\eta = \log\left(\frac{p}{1 - p}\right)$
* **Sufficient statistic**: $T(y) = y$
* **Log-partition function**: $a(\eta) = n \log(1 + e^{\eta})$
* **Base measure**: $b(y) = \binom{n}{y}$

Parameter Relationship
* $p = \frac{1}{1 + e^{-\eta}}$ (also the sigmoid function)

## Negative Binomial
The probability mass function for a Negative Binomial random variable $Y$, representing the number of trials to get $k$ successes, is:
$$
p(y; k, p) = \binom{y - 1}{k - 1} (1 - p)^{y - k} p^k, \quad y \in \{k, k+1, k+2, \dots\}
$$
This can be expressed in exponential family form by reparameterizing with the natural parameter $\eta = \log(1 - p)$ so that $p = 1 - e^{\eta}$. We then have:
$$
p(y; \eta) = \binom{y - 1}{k - 1} \exp\left((y - k) \eta + k \log(1 - e^{\eta})\right)
$$
Exponential Family Form
* **Natural parameter**: $\eta = \log(1 - p)$
* **Sufficient statistic**: $T(y) = y - k$
* **Log-partition function**: $a(\eta) = -k \log(1 - e^{\eta})$
* **Base measure**: $b(y) = \binom{y - 1}{k - 1}$

Parameter Relationship
* $p = 1 - e^{\eta}$

## Poisson
The probability mass function for a Poisson random variable $Y$ with mean $\lambda$ is:
$$
p(y; \lambda) = \frac{\lambda^y e^{-\lambda}}{y!}, \quad y \in \{0, 1, 2, \dots\}
$$
Using $\eta = \log(\lambda)$:
$$
p(y; \eta) = \frac{1}{y!} \exp\left(y \eta - e^{\eta}\right)
$$
Exponential Family Form
* **Natural parameter**: $\eta = \log(\lambda)$
* **Sufficient statistic**: $T(y) = y$
* **Log-partition function**: $a(\eta) = e^{\eta}$
* **Base measure**: $b(y) = \frac{1}{y!}$

Parameter Relationship
* $\lambda = e^{\eta}$

## Geometric
The probability mass function for a Geometric random variable $Y$, counting trials until the first success, with success probability $p$ is:
$$
p(y; p) = (1 - p)^{y - 1} p, \quad y \in \{1, 2, 3, \dots\}
$$
With $\eta = \log(1 - p)$:
$$
p(y; \eta) = \exp\left((y - 1) \eta + \log(1 - e^{\eta})\right)
$$
Exponential Family Form
* **Natural parameter**: $\eta = \log(1 - p)$
* **Sufficient statistic**: $T(y) = y - 1$
* **Log-partition function**: $a(\eta) = -\log(1 - e^{\eta})$
* **Base measure**: $b(y) = 1$

Parameter Relationship
* $p = 1 - e^{\eta}$

## Multinomial
The probability mass function for a Multinomial random variable $Y = (Y_1, \dots, Y_k)$ with $n$ trials and category probabilities $\mathbf{p} = (p_1, \dots, p_k)$ is:
$$
p(y; \mathbf{p}) = \frac{n!}{y_1! y_2! \cdots y_k!} p_1^{y_1} p_2^{y_2} \cdots p_k^{y_k}
$$
Setting $\eta_i = \log(p_i)$, we have:
$$
p(y; \mathbf{\eta}) = \frac{n!}{y_1! y_2! \cdots y_k!} \exp\left( \sum_{i=1}^k y_i \eta_i - n \log\left(\sum_{i=1}^k e^{\eta_i}\right) \right)
$$
Exponential Family Form
* **Natural parameters**: $\eta_i = \log(p_i)$
* **Sufficient statistic**: $T(y) = (y_1, \dots, y_k)$
* **Log-partition function**: $a(\eta) = n \log\left(\sum_{i=1}^k e^{\eta_i}\right)$
* **Base measure**: $b(y) = \frac{n!}{y_1! y_2! \cdots y_k!}$

Parameter Relationship
* $p_i = \frac{e^{\eta_i}}{\sum_{j=1}^k e^{\eta_j}}$ (Softmax function)

## Multivariate Gaussian
For a Multivariate Gaussian random variable $\mathbf{Y}$ with mean vector $\mathbf{\mu}$ and covariance matrix $\Sigma$:
$$
p(\mathbf{y}; \mathbf{\mu}, \Sigma) = \frac{1}{(2 \pi)^{k/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (\mathbf{y} - \mathbf{\mu})^T \Sigma^{-1} (\mathbf{y} - \mathbf{\mu})\right)
$$
With natural parameters $\eta = \Sigma^{-1} \mu$ and $\Lambda = \Sigma^{-1}$, we rewrite this as:
$$
p(\mathbf{y}; \eta, \Lambda) = \exp\left(\eta^T \mathbf{y} - \frac{1}{2} \mathbf{y}^T \Lambda \mathbf{y} - a(\eta, \Lambda)\right)
$$
Exponential Family Form
* **Natural parameters**: $\eta = \Sigma^{-1} \mu$, $\Lambda = \Sigma^{-1}$
* **Sufficient statistic**: $T(\mathbf{y}) = (\mathbf{y}, \mathbf{y} \mathbf{y}^T)$
* **Log-partition function**: $a(\eta, \Lambda) = \frac{1}{2} \eta^T \Lambda^{-1} \eta + \frac{1}{2} \log|\Lambda|$
* **Base measure**: $b(\mathbf{y}) = \frac{1}{(2 \pi)^{k/2}}$

Parameter Relationships
* Mean: $\mathbf{\mu} = \Lambda^{-1} \eta$
* Covariance: $\Sigma = \Lambda^{-1}$

## Other Families
![Pasted image 20241011172017](../../attachments/Pasted%20image%2020241011172017.png)

# Properties of GLMs
* GLMs always give us a convex optimization problem. This means the curve that **maximizes** the log likelihood will be a concave function with **one global maximum**.

* The **response variable** is the target variable $y$.
* The **canonical response function** is defined as:
	* Goes from natural parameter to the mean of the distribution.
$$
g(\eta) = E[T(y);\eta]
$$
* The **canonical link function** is defined as $g^{-1}$
	* Goes from the mean of a distribution back to the natural parameter.

## Response and Link Function
1. **Linear Predictor ($\eta = X \theta$):**
- The linear predictor $\eta$ is given by $X \theta$, where $X$ is the matrix of predictors and $\theta$ is the vector of coefficients.
- This means each entry of $\eta$ is given by $\eta_{i} = \theta^T x_i$

2. **Mean of the Response ($\mu = E(Y)$):**
- The mean $\mu$ (expected value of $Y$) is linked to $\eta$ through a function $g$.

3. **Canonical Link Function ($g(\mu) = \eta$):**
- The link function $g$ relates $\mu$ to $\eta$ with $g(\mu) = \eta$. This transformation aligns the mean with the linear predictor in a way that suits the response distribution:
	- Example: For binomial, $g(\mu) = \log(\frac{\mu}{1 - \mu})$ (logit link).
	- Example: For Poisson, $g(\mu) = \log(\mu)$ (log link).

4. **Canonical Response Function ($\mu = g^{-1}(\eta)$):**
- The inverse link $g^{-1}$ maps $\eta$ back to the natural scale of $\mu$, giving $\mu = g^{-1}(\eta)$:
	- Example: For binomial, $g^{-1}(\eta) = \frac{e^{\eta}}{1 + e^{\eta}}$ (inverse logit).
	- Example: For Poisson, $g^{-1}(\eta) = e^{\eta}$ (exp function).

In summary:
- **Link function:** $g(\mu) = \eta = X \theta$.
- **Response function:** $\mu = g^{-1}(\eta)$.
# Constructing GLMs
* Suppose we want to predict the value of a random variable $y$ as a function of $x$. To derive the GLM for this, we make a **three assumptions**:
![Pasted image 20241029215333](../../attachments/Pasted%20image%2020241029215333.png)
![Pasted image 20241031133506](../../attachments/Pasted%20image%2020241031133506.png)
![Pasted image 20241011172231](../../attachments/Pasted%20image%2020241011172231.png)
![Pasted image 20241011172307](../../attachments/Pasted%20image%2020241011172307.png)

## Example: Ordinary Least Squares
* Suppose the target variable $y$ (aka the response variable) is continuous and modeled as:
$$
y|x \sim \mathcal{N}(\mu, \sigma^2)
$$
* In the derivation of the GLM, we saw that $\mu = \eta$. This means that the hypothesis is:
![Pasted image 20241011174247](../../attachments/Pasted%20image%2020241011174247.png)
* The **canonical response function** is the identity function:
$$
\mu = \eta
$$
* This means that:
$$
\begin{align*}
g(\eta) &= \mu = \eta\\
&= \theta^T x = h_{\theta}(x)
\end{align*}
$$
## Example: Logistic Regression
* Suppose the target variable $y \in \{0, 1\}$ is modeled as:
$$
y|x \sim \text{Bernoulli}(\phi)
$$
* In the derivative, we had that:
$$
\phi = \frac{1}{1 + \exp(-\eta))}
$$
* Following the same steps as before, the hypothesis function is:
![Pasted image 20241011174501](../../attachments/Pasted%20image%2020241011174501.png)
* The **canonical response function** is the sigmoid function:
$$
\phi = \frac{1}{1+e^{-\eta}}
$$
* This means that:
$$
\begin{align*}
g(\eta) &= \phi = \sigma(\eta) \\
 &= \sigma(\theta^T x) = h_{\theta}(x)
\end{align*}
$$
## Example: Softmax Regression
* The response variable (aka the target) is $y \in \{1, 2, \dots, k\}$
![Pasted image 20241011175203](../../attachments/Pasted%20image%2020241011175203.png)

* We parameterize the multinomial with $k - 1$ params $\phi_{i}$ (each specifies the probability of outcome $i$).
$$
\sum_{i=1}^k \phi_{i} = 1
$$
$$
\phi_{i} = p(y=i;\phi) \quad \text{for all $i\in\{1, \dots, k-1\}$}
$$
$$
p(y=k;\phi) = 1 - \sum_{i=1}^{k-1} \phi_{i}
$$
* For notation sake, let $\phi_{k} = p(y=k;\phi)$ — note that $\phi_k$ is **not** a parameter.
![Pasted image 20241011175611](../../attachments/Pasted%20image%2020241011175611.png)
* Using an indicator function, we can write $T(y)$ as:
$$
(T(y))_{i} = 1\{y=i\}
$$
* Furthermore, we have that:
$$
\begin{align*}
E[(T(y))_{i}] &= 0 * (p(y=i-1) + \dots) + 1 * p(y=i)\\
&= p(y=i) \\
&= \phi_{i}
\end{align*}
$$
* To show that the multinomial is part of the exponential family, we rearrange it:
![Pasted image 20241011180023](../../attachments/Pasted%20image%2020241011180023.png)

* The **link function:**
![Pasted image 20241011180132](../../attachments/Pasted%20image%2020241011180132.png)

* The **response function** is:
![Pasted image 20241011180159](../../attachments/Pasted%20image%2020241011180159.png)
* The response function mapping the $\phi$'s to the $\eta$'s is the **softmax function**.
![Pasted image 20241011180619](../../attachments/Pasted%20image%2020241011180619.png)
* This equation can also be written as:
$$
p(y = i| x; \theta) = \frac{e^{\theta_{i}^Tx}}{1 + \sum_{j=1}^{k-1}e^{\theta_{j}^Tx}}
$$
![Pasted image 20241011180958](../../attachments/Pasted%20image%2020241011180958.png)

### Parameter Fitting
* If we have a training set of $n$ training examples $\{(x^{(i)}, y^{(i)}); i=1, \dots, n\}$
* We can learn the parameters $\theta_{i}$ from this model by writing down the likelihoods:
![Pasted image 20241011181235](../../attachments/Pasted%20image%2020241011181235.png)
![Pasted image 20241011181219](../../attachments/Pasted%20image%2020241011181219.png)

* If we take the gradient of this, we will eventually get:
![Pasted image 20241011181319](../../attachments/Pasted%20image%2020241011181319.png)

### Summary
![Pasted image 20241011181330](../../attachments/Pasted%20image%2020241011181330.png)
![Pasted image 20241011181340](../../attachments/Pasted%20image%2020241011181340.png)
![Pasted image 20241011181344](../../attachments/Pasted%20image%2020241011181344.png)

## Example: Poisson Regression
* The response variable is $y\in Z$ as parameterized by $\lambda$.
![Pasted image 20241011181647](../../attachments/Pasted%20image%2020241011181647.png)

* The **canonical response function** is the exponential:
$$
\lambda = \exp(\eta)
$$
* This means that:
$$
\begin{align*}
g(\eta) = E[y;\eta] &= \lambda = \exp(\eta) \\
&= \exp(\theta^T x)
\end{align*}
$$
### Parameter Fitting
![Pasted image 20241011181939](../../attachments/Pasted%20image%2020241011181939.png)


## Example: Exponential
Here's a lecture note-style format of the same content:

# Generalized Linear Models and the Exponential Family

## Key Concept: The Exponential Family
Many common probability distributions belong to the exponential family, which has the general form:
$$ P(y;\eta) = b(y)\exp(\eta^T T(y) - a(\eta)) 
$$

## Example: Exponential Distribution as a GLM
Let's demonstrate how the exponential distribution fits into the GLM framework.
### 1. The Basic Distribution
The exponential distribution has PDF:
$$ p(y;\lambda) = \begin{cases} 
\lambda e^{-\lambda y} & y \geq 0 \\
0 & y < 0
\end{cases} 
$$
where $\lambda > 0$ is the rate parameter.
### 2. Showing It's in the Exponential Family
Let's rewrite the PDF in exponential family form:
$$
 \begin{align*}
p(y;\lambda) &= \lambda e^{-\lambda y} \\
&= e^{\log \lambda}e^{-\lambda y} \\
&= e^{(-\lambda)y-(-\log(\lambda))}
\end{align*} $$
This gives us:
- Natural parameter: $\eta = -\lambda$
- Sufficient statistic: $T(y) = y$
- Base measure: $b(y) = 1$
- Log partition function: $a(\eta) = -\log(-\eta)$

### 3. GLM Components
**Design Choices:**
1. Response Distribution: $y|x;\theta \sim \text{Exponential}(\lambda)$
2. Linear Predictor: $\eta = \theta^T x$ (implies $\lambda = -\theta^T x$)
3. Prediction Function: $h(x) = E[y|x] = \frac{1}{\lambda} = -\frac{1}{\theta^T x}$

### 4. Model Training
The gradient of the log-likelihood has the familiar form:
$$ \frac{\partial}{\partial\theta_j}\ell(\theta) = \sum_{i=1}^m (y^{(i)} - h_\theta(x^{(i)})) x^{(i)}_j $$
This is derived from:
$$ \begin{align*} \ell(\theta) &= \log p(\vec{y} | X;\theta) \\ &= \log \prod_{i=1}^m p(y^{(i)} | x^{(i)};\theta) \\ &= \log \prod_{i=1}^m e^{(-\lambda)y^{(i)}+\log(\lambda)} \\ &= \log \prod_{i=1}^m e^{(\theta^T x^{(i)})y^{(i)}+\log(-\theta^T x^{(i)})} \\ &= \sum_{i=1}^m (\theta^T x^{(i)})y^{(i)} + \log(-\theta^T x^{(i)}) \\ \ \frac{\partial}{\partial\theta_j}\ell(\theta) &= \sum_{i=1}^m x^{(i)}_j y^{(i)} - \frac{1}{(-\theta^T x^{(i)})}x^{(i)}_j \\ &= \sum_{i=1}^m (y^{(i)} - \frac{1}{(-\theta^T x^{(i)})})x^{(i)}_j \\ &= \sum_{i=1}^m (y^{(i)} - h_\theta(x^{(i)})) x^{(i)}_j \end{align*} $$
### Key Takeaways
1. The exponential distribution naturally fits into the exponential family framework
2. The gradient of the log likelihood function with respect to the parameter $\theta$ has the same form as the GLMs for other exponential family distributions we have seen in class: $\frac{\partial}{\partial\theta_j}\ell(\theta) = \sum_{i=1} (y^{(i)} - h_\theta(x^{(i)})) x_j$
	1. (actual - predicted) × feature
