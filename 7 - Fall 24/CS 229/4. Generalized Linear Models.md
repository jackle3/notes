
* Linear regression and logistic regression are a subset of the family of GLMs.
* Linear regression was:$$
y|x;\theta \sim \mathcal{N}(\mu, \sigma^2)
$$
* Logistic regression was:
$$
y|x;\theta \sim \text{Bernoulli}(\phi)
$$
# Exponential Family
* A distribution is in the exponential family if it can be written as:
$$
p(y;\eta) = b(y)\exp(\eta^T T(y) - a(\eta))
$$
* $\eta$ is the **natural parameter** (aka the canonical parameter).
* $T(y)$ is the **sufficient statistic** (often $T(y) = y)$
* $a(\eta)$ is the **log partition function**
	* This quantity makes it so that the distribution sums/integrates over $y$ to 1.

* A fixed choice of $b$, $T$, and $a$ defines a different distribution in the family.
![[Pasted image 20241011171654.png]]


## Bernoulli Example
![[Pasted image 20241011171452.png]]
![[Pasted image 20241011171836.png]]
![[Pasted image 20241011171912.png]]

## Gaussian Example
* To simply the derivation, suppose $\sigma^2 = 1$\
![[Pasted image 20241011171458.png]]
![[Pasted image 20241011171947.png]]

## Other Families
![[Pasted image 20241011172017.png]]

# Properties of GLMs
* GLMs always give us a convex optimization problem. This means the curve that **maximizes** the log likelihood will be a concave function with **one global maximum**.

* The **response variable** is the target variable $y$.
* The **canonical response function** is defined as:
$$
g(\eta) = E[T(y);\eta]
$$
* The **canonical link function** is defined as $g^{-1}$
# Constructing GLMs
* Suppose we want to predict the value of a random variable $y$ as a function of $x$. To derive the GLM for this, we make a few **assumptions**:
![[Pasted image 20241029215333.png]]
![[Pasted image 20241011172231.png]]
![[Pasted image 20241011172307.png]]

## Example: Ordinary Least Squares
* Suppose the target variable $y$ (aka the response variable) is continuous and modeled as:
$$
y|x \sim \mathcal{N}(\mu, \sigma^2)
$$
* In the derivation of the GLM, we saw that $\mu = \eta$. This means that the hypothesis is:
![[Pasted image 20241011174247.png]]
* The **canonical response function** is the identity function:
$$
\mu = \eta
$$
* This means that:
$$
\begin{align*}
g(\eta) &= \mu = \eta\\
&= \theta^T x = h_{\theta}(x)
\end{align*}
$$
## Example: Logistic Regression
* Suppose the target variable $y \in \{0, 1\}$ is modeled as:
$$
y|x \sim \text{Bernoulli}(\phi)
$$
* In the derivative, we had that:
$$
\phi = \frac{1}{1 + \exp(-\eta))}
$$
* Following the same steps as before, the hypothesis function is:
![[Pasted image 20241011174501.png]]
* The **canonical response function** is the sigmoid function:
$$
\phi = \frac{1}{1+e^{-\eta}}
$$
* This means that:
$$
\begin{align*}
g(\eta) &= \phi = \sigma(\eta) \\
 &= \sigma(\theta^T x) = h_{\theta}(x)
\end{align*}
$$
# Example: Softmax Regression
* The response variable (aka the target) is $y \in \{1, 2, \dots, k\}$
![[Pasted image 20241011175203.png]]

* We parameterize the multinomial with $k - 1$ params $\phi_{i}$ (each specifies the probability of outcome $i$).
$$
\sum_{i=1}^k \phi_{i} = 1
$$
$$
\phi_{i} = p(y=i;\phi) \quad \text{for all $i\in\{1, \dots, k-1\}$}
$$
$$
p(y=k;\phi) = 1 - \sum_{i=1}^{k-1} \phi_{i}
$$
* For notation sake, let $\phi_{k} = p(y=k;\phi)$ â€” note that $\phi_k$ is **not** a parameter.
![[Pasted image 20241011175611.png]]
* Using an indicator function, we can write $T(y)$ as:
$$
(T(y))_{i} = 1\{y=i\}
$$
* Furthermore, we have that:
$$
\begin{align*}
E[(T(y))_{i}] &= 0 * (p(y=i-1) + \dots) + 1 * p(y=i)\\
&= p(y=i) \\
&= \phi_{i}
\end{align*}
$$
* To show that the multinomial is part of the exponential family, we rearrange it:
![[Pasted image 20241011180023.png]]

* The **link function:**
![[Pasted image 20241011180132.png]]

* The **response function** is:
![[Pasted image 20241011180159.png]]
* The response function mapping the $\phi$'s to the $\eta$'s is the **softmax function**.
![[Pasted image 20241011180619.png]]
* This equation can also be written as:
$$
p(y = i| x; \theta) = \frac{e^{\theta_{i}^Tx}}{1 + \sum_{j=1}^{k-1}e^{\theta_{j}^Tx}}
$$
![[Pasted image 20241011180958.png]]

## Parameter Fitting
* If we have a training set of $n$ training examples $\{(x^{(i)}, y^{(i)}); i=1, \dots, n\}$
* We can learn the parameters $\theta_{i}$ from this model by writing down the likelihoods:
![[Pasted image 20241011181235.png]]
![[Pasted image 20241011181219.png]]

* If we take the gradient of this, we will eventually get:
![[Pasted image 20241011181319.png]]

## Summary
![[Pasted image 20241011181330.png]]
![[Pasted image 20241011181340.png]]
![[Pasted image 20241011181344.png]]

# Example: Poisson Regression
* The reponse variable is $y\in Z$ as parameterized by $\lambda$.
![[Pasted image 20241011181647.png]]

* The **canonical response function** is the exponential:
$$
\lambda = \exp(\eta)
$$
* This means that:
$$
\begin{align*}
g(\eta) = E[y;\eta] &= \lambda = \exp(\eta) \\
&= \exp(\theta^T x)
\end{align*}
$$

## Parameter Fitting
![[Pasted image 20241011181939.png]]
