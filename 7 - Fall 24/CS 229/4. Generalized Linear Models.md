
* Linear regression and logistic regression are a subset of the family of GLMs.
* Linear regression was:$$
y|x;\theta \sim \mathcal{N}(\mu, \sigma^2)
$$
* Logistic regression was:
$$
y|x;\theta \sim \text{Bernoulli}(\phi)
$$
# Exponential Family
* A distribution is in the exponential family if it can be written as:
$$
p(y;\eta) = b(y)\exp(\eta^T T(y) - a(\eta))
$$
* $\eta$ is the **natural parameter** (aka the canonical parameter).
* $T(y)$ is the **sufficient statistic** (often $T(y) = y)$
* $a(\eta)$ is the **log partition function**
	* This quantity makes it so that the distribution sums/integrates over $y$ to 1.

* A fixed choice of $b$, $T$, and $a$ defines a different distribution in the family.
![[Pasted image 20241011171654.png|400]]


## Bernoulli
![[Pasted image 20241011171452.png|400]]
The probability mass function for a Bernoulli random variable $Y$ with parameter $p$ is:
$$
p(y; \phi) = \phi^y (1 - \phi)^{1 - y}, \quad y \in \{0, 1\}
$$
Reparameterizing with $\eta = \log\left(\frac{\phi}{1 - \phi}\right)$, we get:
$$
p(y; \eta) = \exp\left(y \eta - \log(1 + e^{\eta})\right)
$$
Exponential Family Form
- **Natural parameter**: $\eta = \log\left(\frac{\phi}{1 - \phi}\right)$
- **Sufficient statistic**: $T(y) = y$
- **Log-partition function**: $a(\eta) = -\log(1 - \phi) = \log(1 + e^{\eta})$
- **Base measure**: $b(y) = 1$

Parameter Relationship
- $\phi = \frac{1}{1 + e^{-\eta}}$ (the sigmoid function)

![[Pasted image 20241011171912.png|400]]

## Gaussian
* To simply the derivation, suppose $\sigma^2 = 1$
![[Pasted image 20241011171458.png]]
The probability density function for a Gaussian random variable $Y$ with mean $\mu$ and variance $\sigma^2$ is:
$$
p(y; \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(y - \mu)^2}{2 \sigma^2}\right)$$

Setting $\eta = \frac{\mu}{\sigma^2}$, we rewrite it as:
$$
p(y; \eta) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(\eta y - \frac{\eta^2 \sigma^2}{2}\right)
$$
Exponential Family Form
* **Natural parameter**: $\eta = \frac{\mu}{\sigma^2}$
* **Sufficient statistic**: $T(y) = y$
* **Log-partition function**: $a(\eta) = \frac{\eta^2 \sigma^2}{2}$
* **Base measure**: $b(y) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{y^2}{2 \sigma^2}\right)$

Parameter Relationship
* $\mu = \eta \sigma^2$

## Binomial
The probability mass function for a Binomial random variable $Y$ with parameters $n$ and $p$ is:
$$
p(y; n, p) = \binom{n}{y} p^y (1 - p)^{n - y}
$$
With $\eta = \log\left(\frac{p}{1 - p}\right)$:
$$
p(y; \eta) = \binom{n}{y} \exp\left(y \eta - n \log(1 + e^{\eta})\right)
$$
Exponential Family Form
* **Natural parameter**: $\eta = \log\left(\frac{p}{1 - p}\right)$
* **Sufficient statistic**: $T(y) = y$
* **Log-partition function**: $a(\eta) = n \log(1 + e^{\eta})$
* **Base measure**: $b(y) = \binom{n}{y}$

Parameter Relationship
* $p = \frac{1}{1 + e^{-\eta}}$ (also the sigmoid function)

## Negative Binomial
The probability mass function for a Negative Binomial random variable $Y$, representing the number of trials to get $k$ successes, is:
$$
p(y; k, p) = \binom{y - 1}{k - 1} (1 - p)^{y - k} p^k, \quad y \in \{k, k+1, k+2, \dots\}
$$
This can be expressed in exponential family form by reparameterizing with the natural parameter $\eta = \log(1 - p)$ so that $p = 1 - e^{\eta}$. We then have:
$$
p(y; \eta) = \binom{y - 1}{k - 1} \exp\left((y - k) \eta + k \log(1 - e^{\eta})\right)
$$
Exponential Family Form
* **Natural parameter**: $\eta = \log(1 - p)$
* **Sufficient statistic**: $T(y) = y - k$
* **Log-partition function**: $a(\eta) = -k \log(1 - e^{\eta})$
* **Base measure**: $b(y) = \binom{y - 1}{k - 1}$

Parameter Relationship
* $p = 1 - e^{\eta}$

## Poisson
The probability mass function for a Poisson random variable $Y$ with mean $\lambda$ is:
$$
p(y; \lambda) = \frac{\lambda^y e^{-\lambda}}{y!}, \quad y \in \{0, 1, 2, \dots\}
$$
Using $\eta = \log(\lambda)$:
$$
p(y; \eta) = \frac{1}{y!} \exp\left(y \eta - e^{\eta}\right)
$$
Exponential Family Form
* **Natural parameter**: $\eta = \log(\lambda)$
* **Sufficient statistic**: $T(y) = y$
* **Log-partition function**: $a(\eta) = e^{\eta}$
* **Base measure**: $b(y) = \frac{1}{y!}$

Parameter Relationship
* $\lambda = e^{\eta}$

## Geometric
The probability mass function for a Geometric random variable $Y$, counting trials until the first success, with success probability $p$ is:
$$
p(y; p) = (1 - p)^{y - 1} p, \quad y \in \{1, 2, 3, \dots\}
$$
With $\eta = \log(1 - p)$:
$$
p(y; \eta) = \exp\left((y - 1) \eta + \log(1 - e^{\eta})\right)
$$
Exponential Family Form
* **Natural parameter**: $\eta = \log(1 - p)$
* **Sufficient statistic**: $T(y) = y - 1$
* **Log-partition function**: $a(\eta) = -\log(1 - e^{\eta})$
* **Base measure**: $b(y) = 1$

Parameter Relationship
* $p = 1 - e^{\eta}$

## Multinomial
The probability mass function for a Multinomial random variable $Y = (Y_1, \dots, Y_k)$ with $n$ trials and category probabilities $\mathbf{p} = (p_1, \dots, p_k)$ is:
$$
p(y; \mathbf{p}) = \frac{n!}{y_1! y_2! \cdots y_k!} p_1^{y_1} p_2^{y_2} \cdots p_k^{y_k}
$$
Setting $\eta_i = \log(p_i)$, we have:
$$
p(y; \mathbf{\eta}) = \frac{n!}{y_1! y_2! \cdots y_k!} \exp\left( \sum_{i=1}^k y_i \eta_i - n \log\left(\sum_{i=1}^k e^{\eta_i}\right) \right)
$$
Exponential Family Form
* **Natural parameters**: $\eta_i = \log(p_i)$
* **Sufficient statistic**: $T(y) = (y_1, \dots, y_k)$
* **Log-partition function**: $a(\eta) = n \log\left(\sum_{i=1}^k e^{\eta_i}\right)$
* **Base measure**: $b(y) = \frac{n!}{y_1! y_2! \cdots y_k!}$

Parameter Relationship
* $p_i = \frac{e^{\eta_i}}{\sum_{j=1}^k e^{\eta_j}}$ (Softmax function)

## Multivariate Gaussian
For a Multivariate Gaussian random variable $\mathbf{Y}$ with mean vector $\mathbf{\mu}$ and covariance matrix $\Sigma$:
$$
p(\mathbf{y}; \mathbf{\mu}, \Sigma) = \frac{1}{(2 \pi)^{k/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (\mathbf{y} - \mathbf{\mu})^T \Sigma^{-1} (\mathbf{y} - \mathbf{\mu})\right)
$$
With natural parameters $\eta = \Sigma^{-1} \mu$ and $\Lambda = \Sigma^{-1}$, we rewrite this as:
$$
p(\mathbf{y}; \eta, \Lambda) = \exp\left(\eta^T \mathbf{y} - \frac{1}{2} \mathbf{y}^T \Lambda \mathbf{y} - a(\eta, \Lambda)\right)
$$
Exponential Family Form
* **Natural parameters**: $\eta = \Sigma^{-1} \mu$, $\Lambda = \Sigma^{-1}$
* **Sufficient statistic**: $T(\mathbf{y}) = (\mathbf{y}, \mathbf{y} \mathbf{y}^T)$
* **Log-partition function**: $a(\eta, \Lambda) = \frac{1}{2} \eta^T \Lambda^{-1} \eta + \frac{1}{2} \log|\Lambda|$
* **Base measure**: $b(\mathbf{y}) = \frac{1}{(2 \pi)^{k/2}}$

Parameter Relationships
* Mean: $\mathbf{\mu} = \Lambda^{-1} \eta$
* Covariance: $\Sigma = \Lambda^{-1}$

## Other Families
![[Pasted image 20241011172017.png]]

# Properties of GLMs
* GLMs always give us a convex optimization problem. This means the curve that **maximizes** the log likelihood will be a concave function with **one global maximum**.

* The **response variable** is the target variable $y$.
* The **canonical response function** is defined as:
$$
g(\eta) = E[T(y);\eta]
$$
* The **canonical link function** is defined as $g^{-1}$
# Constructing GLMs
* Suppose we want to predict the value of a random variable $y$ as a function of $x$. To derive the GLM for this, we make a few **assumptions**:
![[Pasted image 20241029215333.png]]
![[Pasted image 20241011172231.png]]
![[Pasted image 20241011172307.png]]

## Example: Ordinary Least Squares
* Suppose the target variable $y$ (aka the response variable) is continuous and modeled as:
$$
y|x \sim \mathcal{N}(\mu, \sigma^2)
$$
* In the derivation of the GLM, we saw that $\mu = \eta$. This means that the hypothesis is:
![[Pasted image 20241011174247.png]]
* The **canonical response function** is the identity function:
$$
\mu = \eta
$$
* This means that:
$$
\begin{align*}
g(\eta) &= \mu = \eta\\
&= \theta^T x = h_{\theta}(x)
\end{align*}
$$
## Example: Logistic Regression
* Suppose the target variable $y \in \{0, 1\}$ is modeled as:
$$
y|x \sim \text{Bernoulli}(\phi)
$$
* In the derivative, we had that:
$$
\phi = \frac{1}{1 + \exp(-\eta))}
$$
* Following the same steps as before, the hypothesis function is:
![[Pasted image 20241011174501.png]]
* The **canonical response function** is the sigmoid function:
$$
\phi = \frac{1}{1+e^{-\eta}}
$$
* This means that:
$$
\begin{align*}
g(\eta) &= \phi = \sigma(\eta) \\
 &= \sigma(\theta^T x) = h_{\theta}(x)
\end{align*}
$$
## Example: Softmax Regression
* The response variable (aka the target) is $y \in \{1, 2, \dots, k\}$
![[Pasted image 20241011175203.png]]

* We parameterize the multinomial with $k - 1$ params $\phi_{i}$ (each specifies the probability of outcome $i$).
$$
\sum_{i=1}^k \phi_{i} = 1
$$
$$
\phi_{i} = p(y=i;\phi) \quad \text{for all $i\in\{1, \dots, k-1\}$}
$$
$$
p(y=k;\phi) = 1 - \sum_{i=1}^{k-1} \phi_{i}
$$
* For notation sake, let $\phi_{k} = p(y=k;\phi)$ â€” note that $\phi_k$ is **not** a parameter.
![[Pasted image 20241011175611.png]]
* Using an indicator function, we can write $T(y)$ as:
$$
(T(y))_{i} = 1\{y=i\}
$$
* Furthermore, we have that:
$$
\begin{align*}
E[(T(y))_{i}] &= 0 * (p(y=i-1) + \dots) + 1 * p(y=i)\\
&= p(y=i) \\
&= \phi_{i}
\end{align*}
$$
* To show that the multinomial is part of the exponential family, we rearrange it:
![[Pasted image 20241011180023.png]]

* The **link function:**
![[Pasted image 20241011180132.png]]

* The **response function** is:
![[Pasted image 20241011180159.png]]
* The response function mapping the $\phi$'s to the $\eta$'s is the **softmax function**.
![[Pasted image 20241011180619.png]]
* This equation can also be written as:
$$
p(y = i| x; \theta) = \frac{e^{\theta_{i}^Tx}}{1 + \sum_{j=1}^{k-1}e^{\theta_{j}^Tx}}
$$
![[Pasted image 20241011180958.png]]

### Parameter Fitting
* If we have a training set of $n$ training examples $\{(x^{(i)}, y^{(i)}); i=1, \dots, n\}$
* We can learn the parameters $\theta_{i}$ from this model by writing down the likelihoods:
![[Pasted image 20241011181235.png]]
![[Pasted image 20241011181219.png]]

* If we take the gradient of this, we will eventually get:
![[Pasted image 20241011181319.png]]

### Summary
![[Pasted image 20241011181330.png]]
![[Pasted image 20241011181340.png]]
![[Pasted image 20241011181344.png]]

## Example: Poisson Regression
* The response variable is $y\in Z$ as parameterized by $\lambda$.
![[Pasted image 20241011181647.png]]

* The **canonical response function** is the exponential:
$$
\lambda = \exp(\eta)
$$
* This means that:
$$
\begin{align*}
g(\eta) = E[y;\eta] &= \lambda = \exp(\eta) \\
&= \exp(\theta^T x)
\end{align*}
$$

### Parameter Fitting
![[Pasted image 20241011181939.png]]
