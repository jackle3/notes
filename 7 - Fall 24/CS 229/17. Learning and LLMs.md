* AI models are based on the type of learning task ⟶ LLMs use self-supervised learning.

# Classical Learning
## **Supervised Learning**
* Learn a function $f(x)$ to predict $y$ from $x$
![[Pasted image 20241129221827.png]]

## **Unsupervised Learning**
* There is no $y$. You want to discover structure from data.
![[Pasted image 20241129221857.png]]

# **Learning From Less Data**
![[Pasted image 20241129221918.png]]

## **Semi-supervised Learning**
* Use a little bit of labeled data and lots of unlabeled data to make a prediction.
* For LLMs, it has data from the entire web, but only some data from task at hand.
![[Pasted image 20241129221930.png]]
* Given a Gaussian of what we know so far:
	* take a guess for unlabeled data ⟶ $r_{ik} \sim P$
	* for labeled data, fix the prediction $r_{ik} = 1$ if $k = y$, $0$ otherwise.
* Then we learn on weighted data.
![[Pasted image 20241129222202.png]]

## Weakly Supervised Learning
* In computer vision, it's much cheaper to have weaker labels ⟶ uncertainty model.
* For instance, the types of labels might be:
	* Fully observed label ⟶ perfect bounding box around cat
	* High-level label ⟶ "there is a cat in this image somewhere, somehow"
	* Cheap label ⟶ "there is a cat on this scribble"
![[Pasted image 20241129223122.png]]

## Mutitask Learning
* Build a model that is able to perform multiple tasks.
* For the model:
	* There is a common structure shared between tasks (i.e. foundation model)
	* There is a **much simpler** task-specific structure.
![[Pasted image 20241129224032.png]]

## Transfer Learning
* Suppose you have a lot of data for cats and dogs, but not much for others.
* Transfer a **model trained on abundant data** to a **new task with limited dat**a.
![[Pasted image 20241129224132.png]]
* We use the neural net (i.e. foundation model) as a feature extractor, then fine-tune it to a simple classifier.
![[Pasted image 20241129224153.png]]
* We keep the generic layers and drop the last few specific layers.
![[Pasted image 20241129224238.png]]
* This allows you to learn from much smaller datasets ⟶ fix the foundation models, train the classifier
![[Pasted image 20241129224252.png]]

## Few-Shot Learning
* Lots of data for one task, little data for another task.
	* Learn a new task using very little data by reusing a model from another task.
![[Pasted image 20241129224415.png]]

## Zero-Shot Learning
* This is what LLMs do ⟶ without ever being given explicit data, it can make predictions
* It learns this via **side information**
	* E.g. "zebras are like horses with stripes", and if the model knows horses and stripes, it can learn zebras
![[Pasted image 20241129224524.png]]

# Embeddings
![[Pasted image 20241129224840.png]]
* **Embeddings**: Represent each word as a high dimensional vector **learned from data**
	* Learn embeddings such that they can be used to predict the next word in a sentence
	* Maps sentence embedding $x \in \mathbb{R}^{300}$ to next word $y \in \mathbb{R}^{|V|}$ via function $f(x)$
		* think of $y$ as a softmax giving likelihood of each index being next word

### GloVe Embeddings
* Allows you to perform nearest neighbors in the embedding space.
	* This meant you no longer needs to keep a dictionary of plurals, synonyms, etc ⟶ their embeddings would just be close to each other.
![[Pasted image 20241129225204.png]]

* This also gives **linear structures** in the embedding space.
	* There is a linear relationship between:
		* `embedding of CEO - embedding of company name`
		* `zip code - city name`
		* `preposition of concepts (e.g. strong, stronger, strongest)
![[Pasted image 20241129225404.png]]

* This can also be used for the **analogy task** ⟶ A is to X as B is to Y.
	* E.g. `king - man + women = queen`
![[Pasted image 20241129225514.png]]

# Self-Supervised Learning
* There is no explicit label $y$
	* $y$ could be somehow derived from the data (e.g. label is next word based on sentence so far)
* It can change between time steps. Eg:
	* in step T, $x$ = "machine" and and we predict  $y$ = "learning"
	* in step T + 1, $x$ = "learning" and we preidct $y$ = "class"
![[Pasted image 20241129225745.png]]

* In the context of training LLMs:
	* Loss function: predict next word $y$ given embedding $x$ (cross entropy)
	* Parameters:
		* logistic regression parameters ⟶ maps $\mathbb{R}^{300} \to \mathbb{R}^{|V|}$
		* embedding params ⟶ for each word, we have a 300-dim vector
			* so we have $\text{\# words} \times 300$
	* Optimizer: some variant of gradient descent (e.g. Adam)

# Emergence
* Language models have been getting bigger (in terms of # of params) over the years
* When language models get big enough, new capabilities begin to **emerge**

## What is Emergence?
* When you combine self-supervised learning (i.e. next word prediction) and scale, you can get the model to perform **few-shot learning**
* You can give it some new context and the model will just perform the task!
![[Pasted image 20241129231758.png]]

## How is Emergence Possible?
* If we think about the auto-complete task:
	* notice that in each of these examples, the model has learned different aspects of language
![[Pasted image 20241129231931.png]]

## Learning from Context
* With foundational models and emergence, we can give it a **short context x** as a task
![[Pasted image 20241129231957.png]]
* This is the same with **writing code from comments!**
![[Pasted image 20241129232430.png]]

# Why Can't We Use Supervised Learning?
* SQuAD is a supervised dataset ⟶ it has 50 million tokens
* The DCLM dataset is used to train LLMs ⟶ it has 240 trillion tokens, most of which is not labeled
![[Pasted image 20241129232148.png]]

# Image Generation
* In 2014, images were generated using GANs
![[Pasted image 20241129232522.png]]
* Now, we can generate it simply from text with LLMs
![[Pasted image 20241129232538.png]]

## Multimodal Embeddings
**Training**
* Take images and put it through some NN to get an embedding.
* Take the caption and put it through some NN to get another embedding.
* Train to get these two embeddings to match ⟶ model brings text and images into embedding space.

**Predicting**
* Give a new caption to get an embedding.
* Then suppose this embedding is an image, and **decode** (reverse process) to get an image back.
![[Pasted image 20241129232730.png]]

# Scaling Laws
* After a certain scale of model, capabilities emerge ⟶ huge boost in performance
![[Pasted image 20241129232112.png]]

* The **behavior of models as they scare are predictable**.
	* You can train small models to fit a line between loss and number of parameters.
	* You can then say "if I want this loss, how many params do I need based on the fit line"
![[Pasted image 20241129232850.png]]

* The amount of training data should also increase with the number of params.
![[Pasted image 20241129232959.png]]

# Foundation Model
* Foundation models are:
	* Trained on broad data (self-supervised at scale)
	* Can be adapted (lightly and effectively) to a wide range of downstream tasks.
	* This is **transfer learning** ⟶ same model for new tasks
![[Pasted image 20241129233208.png]]

* Foundation models can do this via **prompting**
![[Pasted image 20241129233317.png]]

## Example Prompts
* This is zero-shot learning (we gave it no examples)
![[Pasted image 20241129233327.png]]
* **In-context learning** is a general term for this ⟶ model learns from prompt.
![[Pasted image 20241129233350.png]]

## LLMs as Few-shot Learners
* Having good prompts with good descriptions of the tasks make a big difference.
![[Pasted image 20241129233448.png]]
