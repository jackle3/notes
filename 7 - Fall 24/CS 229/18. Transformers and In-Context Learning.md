
# In-Context Learning
* Add examples to the prompt so that the model learns the task.
![Pasted image 20241201205814](Pasted%20image%2020241201205814.png)
# Fine-tuning
* If you have specific task data, you can use gradient descent to update the parameters of the model ⟶ tune it to the task.
![Pasted image 20241201210228](Pasted%20image%2020241201210228.png)

# Risks and Harms of Foundation Models
* Models can be brittle
	* Lacks common sense, internal consistency
* Models can be harmful
	* Generate offensive content, untruthful content, and enable disinformation

# Transformers
* These are the basic structures of LLMs
* These models are built in terms of words/tokens ⟶ trained for **predicting next word**
## Naive Implementation
* Given an embedding, it passes it through:
	* Linear embedding function ⟶ creates logits (unnormalized weights for each possible output)
	* It then uses a softmax to turn it into a probability distribution.
	* The max of this gives us the most likely next word
![Pasted image 20241201210526](Pasted%20image%2020241201210526.png)

## Context Window
* A lot of the current research in LLMs is to make context window as large as possible.
![Pasted image 20241201210641](Pasted%20image%2020241201210641.png)

### Finite Lag
* Line of the embeddings of the last K words (e.g. 4), then pass it through an NN.
![Pasted image 20241201210707](Pasted%20image%2020241201210707.png)
* **Disadvantage:** limited to small context window because the input to the NN becomes too large otherwise.

## Adding Up Previous Context
* Instead, we can add up the entire context to make the input to the NN smaller.
![Pasted image 20241201210856](Pasted%20image%2020241201210856.png)
* **Disadvantage:** this ignores word order ⟶ pretty much a bag-of-words model now.

## Attention Block
* Performs a **weighted average** to put more attention on more important words ⟶ this weight is learned.
![Pasted image 20241201211001](Pasted%20image%2020241201211001.png)

## Self-Attention
* Notice that the model can **learn** which part of the sentence it should focus on:
	* E.g. depending on "tired" vs "wide", the subject differs.
![Pasted image 20241201211113](Pasted%20image%2020241201211113.png)

### Computing Self-Attention
* The **key** $k_i, \dots$ is the set of previous words.
* The **query** $q_i$ is the current word.
* Attention is how much the key and the query are aligned.
![Pasted image 20241201211258](Pasted%20image%2020241201211258.png)
* How much the query $i$ aligns with the key $j$ is
$$
q_{i} \cdot k_{j} \tag{attention value}
$$
* We have a bunch of these for each key and query pair.
![Pasted image 20241201211404](Pasted%20image%2020241201211404.png)
* We can apply a softmax to normalize this.

## Transformer Block
* Each attention block (in red) attends to the previous words in the context.
* It is trained to predict the next word (should match the next green)
![Pasted image 20241201211558](Pasted%20image%2020241201211558.png)
* The transformer basically just computes a better embedding for word $i$
	* Given input embedding $i$
	* It transforms it into a better embedding for word $i$ comptued by the attention weighted average of all embeddings in context with the goal of predicting the next word.
![Pasted image 20241201211729](Pasted%20image%2020241201211729.png)
![Pasted image 20241201212325](Pasted%20image%2020241201212325.png)

## Computing Query, Key, Value Vectors
* These fixed parameters (the weight matrices) are **learned** with gradient descent on training data with loss function being cross entropy for next word prediction.
* These are the **self-attention block parameters**
![Pasted image 20241201212540](Pasted%20image%2020241201212540.png)

## Position Embeddings
* We can implicitly take positions in input into account.
* We modify the embedding by adding a **positional encoding** to it
	* Position encodings kind of look like fourrier functions (on the right)
![Pasted image 20241201214547](Pasted%20image%2020241201214547.png)

# Residual Connections and Layer Norm
* For very deep models, gradients from previous layers can become zero.
* To fix this, we add
	* **residual connections** ⟶ just directly add embedding from previous layer
	* **layer normalization** ⟶ normalize inputs by norm and variance
![Pasted image 20241201214646](Pasted%20image%2020241201214646.png)

## Full Model
![Pasted image 20241201214636](Pasted%20image%2020241201214636.png)
![Pasted image 20241201214909](Pasted%20image%2020241201214909.png)

# Training LLMs
* LLMs have three training stages:
	1. Pretraining
		* Captures broad knowledge (e.g. language understanding)
		* This is where we feed the model the entire web data
	2. Instruction fine-tuning
		* Tunes ability to follow instruction prompts
	3. Human preference tuning
		* Align model with human preferences
![Pasted image 20241201215407](Pasted%20image%2020241201215407.png)

## Pretraining
![Pasted image 20241201215429](Pasted%20image%2020241201215429.png)
![Pasted image 20241201215448](Pasted%20image%2020241201215448.png)
* The loss during pretraining is very unstable
	* Requires you to manually backtrack the model every time the loss explodes.
![Pasted image 20241201215612](Pasted%20image%2020241201215612.png)
* The cross entropy loss function is:
	* What is the log probability of the next token? How likely am I to predict or correctly?
	* This is similar to logistic regression, but it's the self-supervised version ⟶ conditions on previous tokens.
![Pasted image 20241201215706](Pasted%20image%2020241201215706.png)

## Instruction Fine-Tuning
* We want to be able to ask the model questions about its data ⟶ make it follow instructions.
![Pasted image 20241201215750](Pasted%20image%2020241201215750.png)

**The dataset is:**
* given instruction $x^{(i)}$, we want to get response $y^{(i)}$ with high probability.
* the response is provided ⟶ supervised learning
![Pasted image 20241201215812](Pasted%20image%2020241201215812.png)
![Pasted image 20241201215930](Pasted%20image%2020241201215930.png)

**This is supervised fine-tuning**
* Our goal is predict next word of response $y$ ⟶ modify loss function to condition on response and instruction
![Pasted image 20241201220021](Pasted%20image%2020241201220021.png)

**We usually want to regularize SFT**
* Risks without regularization:
	* Overfit to instruction data
	* Forget pretrained knowledge
* We use **KL Regularization**:
	* Penalize models that get too far from pretrained model
	* $\text{KL}(\pi_{\theta}||\pi_{\text{pretrained}})$
* The overall loss is:
$$
\mathcal{L}_{\text{SFT}}(\theta) + \lambda \cdot \text{KL}(\pi_{\theta}||\pi_{\text{pretrained}})
$$
## Preference Tuning
![Pasted image 20241201220412](Pasted%20image%2020241201220412.png)

**Below are two examples of AI outputs, which is better?**
![Pasted image 20241201220428](Pasted%20image%2020241201220428.png)
![Pasted image 20241201220450](Pasted%20image%2020241201220450.png)

**If we naively use human feedback, it can be tough**
![Pasted image 20241201220501](Pasted%20image%2020241201220501.png)

**The fix is pairwise comparison**
![Pasted image 20241201220544](Pasted%20image%2020241201220544.png)
![Pasted image 20241201220549](Pasted%20image%2020241201220549.png)

**To learn from pairwise preferences, we have:**
* Starting model $\pi_{\text{ref}}$

* The dataset is:
![Pasted image 20241201220649](Pasted%20image%2020241201220649.png)

* To train on this, we have to use different methods (can't just do cross entropy)
![Pasted image 20241201220656](Pasted%20image%2020241201220656.png)

### Direct Preference Optimization (DPO)
* To want to increase the relative increase in probability for preferred responses.
![Pasted image 20241201220810](Pasted%20image%2020241201220810.png)
![Pasted image 20241201220930](Pasted%20image%2020241201220930.png)
