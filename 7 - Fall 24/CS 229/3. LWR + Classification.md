
# Recap
![[Pasted image 20241001203617.png]]

# Probabilistic Interpretation
* Why might linear regression be a good choice?
* We'll motivate that by deriving the linear regression algorithm under some probabilistic assumptions.

* Suppose the target variables and inputs are related via an error term.
![[Pasted image 20241001204318.png]]
![[Pasted image 20241001204347.png]]

* This assumption means that the distribution of $y^{(i)}$ given $x^{(i)}$ as parametrized by $\theta$ is:
![[Pasted image 20241001204536.png]]

## Likelihood
* Given $X$ (the design matrix containing all the inputs $x^{(i)}$) and $\theta$, we can try to find the distribution of the $y^{(i)}$'s.
* Define **likelihood** as the probability of seeing these data examples as parameterized by $\theta$.
![[Pasted image 20241001204827.png]]
![[Pasted image 20241001204832.png]]

## Maximum Likelihood Estimation
* To get a good $\theta$, we should choose $\theta$ so as to make the data as high probability as possible.
	* In other words, choose $\theta$ to get the maximum likelihood $L(\theta)$.
![[Pasted image 20241001205042.png]]
![[Pasted image 20241001205114.png]]

## Summary
* Under the assumptions on the data, the least-squares cost function corresponds to finding the MLE of $\theta$.
* This is why least squares/linear regression is a pretty natural method for solving regression problems.

---
# Locally Weighted Regression
* A modification on linear regression to better fit nonlinear functions.
	* Assuming sufficient training data, it makes the choice of features less critical.

* Consider the following dataset
![[Pasted image 20241001203817.png]]

* So far with linear regression, we try to fit a line that best matches all the data.
![[Pasted image 20241001203834.png]]

* With locally weighted regression, we look at a **window** surrounding our prediction point to fit a line just for that prediction.
	* In other words, we retrain each time we make a prediction.
![[Pasted image 20241001203900.png]]

## Definition
* For this, we add non-negative valued weights $w^{(i)}$ as coefficients.
![[Pasted image 20241001203931.png]]

## Parametric Learning
* For parametric algorithms, once we've fit the $\theta$s and stored them, we no longer need to keep the data around.
* For non-parametric algorithms, we need to keep data around to retrain for each prediction.
![[Pasted image 20241001204042.png]]


# Classification
* Consider the problem where the **label** takes on binary values $y = \{0, 1\}$.
![[Pasted image 20241001205324.png]]

* Linear regression doesn't make sense for this because it doesn't make sense for our prediction $h_\theta(x)$ to take values larger than 1 or smaller than 0.

## Logistic Regression
* Let $h_\theta = g(\theta^T x)$ where $g(z) = \frac{1}{1+e^{-z}}$.
$$
h_\theta(x) = \frac{1}{1+e^{-\theta^T z}}
$$
* $g(z)$ is called the logistic or sigmoid function.

## Likelihood
* We can use this to define probabilities of each class:
![[Pasted image 20241008175711.png]]

* If the $n$ training examples were IID, we can write the likelihood of the parameters $\theta$ as:
![[Pasted image 20241008175745.png]]

## Gradient Ascent
* We can maximize this log-likelihood using gradient ascent.
$$
\theta := \theta + \alpha \nabla_\theta \ell(\theta)
$$
![[Pasted image 20241008180029.png]]
* Above, we used the fact that for the sigmoid function $g$, $g'(z) = g(z)(1 - g(z))$
![[Pasted image 20241008180415.png]]

## Decision Boundaries
1. Linear decision boundary with original input:
	* When you use logistic regression with the original input features x and output y, the decision boundary will indeed be linear in the input space of x.
	* This is because logistic regression models the probability of the output using a linear combination of input features, passed through a sigmoid function.
2. Potentially non-linear decision boundary with transformed input:
	* If you transform the original input x to x' using a feature map before passing it to the logistic regression model:
		* a) The decision boundary will be linear in the transformed space (x' vs y).
		* b) The decision boundary may be non-linear in the original input space (x vs y).

## Perfect Separation with Logistic Regression
In logistic regression:
* The likelihood function is:
$$
L(\theta) = \prod_{i=1}^m p(y^{(i)}|x^{(i)}, \theta)
$$
	* Where $p(y^{(i)}|x^{(i)}, \theta) = (h_\theta(x^{(i)}))^{y^{(i)}} \cdot (1 - h_\theta(x^{(i)}))^{1-y^{(i)}}$
	* And $h_\theta(x^{(i)}) = \frac{1}{1 + e^{-(\theta^T x^{(i)})}}$

* The log-likelihood is:
$$
ll(\theta) = \sum_{i=1}^m [y^{(i)} \log(h_\theta(x^{(i)})) + (1-y^{(i)}) \log(1-h_\theta(x^{(i)}))
$$
* Perfect separation occurs when there exists a $\theta$ such that for all examples $i$:
	* $y^{(i)} = 1 \text{ when } \theta^T x^{(i)} > 0$
	* $y^{(i)} = 0 \text{ when } \theta^T x^{(i)} < 0$

* As $\|\theta\|_{2} \to \infty$:
	* For positive examples: $h_\theta(x^{(i)}) \to 1$, so $\log(h_\theta(x^{(i)})) \to 0$
	* For negative examples: $h_\theta(x^{(i)}) \to 0$, so $\log(1-h_\theta(x^{(i)})) \to 0$

* The gradient of the log-likelihood becomes:
$$
\nabla ll(\theta) = \sum_{i=1}^m (y^{(i)} - h_\theta(x^{(i)}))x^{(i)}
$$
* In perfect separation, as $\|\theta\| \to \infty$, $(y^{(i)} - h_\theta(x^{(i)})) \to 0$ for all $i$, but never quite reaches 0.
* To address this, regularization can be applied:
$$
ll_{regularized}(\theta) = ll(\theta) - \lambda\|\theta\|^2
$$
	* This ensures a finite maximum likelihood estimate even with perfect separation.

# Newton's Method
* This is another algorithm for maximizing the log likelihood $\ell(\theta)$.

* Newton's method finds $\theta$ where $f(\theta) = 0$ by doing the following update:
$$
\theta^{(t+1)} := \theta^{(t)} - \frac{f(\theta^{(t)})}{f'(\theta^{(t)})}
$$
* The steps are:
	* Fit tangent line to $f$ at the current guess $\theta^{(t)}$.
	* Solve for where the linear tangent line intersects zero .
	* Set $\theta^{(t+1)}$ to be that point and repeat.
![[Pasted image 20241008180646.png]]

* If we let the derivative of the cost function $J'(\theta) = f(\theta)$, we can find where the derivative is zero:
![[Pasted image 20241008181017.png]]

## Vector-setting
* When $\theta$ is vector-valued, the update equation becomes:
$$
\theta^{(t+1)} := \theta^{(t)} - H^{-1}\nabla_{\theta^{(t)}} J(\theta^{(t)})
$$
* Where $H$ is a $(d+1) \times (d+1)$ Hessian matrix who's entries are:
$$
H_{ij} = \frac{\partial^2 J(\theta)}{\partial \theta_i \partial \theta_j}
$$
## Convergence
* Newton's Method has quadratic convergence â€” often quicker than batch GD with much fewer iterations.
* However, each iteration can be expensive because we need to find $H$ and $H^{-1}$.
* In general, it's faster when $d$ is not too large (not too many features)

# Newton vs. GD
* Newton's method typically has faster convergence (in lower dimensions)
* Gradient descent can avoid inverting $d \times d$ Hessians (very expensive)
