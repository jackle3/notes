
# 1 Deep Learning
* Same as supervised learning, but we now work with **nonlinear models**
* In supervised learning, our hypothesis was linear in $\theta$ (maybe nonlinear in $x$)
![Pasted image 20241113140710](../../attachments/Pasted%20image%2020241113140710.png)
* In deep learning, our hypothesis is nonlinear in both $\theta$ and $x$
![Pasted image 20241113140716](../../attachments/Pasted%20image%2020241113140716.png)

# 2 General Learning Setup
* We have a dataset and and a hypothesis function.
![Pasted image 20241113140743](../../attachments/Pasted%20image%2020241113140743.png)
* The cost function on the i-th example is the squared loss:
![Pasted image 20241113140842](../../attachments/Pasted%20image%2020241113140842.png)
* The cost function for the whole dataset is the mean squared loss:
![Pasted image 20241113140902](../../attachments/Pasted%20image%2020241113140902.png)
* The optimization objective is thus:
$$
\min_{\theta} J(\theta)
$$
* To optimize this we can use **batch gradient descent:**
![Pasted image 20241113140945](../../attachments/Pasted%20image%2020241113140945.png)
* Alternatively we can use **SGD:**
	* In each iteration, we sample data point without replacement and update the parameters using the gradient of that example's cost function.
![Pasted image 20241113141033](../../attachments/Pasted%20image%2020241113141033.png)
* We can also use **minibatch SGD** to have better parallelism:
	* We compute $B$ gradients in parallel ⟶ $\nabla J^{j_{1}}(\theta) \dots \nabla J^{j_{B}}(\theta)$
		* This is faster than doing it sequentially in SGB
	* In each iteration we sample $B$ examples without replacement
	* Then we update theta using the **average of the gradients** for the $B$ examples.
![Pasted image 20241113141306](../../attachments/Pasted%20image%2020241113141306.png)

# 3 Roadmap
1. Define Neural Network parameterization $h_\theta(x)$
	* It uses a family of hypotheses that are different from what we are used to:
2. Run SGB or minibatch SGB to train the network
	* We need to compute gradients of $J^{(i)}(\theta))$
	* We can use backprop to efficiently compute gradients

* Note that neural network objectives are much more complicated ⟶ high dimensional landscape with **many peaks and valleys**.
	* Though there are some cases where the local minima is as good as global minima.
![Pasted image 20241113141700](../../attachments/Pasted%20image%2020241113141700.png)

# 4 Neural Networks
* It's a combination of matrix multiplication and doing entry-wise nonlinear operations.
* Training and evaluation can done efficiently because it plays into strengths of current hardware.

## 4.1 Single-layer NN
* Suppose we are predicting housing prices based on size (only one feature $x$)
* We have one neuron that computes the hypothesis and applies an **activation function**.
![300](../../attachments/Pasted%20image%2020241113142105.png)
* To get a prediction that doesn't go negative for prices, we use ReLU
![Pasted image 20241113142036](../../attachments/Pasted%20image%2020241113142036.png)
* In the **multidimensional** case, the hypothesis is:
![Pasted image 20241113142201](../../attachments/Pasted%20image%2020241113142201.png)

## 4.2 Stacking Neurons
* We can have one neuron pass its output as input to the next neuron.
* Using the previous example
	* suppose we now have many more features (size, num of bedrooms, zip code, wealth).
	* suppose the actual indicators were family size, walkability, school quality
	* **we can use input features to infer the indicators (thought it may also infer other things)**
* We can have model it as one neuron per indicator, and take as inputs the input features.
![Pasted image 20241113143742](../../attachments/Pasted%20image%2020241113143742.png)

* In terms of the math, we have: inputs $x$, intermediate variables $a$ (hidden layers), output $y$
![Pasted image 20241113144123](../../attachments/Pasted%20image%2020241113144123.png)

* In order to calculate the intermediate variables, we apply our parameters $\theta$ to our features:
![Pasted image 20241113144431](../../attachments/Pasted%20image%2020241113144431.png)

* Our output is this below. Note that generally we don't apply ReLU at the last layer.
![Pasted image 20241113144445](../../attachments/Pasted%20image%2020241113144445.png)

## 4.3 2-layer Fully Connected Neural Network
* Instead of using domain knowledge to build the neural network (e.g. only give some features to some neurons), we can **fully connect them**
* We can then have the model **learn** which features are important and which are not.
![Pasted image 20241113182541](../../attachments/Pasted%20image%2020241113182541.png)

# 5 Hidden Units
* Consider a 2-layer fully connected neural network with:
	* $m$ hidden units
	* $d$ dimensional input $x \in \mathbb{R}^d$
* The hidden units are defined as:
	* Note that $w^{[i]}_j$ is the weight vector of hidden unit $j$ in layer $i$
![Pasted image 20241113182738](../../attachments/Pasted%20image%2020241113182738.png)
* The hypothesis is thus:
![Pasted image 20241113182759](../../attachments/Pasted%20image%2020241113182759.png)

# 6 Vectorization
* We can simplify the previous expressions using matrix multiplication!
* We define a weight matrix $W^{[1]} \in \mathbb{R}^{m\times d}$ where each weight vector $w_j^{[1]}$ is a row.
![Pasted image 20241113183043](../../attachments/Pasted%20image%2020241113183043.png)
* We can then calculate the linear combination of input features (prior to activation) as:
![Pasted image 20241113183054](../../attachments/Pasted%20image%2020241113183054.png)
* Then we can write the **hidden layer** output as:
$$
a = \text{ReLU}(z) = \text{ReLU}(W^{[1]}x + b^{[1]})
$$
* The hypothesis is thus:
![Pasted image 20241113183404](../../attachments/Pasted%20image%2020241113183404.png)

* In summary, the **parameters** $\theta$ are:
	* Weight matrices $W^{[1]}$ and $W^{[2]}$
	* Biases $b^{[1]}$ and $b^{[2]}$
	* The first layer is $W^{[1]}$ and $b^{[1]}$
	* The second layer is $W^{[2]}$ and $b^{2]}$

# 7 Multi-layer FC Neural Network
* Suppose we have $r$ layers with parameters:
	* Weight matrices $W^{[1]}, \dots ,W^{[r]}$
	* Biases $b^{[1]}, \dots ,b^{[r]}$
* The outputs of each **hidden layer** is thus:
![Pasted image 20241113183708](../../attachments/Pasted%20image%2020241113183708.png)
* The output of the **output layer** (the hypothesis) is thus:
![Pasted image 20241113183732](../../attachments/Pasted%20image%2020241113183732.png)
* In terms of dimensions:
	* Let $m_i$ be the number of neurons in layer $i$. It must be true that:
		* $W^{[k]} \in \mathbb{R}^{m_k \times m_{k-1}}$
		* $a^{[k]} \in \mathbb{R}^{m_k}$
		* $b^{[k]} \in \mathbb{R}^{m_k}$
	* The exception is that the first and last layers, where:
		* $W^{[1]} \in \mathbb{R}^{m_1 \times d}$
		* $W^{[r]} \in \mathbb{R}^{1 \times m_{r-1}}$
* The total number of neurons is: $\sum_{i=1}^r m_{i}$
* The total number of parameters is:
$$
\left[ m_{1}\times d + m_{1}\times 1 \right] + \left[ m_{2}\times m_{1} + m_{2}\times 1 \right] + \dots + \left[ 1\times m_{r-1} + 1 \right]
$$

## 7.1 Recursive Definition
![Pasted image 20241113184426](../../attachments/Pasted%20image%2020241113184426.png)
* Note that for the last layer, we want it to be a useful representation for a particular task:
	* For classification tasks, you may apply a sigmoid function
	* For regression tasks, you may leave it as linear

# 8 Other Activation Functions
* The reason we may use Leaky ReLU or Swish is so that we have signal when the values are negative ⟶ ReLU has zero gradient for negative inputs.
![Pasted image 20241113184809](../../attachments/Pasted%20image%2020241113184809.png)

## 8.1 Why not Identity Function?
![Pasted image 20241113184827](../../attachments/Pasted%20image%2020241113184827.png)

# 9 Connection to Kernel Methods
* In kernel methods, we applied a feature map $\phi$ to our input $x$. We explicitly choose $\phi$, and we were constrained to models that were linear in $\theta$.
$$
\theta^T\phi(x)
$$
* Deep learning automatically **learns the correct feature map/representation**
	* Let $\beta$ be the params (except for the last layer)
	* We can then write out final hidden layer as
$$
a^{[r-1]}=\phi_{\beta}(x)
$$
	* And the hypothesis is:
$$
h_{\theta}(x)=W^{[r]}\phi_{\beta}(x) + b^{[r]}
$$
	* When $\beta$ is fixed:
		* $\phi_{\beta}(\cdot)$ can be viewed as a feature map
		* and $h_\theta(x)$ is a linear model over the features $\phi_\beta(x)$
	* In deep learning $\beta$ is not fixed, so we are simultaneously:
		* Learning the best feature map
		* Learning a good linear model on top of that feature map
## 9.1 Representations
* Note that the penultimate layer $a^{[r-1]}$ is often referred to as the learned features or representations.
	* E.g. this is effectively the embeddings ⟶ allows you to use this on other tasks.
