# Review: NN Learning
* We try to optimize the objective function:
$$
J^{(j)}(\theta) = (y^{(j)} - h_{\theta}(x^{(j)}))^2
$$
* where the hypothesis $h_\theta$ comes from a neural net.
* To optimize, we perform SGB with the following equation:
$$
\theta = \theta - \alpha \nabla_{\theta}J^{(j)}(\theta)
$$
* The focus today is **how to compute the gradient?**

## Differentiable Circuits/Network
* Suppose you are wanting to describe some complex computation.
* This can be written as a **composition of sequence of**:
	* **arithmetic operations** ⟶ +, -, x, /
	* **elementary functions** ⟶ cos, sin, exp, log, ReLU, sigmoid
* This allows us to model our computation as a chain of operations
	* Left side is input, each edge is applying an operation, right side is output.
* The size of the circuit is the total number of operations and elementary functions.
	* If we assume each operation takes $O(1)$ time, then computing the entire circuit (or function) takes $O(N)$ time.
![500](Pasted%20image%2020241114210149.png)

* If you have a computation is this way, we have:

> [!NOTE] Theorem (backprogation, informally states)
> Suppose we have a differentiable circuit of size $N$ (num of nodes/edges). This circuit computes a real valued function $f: \mathbb{R}^l \to \mathbb{R}$
>
> Then the gradient $\nabla f(x) \in \mathbb{R}^l$ can be computed in $O(N + l)$ time by a circuilt of size $O(N)$. We implicitly assume that $N \geq l$, so the time is actually $O(N)$.

* The alternate approach to compute derivatives is:
![Pasted image 20241114210447](Pasted%20image%2020241114210447.png)
* We can approximate the gradient in $l + 1$ evaluations of $f$. It takes $O(N)$ time to evaluate the function, so this is $O(nl)$ time.
* The theorem above states that we can compute it in $O(N + l)$ time.

* If we suppose $f$ is the loss function, then the theorem states that the gradient can be computed in $O(N)$ time.
![Pasted image 20241114210945](Pasted%20image%2020241114210945.png)


> [!NOTE] Corollay
> For all vectors $v \in \mathbb{R}^l$:
> $$
> \left(\nabla^2 f(x) \right)^T  v
> $$
> can be computed in $O(N + l)$ time. Note that this is the inner product of the $\mathbb{R}^{l \times l}$ Hessian with the vector $v$.
>
> ![Pasted image 20241114211233](Pasted%20image%2020241114211233.png)
> ![Pasted image 20241114211338](Pasted%20image%2020241114211338.png)


# Prelim
## Chain Rule
![Pasted image 20241114211427](Pasted%20image%2020241114211427.png)


## Notation
![Pasted image 20241114211650](Pasted%20image%2020241114211650.png)

****
# Two-Layer NN
* In this NN, we pass in our inputs, then there is a hidden layer, then the output layer.
![Pasted image 20241114211822](Pasted%20image%2020241114211822.png)
* Our loss function for a single example is:
![400](Pasted%20image%2020241114211852.png)

## Lemma 1: Linear Combination
![400](Pasted%20image%2020241114212045.png)

* We show the first statement: let's consider a single element $W_{ij}$
	* $z_k$ is an entry of $z$ ⟶ which is $w_k u + b_{k}$ where $w_k$ is the $k$-th row of $W$.
![Pasted image 20241114212552](Pasted%20image%2020241114212552.png)

* We can apply this lemma to our NN:
![Pasted image 20241114212739](Pasted%20image%2020241114212739.png)

## Lemma 2: Activation Function
![400](Pasted%20image%2020241114212844.png)

* We can apply this lemma to our NN:
![400](Pasted%20image%2020241114212934.png)

## Lemma 3: Output Layer
![400](Pasted%20image%2020241114213022.png)

* Applying this our NN, we have:
![Pasted image 20241114213256](Pasted%20image%2020241114213256.png)
* where we know what $J(\tau)$ is ⟶ $J(\tau) = \frac{1}{2}(y - \tau)^2$

* We can also use our other lemmas here to compute stuff:
![500](Pasted%20image%2020241114213250.png)

# Deep NN
* The equations for each layer are below:
![400](Pasted%20image%2020241114213402.png)

### w.r.t W
* For an arbitrary layer, we can write it below:
	* First we compute $z^{[k]}$ in the formulation above.
	* Then we apply the rest of the neural net (abstract that as the function $J$) onto $z^{[k]}$
![Pasted image 20241114213530](Pasted%20image%2020241114213530.png)
* By Lemma 1, this is equal to:
![Pasted image 20241114215547](Pasted%20image%2020241114215547.png)

## w.r.t Z
* We know that $z^{[k]}$ is used to calculate $a^{[k]}$:
![Pasted image 20241114215658](Pasted%20image%2020241114215658.png)
* By Lemma 2, this means that:
![Pasted image 20241114220114](Pasted%20image%2020241114220114.png)

## w.r.t A
* We know that $a^{[k]}$ is used to calculate $z^{[k+1]}$
![Pasted image 20241114220000](Pasted%20image%2020241114220000.png)
* By Lemma 3, this means that:
![Pasted image 20241114220009](Pasted%20image%2020241114220009.png)

## Backprop Intuition
* Notice that to compute derivative w.r.t $z^{[k]}$, we need derivative w.r.t $a^{[k]}$, which needs derivative w.r.t $z^{[k+1]}$.
	* This suggests a **backwards** process for computing derivatives, alternating between the two steps in red.
![Pasted image 20241114220205](Pasted%20image%2020241114220205.png)

# Backprop Algorithm

## Forwards Pass
Apply the neural network and compute the values we need:
$$
z^{[1]}, a^{[1]}, z^{[2]}, a^{[2]}, \dots
$$
* Recall that the equations for each layer are:
$$
z^{[k]} = W^{[k]} a^{[k-1]} + b^{[k]}
$$
$$
a^{[k]} = \sigma(z^{[k]})
$$
where $\sigma$ is the activation function (in our case, ReLU).

* The number of operations required to compute the forwards pass is $O(N)$

## Backwards Pass
* Compute derivatives starting from the end back to the front.
![Pasted image 20241114220516](Pasted%20image%2020241114220516.png)
* We keep repeating this for each subsequent layer
![Pasted image 20241114220614](Pasted%20image%2020241114220614.png)
* Note that for each layer, you can use what you compute to find the **gradient with respect to the parameters:**
$$
\text{After obtaining } \frac{\partial J}{\partial z^{[k]}} \text{ and } \frac{\partial J}{\partial a^{[k]}}
$$
$$
\underbrace{\frac{\partial J}{\partial W^{[k]}}}_{\mathbb{R}^{n^{[k]} \times n^{[k-1]}}} = \underbrace{\frac{\partial J}{\partial z^{[k]}}}_{\mathbb{R}^{n^{[k]} \times 1}} \underbrace{(a^{[k-1]})^T}_{\mathbb{R}^{1 \times n^{[k-1]}}} \tag{Lemma 1}
$$
$$
\underbrace{\frac{\partial J}{\partial b^{[k]}}}_{\mathbb{R}^{n^{[k]} \times 1}} = \underbrace{\frac{\partial J}{\partial z^{[k]}}}_{\mathbb{R}^{n^{[k]} \times 1}} \tag{Lemma 1}
$$
* The number of operations requires to calculate the backwards pass is $O(N)$.
* We also computed the gradient of the loss function w.r.t. all of the model parameters and layers.

# Pictorial View
## Forwards Pass
* We had input $x$ (which is $a^{[0]}$), which was then:
	* Fed into matmul layer to produce $z^{[1]}$
		* Using parameters $W^{[1]}, b^{[1]}$
	* Through ReLU activation to produce $a^{[1]}$
	* Fed into matmul layer to produce $z^{[2]}$
		* Using parameters $W^{[2]}, b^{[2]}$
	* Through ReLU activation to produce $a^{[2]}$
	…
	* Fed into matmul layer to produce $z^{[r-1]}$
		* Using parameters $W^{[r-1]}, b^{[r-1]}$
	* Through ReLU activation to produce $a^{[r-1]}$
	* Fed into matmul layer to produce $z^{[r]}$ (which is $\tau$)
		* Using parameters $W^{[r]}, b^{[r]}$
	* Used to compute the loss $J(\tau)$
![Pasted image 20241114222157](Pasted%20image%2020241114222157.png)

## Backwards Pass
* We calculate the derivative of the loss function w.r.t. each layer's output:
	* We first calculate $\frac{\partial J}{\partial z^{[r]}}$
	* Then we calculate $\frac{\partial J}{\partial a^{[r-1]}}$
	* Then we calculate $\frac{\partial J}{\partial z^{[r-1]}}$
	* Then we calculate $\frac{\partial J}{\partial a^{[r-2]}}$
	* …
	* Then we calculate $\frac{\partial J}{\partial z^{[1]}}$
	* Then we calculate $\frac{\partial J}{\partial a^{[0]}} = \frac{\partial J}{\partial x}$
* After calculating $\frac{\partial J}{\partial z^{[k]}}$ and $\frac{\partial J}{\partial a^{[k]}}$ for all layers, we can calculate the gradient of the loss function w.r.t. the parameters:
	* $\frac{\partial J}{\partial W^{[k]}} = \frac{\partial J}{\partial z^{[k]}} (a^{[k-1]})^T$
	* $\frac{\partial J}{\partial b^{[k]}} = \frac{\partial J}{\partial z^{[k]}}$
![Pasted image 20241114222606](Pasted%20image%2020241114222606.png)

# Vanishing/Exploding Gradients
* Notice that in the backwards pass, we repeatedly apply matrix multiplications. This can lead to **vanishing/exploding gradients**.
* To understand why, look at how gradients flow backwards through layers:
	* For the output layer: $\frac{\partial J}{\partial z^{[r]}} = -(y-z^{[r]})$
	* For hidden layers: $\frac{\partial J}{\partial a^{[k]}} = (W^{[k+1]})^T \cdot \frac{\partial J}{\partial z^{[k+1]}}$
	* And: $\frac{\partial J}{\partial z^{[k]}} = \frac{\partial J}{\partial a^{[k]}} \odot \text{ReLU}'(z^{[k]})$
* As we chain these together through multiple layers, we get products of matrices and derivatives:
$$
\frac{\partial J}{\partial z^{[k-1]}} = ((W^{[k]})^T \cdot \frac{\partial J}{\partial z^{[k]}}) \odot \text{ReLU}'(z^{[k-1]})
$$
* Repeated multiplication of matrices can lead to either **vanishing** or **exploding** gradients.
* Example: if we use sigmoid, the gradient is near zero when values are largely positive or negative, making the vanishing gradient problem worse through multiplication

# Initializing Parameters
* We can't initialize all the weights to zero because then every neuron will pretty much learn in the same way (symmetric weights).

## Xavier Initialization
* Suppose our layer produces output via the following:
$$
o_i = \sum_j w_{ij} x_j
$$
* Now suppose the inputs and weights are Gaussian. Then the variance of the output is:
$$
\begin{align*}
\text{Var}(o_i) &= \sum_j \text{Var}(w_{ij}) \text{Var}(x_j) \\
&= n \text{Var}(w_{ij}) * \text{Var}(x_i)
\end{align*}
$$
* A common heuristic is to set
$$
\frac{1}{2} (n_{in} + n_{out}) \text{Var}(w_{ij}) = 1
$$
* This means that the variance of the weights is:
$$
\text{Var}(w_{ij}) = \frac{2}{n_{in} + n_{out}}
$$
* We then choose $w_{ij}$ IID from a Gaussian with mean 0 and standard deviation $\sqrt{\frac{2}{n_{in} + n_{out}}}$.
