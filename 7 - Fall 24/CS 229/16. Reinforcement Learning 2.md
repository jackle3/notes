# 1 Recap: MDP
* Recall that an MDP is formulated as:
	* $S$ is set of states
	* $A$ is set of actions
	* $P_{sa}$ is set of transition probs (if in state s and you take action a, what state $s'$ do you go to?)
	* $\gamma$ is discount factor
	* $R$ is the reward function
$$
\text{MDP}(S, A, \{P_{sa}\}, \gamma, R)
$$
* The algorithm needs to come up with a policy (in state s, what action a)
$$
\pi: S \to A
$$
* The expected total future payoff is the **value**:
$$
V^\pi(s) = \text{Expected total discounted future payoff of policy $\pi$ starting from $s$}
$$
* The best value is $V^*$:
$$
V^*(s) = \max_{\pi}V^\pi(s)
$$
* The **Bellman Equations** let us calculate $V^*$ and $V^\pi$
$$
\begin{align*}
V^\pi(s) &= R(s) + \gamma \sum_{s'}P_{s, a}(s') \cdot \max_{\pi}V^\pi(s')\\
V^*(s)&= \max_{\pi}V^\pi(s) \\
&= \max_{\pi}\left(R(s) + \gamma \sum_{s'}P_{s, \pi(s)}(s') \cdot V^\pi(s')\right)\\
&= R(s) + \max_{\pi} \left(\gamma \sum_{s'}P_{s, \pi(s)}(s') \cdot V^\pi(s')\right) \\
&= R(s) + \max_{a} \left(\gamma \sum_{s'}P_{s, a}(s') \cdot \max_{\pi}V^\pi(s')\right) \\
&= R(s) + \gamma \left(\max_{a} \sum_{s'}P_{s, a}(s') \cdot V^*(s')\right)
\end{align*}
$$
* The optimal policy is $\pi^*$ ⟶ greedy w.r.t $V^*$
	* If you know $V^*$, you can recover $\pi^*$
$$
\pi^*(s) = \arg\max_{a} \left( R(s) + \gamma  \sum_{s'}P_{s, a}(s') \cdot V^*(s') \right)
$$

## 1.1 Value Iteration
* In this algorithm, we **iteratively update the values** then read off the policy.
* We don't calculate the exact $V^*$. We iteratively loop to make $V$ closer and closer to $V^*$.
	* We usually stop the loop once $V$ has not changed much between iterations, meaning we got close enough to $V^*$.
	* Not guaranteed to converge to the optimum policy in finite time.
* In practice, value iteration is slightly better than policy iteration.
---
* Initialize $V \in \mathbb{R}^{|S|}$ to $V(s) = 0 \quad \forall s \in S$
* Loop:
	* $V := B(V)$
	* This means $\forall s, V(s) := R(s) + \gamma \left(\max_{a} \sum_{s'}P_{s, a}(s') \cdot V(s')\right)$
* $\pi^*(s) = \arg\max_{a} \left(\sum_{s'}P_{s, a}(s') \cdot V^*(s') \right)$
---
![Pasted image 20241121105253](../../attachments/Pasted%20image%2020241121105253.png)

## 1.2 Policy Iteration
* We **iteratively update the policy** each time.
* Steps are:
	* We start with a policy.
	* We calculate the value at every state with this policy.
	* We use this to update the policy.
* If $V^*$ and $\pi^*$ are stationary, then policy iteration finds $\pi^*$ and:
	* $V^* = V^{\pi^*}$
* This converges in **finite time** ⟶ you never repeat the same policy.
* If the algorithm reaches a stationary point, that is the optimum solution.
---
* Initialize policy $\pi$
* Loop:
	* $V = V^\pi$ (calculate $V^\pi$ by solving linear system w/ Bellman Equation)
	* $\pi(s) = \arg\max_{a} \left(\sum_{s'}P_{s, a}(s') \cdot V(s') \right)$
---
![Pasted image 20241121105321](../../attachments/Pasted%20image%2020241121105321.png)

# 2 Reinforcement Learning
## 2.1 Known Dynamics and Rewards
* Our previous algorithms (value/policy iteration) have **known dynamics and rewards**
	* This is not true in many settings ⟶ e.g. robotics where you don't know the environment
* **Key Assumption**: Given $s, a$, we can sample $s' \sim P_{s, a}$
* There are two classes of algorithms:

### 2.1.1 Model-based RL
* **Explicitly** learn the transition dynamics and rewards
	* Initialize dynamics and rewards to some value (e.g. uniform)
	* Loop:
		* Use value/policy iteration to find optimal policy given current dynamics.
		* Let the robot run until failure.
		* When it fails we update our dynamics.

### 2.1.2 Model-free RL
* **Don't learn** transition dynamics and rewards
	* Policy gradient (REINFORCE)
	* Q-learning

## 2.2 Tabular Vs Continuous
### 2.2.1 Tabular Case
* This is what we've been talking about so far.
* Discrete state space with finite $S$ and finite $A$

### 2.2.2 Continuous Case
* Non-finite state space $S \in \mathbb{R}^d$
* Action space can be discrete or continuous $A \in \mathbb{R}^k$
* Examples:
	* To design a self-driving car, your model is:
		* $\text{Car}(x, y, \theta, v_x, v_y, \omega)$
	* For the inverted pendulum, your model is:
		* $\text{Pendulum}(x, \theta, v_x, \omega)$
	* For Andrew Ng's helicopter, your model is:
		* $\text{Helicopter}(x, y, z, \text{Roll}, \text{Pitch}, \text{Yaw}, \text{…(derivatives)})$
* One approach to discretize is to use a grid ⟶ $O(|S|^d)$
	* e.g. if space is two-dimensional, space is $O(|S|^2)$

## 2.3 Learning Transition Dynamics
### 2.3.1 Q1: How Do We learn/represent the Transition Dynamics $P_{s, a}$?
* A1: Physics simulator that gives you dynamics
	![200](../../attachments/Pasted%20image%2020241121112933.png)
* A2: Learn a model from data
	* Represent dynamics $P_{s, a}$ in some parametric way
	* Model $s' \sim P_{s, a}$ by $s' = f(s, a) + \epsilon$
		* $f$ is some deterministic dynamics model, $\epsilon$ is noise

#### 2.3.1.1 Linear System
* E.g. suppose we model our dynamics by a **linear** system:
	* $s \in \mathbb{R}^d$ and $a \in \mathbb{R}^k$
	* $f(s, a) = A \cdot s + B \cdot a$
	* $s' = A \cdot s + B \cdot a + \epsilon$
		* $A \in \mathbb{R}^{d \times d}$ and $B \in \mathbb{R}^{d \times k}$ are parameters to learn
		* $\epsilon \in \mathbb{R}^d$ is Gaussian noise
	* To learn, suppose we treat trajectories as supervised learning:
		![500](../../attachments/Pasted%20image%2020241121113348.png)
		* We can learn $A$ and $B$ by linear regression on this data.
			* $s^{(i)}_{t+1}$ is the label to predict
			* $A s^{(i)}_t + B a^{(i)}_t$ is our prediction
$$
\begin{align*}
A, B = \arg\min_{A, B} \sum_{i=1}^N \sum_{t=0}^{T-1} \left\| s^{(i)}_{t+1} - (A  s^{(i)}_t + B  a^{(i)}_t) \right\|^2_2
\end{align*}
$$

#### 2.3.1.2 Non-linear System
* E.g. suppose we model our dynamics by a **non-linear system**:
	* $s' = f_{\theta}(s, a) + \epsilon$
	* $f_{\theta}$ is some deterministic dynamics model parameterized by $\theta$
		* E.g. $f_{\theta}(s, a) = A \phi_{1}(s) + B \phi_{2}(a)$ where $\phi$ is some feature map
		* E.g. $f_{\theta}(s, a)$ is a neural network
	* We can learn $\theta$ by supervised learning on this data.
$$
\theta = \arg\min_{\theta} \sum_{i=1}^N \sum_{t=0}^{T-1} \left\| s^{(i)}_{t+1} - f_{\theta}(s^{(i)}_t, a^{(i)}_t) \right\|^2_2
$$

## 2.4 Value Iteration and Policy Iteration with Continuous State Spaces
### 2.4.1 Q2: How Do We Perform Value Iteration (VI) or Policy Iteration (PI) with Continuous State Spaces?
* Represent the value function $V: \mathbb{R}^d \to \mathbb{R}$.
	* This can be a linear function: $V(s) = \theta^T \phi(s)$.
	* Alternatively, it can be a neural network: $V(s) = f_{\theta}(s)$.
* In the discrete state space scenario, the value function is given by:
$$
\begin{align*}
V(s) &= R(s) + \gamma \max_{a} \sum_{s'} P_{s, a}(s') \cdot V(s') \\
&= R(s) + \gamma \max_{a} \mathbb{E}_{s' \sim P_{s, a}} \left[ V(s') \right] \\
&= \max_{a} \left( R(s) + \gamma \mathbb{E}_{s' \sim P_{s, a}} \left[ V(s') \right] \right)
\end{align*}
$$
* Assume the action space $A$ is discrete while the state space $S$ is continuous.

### 2.4.2 Solution: Fitted Value Iteration
* Evaluate all the actions and take the maximum.
1. Enforce the Bellman Equation on randomly selected states:
	* Iteratively:
		* Randomly sample states $s^{(1)}, s^{(2)}, \ldots, s^{(n)}$.
		* Estimate the right-hand side of the Bellman Equation:
$$
\frac{1}{n} \sum_{i=1}^n \left( R(s^{(i)}) + \gamma \max_{a} \mathbb{E}_{s' \sim P_{s^{(i)}, a}} \left[ V(s') \right] \right)
$$
		* For each action $a \in A$, sample $s_1', s_2', \ldots, s_m' \sim P_{s^{(i)}, a}$ and compute:
			* The sum provides an estimate for the expectation:
$$
q(a) = R(s) + \frac{\gamma}{k} \sum_{j=1}^k V(s_j')
$$
		* Set the target value:
$$
y^{(i)} = \max_{a} q(a)
$$
2. Ensure that $V(s^{(i)})$ approximates $y^{(i)}$:
$$
\theta = \arg\min_{\theta} \frac{1}{2}\sum_{i=1}^n \left( y^{(i)} - \theta^T \phi(s^{(i)}) \right)^2
$$

### 2.4.3 Estimating Policy
* This gives you an optimal value function $V^*$. You can now read off the policy.
$$
\begin{align*}
\pi^*(s) &= \arg\max_{a} \left(\sum_{s'}P_{s, a}(s') \cdot V^*(s') \right) \\
&= \arg\max_{a} \mathbb{E}_{s' \sim P_{s, a}} \left[ V^*(s') \right]
\end{align*}
$$
* To calculate this, there are two ways:
	1. Estimate expectation by sampling ⟶ $\frac{1}{k} \sum_{j=1}^k V(s_j')$ where $s_j' \sim P_{s, a}$
	2. $\mathbb{E}\left[V^*(s')\right] = \mathbb{E}\left[V^*(f(s, a) + \xi)\right] \approx V^*(f(s, a))$

### 2.4.4 Continuous Actions
$$
y^{(i)} = \max_{a} \left( R(s^{(i)}) + \gamma \mathbb{E}_{s' \sim P_{s^{(i)}, a}} \left[ V^*(f(s^{(i)}, a)) \right] \right)
$$
* Calculate this via gradient descent over actions

## 2.5 Exploration Vs Exploitation
* Exploration:
	* Trying out actions to find out more about the environment.
	* Collect data that is different from what we have seen before.
* Exploitation:
	* Finding the best policy given current information.
	* Taking actions that are known to be good.
* We need to balance exploration and exploitation.
![Pasted image 20241121112029](../../attachments/Pasted%20image%2020241121112029.png)
* People often apply an $\epsilon$-greedy policy to balance:
	* $\epsilon$ probability of exploring (random action)
	* $1 - \epsilon$ probability of exploiting (follow current best/greedy action)

## 2.6 Model-based RL
### 2.6.1 Calculating Dynamics and Rewards
* Suppose we have a few trajectories
![400](../../attachments/Pasted%20image%2020241121110922.png)

* To calculate our dynamics, we loop over all trajectories and count
$$
P_{s,a}(s') = \frac{\text{\# of times we got to $s'$ after taking action $a$ in $s$}}{\text{\# times we took action $a$ in $s$}}
$$
* To calculate our rewards, suppose rewards can be $-1$ or $1$.
	* The reward is the average of the rewards we got in $s$.
$$
R(s) = \frac{\left(-1 * \text{\# of times we got $-1$ in $s$}\right) + \left(1 * \text{\# of times we got $1$ in $s$}\right)}{\text{\# of times we visited $s$}}
$$
* We usually **do not** do Laplace smoothing.
	* Instead, we replace any $0/0$ with $1/|S|$ then normalize.

```python
while not converged:
   # Act in the environment
   - Choose actions using current value function (current policy)
   - Keep track of counts for state transitions and rewards
   
   # When failure occurs
   - Update the model (dynamics and rewards) based on observed data
   - Run value iteration to find optimal value function for current model
```

### 2.6.2 Algorithm
* Initialize $\pi$ randomly.
* Initialize dataset of trajectories $D$ initially empty.
* Loop:
	* Execute $\pi$ in real environment and add trajectory to $D$.
	* Estimate $P_{s, a}$ and $R(s)$ using $D$ (using counting equations from above)
	* Run PI or VI on the model to get $V^*$ for estimated $P_{s, a}$ and $R(s)$
	* Update $\pi$ to $\pi^*$ (greedy w.r.t. $V^*$)
$$
\pi^*(s) = \arg\max_{a} \left(\sum_{s'}P_{s, a}(s') \cdot V^*(s') \right)
$$

# 3 On-policy Vs Off-policy RL
* Off-policy:
	* Learn the optimal policy using data collected from a different policy.
	* Q-learning is a key example:
		* Learn the value of the optimal policy for each state-action pair, denoted as $Q^*(s, a)$.
	* Policy Gradient:
		* Use a parameterized representation of the policy.
		* Represent, for every state, a probability distribution over possible actions.
		* Use a function to evaluate how good the policy is.
		* Run an update process to adjust the parameters $\theta$ to optimize $J(\theta)$.
		* Utilize the policy gradient theorem, which provides a convenient form for this gradient.
