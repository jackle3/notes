
# K-means
* Suppose we have a bunch of data with no structures. We run K-means to divide into two clusters.
![[Pasted image 20241025165442.png|350]]

* It initially **guesses two random points as cluster centroids**.
![[Pasted image 20241025165510.png|350]]

* It puts places points into classes based on **distance from the centroids.**
![[Pasted image 20241025165550.png|350]]

* It then moves the **centroids to the mean of their sets**.
![[Pasted image 20241025165630.png|350]]

* It then **reassigns the cluster assignments** based on the new centroids.
![[Pasted image 20241025165658.png|350]]

* It the computes the cluster centers, and repeats until convergence.
![[Pasted image 20241025165715.png|350]]

# EM Algorithm for GMM
* In the example below:
	* You pick three clusters and assign them weights of $p = 0.333$
		* This algorithm **depends on the initialization** (can be random). Once initialized, the algorithm is deterministic.
	* You then do the E-step to guess the values of $z^{(i)}$ ⟶ this is shown in the circles
	* You then do the M-step to move the cluster centers.
	* The ellipses show what the covariance matrices look like.

* In the initial position
![[Pasted image 20241027023940.png|350]]

* After the first iteration
![[Pasted image 20241027023951.png|350]]

* After the second iteration ⟶ at this point green points are more or less convinced.
![[Pasted image 20241027024008.png|350]]

* After the sixth iteration ⟶ at this point the blue points are converging
![[Pasted image 20241027024050.png|350]]

* After the 20th iteration ⟶ pretty much everyone has converged
![[Pasted image 20241027024122.png|350]]
