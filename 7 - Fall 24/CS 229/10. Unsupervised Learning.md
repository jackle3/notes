# Unsupervised Learning
* Supervised learning is when we have both data and labels. Unsupervised is when we don't have labels. Generally, this means doing clustering or other tasks ⟶ find structure in data.
![[Pasted image 20241029194651.png]]


# K-means

## Algorithm
* The inputs are a dataset $\{x^{(i)}, \dots, x^{(n)}\}$
* Initialize the cluster centroids $\mu_{1}, \dots, \mu_{k}$ "randomly"
![[Pasted image 20241029195839.png]]

## Visualization

| 1. Suppose we have a bunch of data with no structures. We run K-means to divide into two clusters.<br>![[Pasted image 20241025165442.png\|350]] | 2. It initially **guesses two random points as cluster centroids**.![[Pasted image 20241025165510.png\|350]]        |
| ----------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------- |
| 3. It puts points into classes based on **distance from the centroids.**<br>![[Pasted image 20241025165550.png\|350]]                           | 4. It then moves the **centroids to the mean of their sets**.<br>![[Pasted image 20241025165630.png\|350]]          |
| 5. It then **reassigns the cluster assignments** based on the new centroids.<br>![[Pasted image 20241025165658.png\|350]]                       | 6. It the computes the cluster centers, and repeats until convergence.<br>![[Pasted image 20241025165715.png\|350]] |

## Picking Initial Cluster Centers
* In practice, K-means++ is commonly used in order to pick initializations.
	* Goal is to pick datapoints that are spread out in the space to act as clusters.
	* Steps are:
		* Pick random point as first centroid
		* For each subsequent centroid:
			* Compute for all points the distance to nearest existing centroid
			* Sample points proportional to the distance^2
				* i.e. points with larger distances are more likely to be picked.
				* i.e. this picks points far away from centroids that are already picked.

## Convergence
* **Convergence:** K-means is guaranteed to converge to local minima but not global minima ⟶ it depends on where the clusters are initialized.
![[Pasted image 20241029195359.png]]
* $J$ measures the sum of squared distances between each example $x$ and the centroid to which it has been assigned.
	* If you fix $\mu$, then the first step minimizes the assignments $c$
	* If you fix $c$, then the second step minimizes the centroids $\mu$.
	* This means that $J$ **must monotoically decreases**, and that it must converge.
* The distortion function will eventually converge to some value, though might not be global:
	* $J$ is not convex, so coordinate descent might converge to local minimas.
* To find the global minima:
	* Run k-means many times (using different random initial values for the cluster centroids $\mu$).
	* Then, out of all the different clusterings, pick the one that gives the lowest distortion $J(c, \mu)$

# Gaussian Mixture Models
* K-means is good when clusters are well defined and separated. However, what is they are not?
![[Pasted image 20241029200002.png]]
* Furthermore, K-means predict circular clusters.
![[Pasted image 20241029200215.png|500]]

## Gaussians in M Dimensions
![[Pasted image 20241029200239.png]]
![[Pasted image 20241029200252.png]]

## Density Estimation
* GMMs are generally used to perform density estimation (e.g. anomaly detection)
![[Pasted image 20241029200021.png]]
![[Pasted image 20241029200029.png]]
* Given a piece of data, GMMs try to model it by taking the superposition of $k$ Gaussians.
![[Pasted image 20241029200037.png]]

## Mixture of Gaussians
* We essentially want to use GMM to answer: what mixture of Gaussians would have generated this data?
![[Pasted image 20241029200409.png]]

## GMM Assumption
![[Pasted image 20241029200452.png]]

## Summary of GMM Components
* In the image below, $\pi_k$ is the same as $\phi_k$ ⟶ the prob of sampling from $k$-th Gaussian.
![[Pasted image 20241029200526.png]]

# EM Algorithm for GMM
* We wish to model the data by specifying a joint distribution
$$
p(x^{(i)}, z^{(i)}) = p(x^{(i)} | z^{(i)})p(z^{(i)})
$$
* Where:
	* $z^{(i)} \sim \text{Multinomial}(\phi)$ where $\phi_k \geq 0$ and $\sum_{j=1}^K \phi_{k} = 1$ and $p(z^{(i)} = k) = \phi_k$
	* $(x^{(i)} | z^{(i)} = k) \sim \mathcal{N}(\mu_{k}, \Sigma_{k})$
* This model posits that each $x^{(i)}$ is generated by randomly choosing $z^{(i)} \in \{1, \dots K\}$ and then $x^{(i)}$ was sampled from one of $K$ Gaussians.
* The $z^{(i)}$ are known as **latent** random variables, meaning they are hidden. They represent: what Gaussian $z^{(i)} \in \{1, \dots, k\}$ is the point $x^{(i)}$ sampled from?

* This is similar to K-means (two steps). The difference is that:
	* K-means had a hard cluster assignment ⟶ each point assigned to one cluster.
		* A single best guess ($c^{(i)}$ as taking values in $\{0, 1\}$ or $\{1, \dots, k\}$) for the values of $z^{(i)}$.
	* EM has a soft cluster assignment ⟶ each point has a distribution for what cluster it belongs to.
		* Our guesses being probabilities $w_j^{i}$ and taking values in [0, 1]
* If you know $z^{(i)}$, this is becomes very similar to GDA:
	* except that $z^{(i)}$ plays the role of class labels and we use different $\Sigma$ for each.
	* In GDA you use $p(y | x) \propto p(x | y)p(y)$
	* Here you would use $p(x, z) = p(x | z)p(z)$

* **Key Idea:** EM is useful when the $z^{(i)}$ is not known.
	* E-step ⟶ it tries to “guess” the values of the $z^{(i)}$'s
		* Guess assignment of points to classes
		* In standard (“soft”) EM: each point associated with prob. of being in each class
	* M-step ⟶ it updates the parameters of our model based on our guesses.
		* Since in the M-step we are pretending that the guesses in the first part were correct, the maximization becomes easy (very similar to GDA)
## E-step
* We come up with best guesses for the value of $z^{(i)}$.
* For each $i, j$ we set:
$$
w_j^{(i)} := p(z^{(i)} = j | x^{(i)}; \phi, \mu, \Sigma)
$$
* In the E-step, we calculate the posterior probability of our parameters $z^{(i)}$'s, given the $x^{(i)}$ and using the current setting of our parameters.
* Using Bayes rule, we obtain:
$$
p(z^{(i)} = j | x^{(i)}; \phi, \mu, \Sigma) = \frac{p(x^{(i)} | z^{(i)} = j; \mu, \Sigma)p(z^{(i)} = j; \phi)}{\sum_{l=1}^k p(x^{(i)} | z^{(i)} = l; \mu, \Sigma)p(z^{(i)} = l; \phi)}
$$
* Here:
	* $p(x^{(i)} | z^{(i)} = j; \mu, \Sigma)$ is given by evaluating the PDF of a Gaussian with mean $\mu_j$ and covariance $\Sigma_j$ at $x^{(i)}$.
	* $p(z^{(i)} = j; \phi)$ is given by $\phi_j$, and so on.
	* The values $w_j^{(i)}$ calculated in the E-step represent our “soft” guesses
		* $w^{(i)}$ is a distribution over what $z^{(i)}$ could be.
![[Pasted image 20241029203807.png]]
## M-step
* Assume the guesses in the E-step (for the class labels $z^{(i)}$) were correct and maximize likelihood.
	* We update the parameters of the model using MLE
$$
\phi_j := \frac{1}{n} \sum_{i=1}^n w_j^{(i)}
$$
$$
\mu_j := \frac{\sum_{i=1}^n w_j^{(i)} x^{(i)}}{\sum_{i=1}^n w_j^{(i)}}
$$
$$
\Sigma_j := \frac{\sum_{i=1}^n w_j^{(i)} (x^{(i)} - \mu_j)(x^{(i)} - \mu_j)^T}{\sum_{i=1}^n w_j^{(i)}}
$$
* We use this in the E step to calculate the next $w_j$
## Visualization
* In the example below:
	* You pick three clusters and assign them weights of $p = 0.333$
		* This algorithm **depends on the initialization** (can be random). Once initialized, the algorithm is deterministic.
	* You then do the E-step to guess the values of $z^{(i)}$ ⟶ this is shown in the circles
		* This is effectively the probability of sampling $x^{(i)}$ from distribution $z^{(i)} \in \{1, \dots, K\}$
	* You then do the M-step to move the cluster centers.
	* The ellipses show what the covariance matrices look like.

| 1. In the initial position<br>![[Pasted image 20241027023940.png\|350]]                                                             | 2. After the first iteration<br>![[Pasted image 20241027023951.png\|350]]                                                |
| ----------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| 3. After the second iteration ⟶ at this point green points are more or less convinced.<br>![[Pasted image 20241027024008.png\|350]] | 4. After the sixth iteration ⟶ at this point the blue points are converging<br>![[Pasted image 20241027024050.png\|350]] |
| 5. After the 20th iteration ⟶ pretty much everyone has converged<br>![[Pasted image 20241027024122.png\|350]]                       |                                                                                                                          |

## Convergence
* It is guaranteed to converge to local optima, but not necessarily global optima.
	* Might be a good idea to run multiple times with different initial parameters.

### Jensen's Inequality
* Recall that $f$ is a convex function:
	* if $f''(x) \geq 0$ (for all $x \in \mathbb{R}$)
	* if the Hessian $H$ is positive semi-definite ($H \geq 0$).
	* strictly convex if $f''(x) > 0$ for all $x$
	* strictly convex if the Hessian $H$ is positive definite ($H > 0$)
* Jensen’s inequality can then be stated as follows:
	* Let $f$ be a convex function, and let $X$ be a random variable. Then:
$$
\mathbb{E}[f(X)] \geq f(\mathbb{E}[X]).
$$
	* If $f$ is a concave function then:
$$
\mathbb{E}[f(X)] \leq f(\mathbb{E}[X]).
$$
	* If $f$ is strictly convex, then $\mathbb{E}[f(X)] = f(\mathbb{E}[X])$ iff $X = \mathbb{E}[X]$ with prob 1 (i.e., if $X$ is a constant).
![[Pasted image 20241029204808.png]]

# General EM Algorithms
* Suppose we have an estimation problem in which we have a training set $\{x^{(1)}, \dots, x^{(n)}\}$ consisting of $n$ independent examples.
* We have a latent variable model $p(x, z; \theta)$ with $z$ being the latent variable (which for simplicity is assumed to take a finite number of values).
* The density for $x$ can be obtained by marginalizing over the latent variable $z$:
$$
p(x; \theta) = \sum_z p(x, z; \theta)
$$
* We wish to fit the parameters $\theta$ by maximizing the log-likelihood of the data, defined by
$$
\ell(\theta) = \sum_{i=1}^n \log p(x^{(i)}; \theta)
$$
* We can rewrite the objective in terms of the joint density $p(x, z; \theta)$ by
$$
\ell(\theta) = \sum_{i=1}^n \log p(x^{(i)}; \theta) = \sum_{i=1}^n \log \sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \theta)
$$
* In such a setting, the EM algorithm gives an efficient method for maximum likelihood estimation.
	* Maximizing $\ell(\theta)$ explicitly might be difficult because its not convex.
	* Our strategy will be to instead repeatedly construct a lower bound on $\ell$ (E-step), and then optimize that lower bound (M-step).
* Focusing on a single term $\log p(x; \theta)$, which can be rewritten as
$$
\log p(x; \theta) = \log \sum_z p(x, z; \theta)
$$
* Let $Q$ be a distribution over the possible values of $z$, such that $\sum_z Q(z) = 1$ and $Q(z) \geq 0$. We can add $Q/Q$:
$$
\log p(x; \theta) = \log \sum_z Q(z) \cdot \frac{ p(x, z; \theta)}{Q(z)}
$$
* Since $f = \log$ is concave, using Jensen's Inequality, we have:
$$
\log \left( \mathbb{E}_{z \sim Q} \left[ \frac{p(x, z; \theta)}{Q(z)} \right] \right) \geq \mathbb{E}_{z \sim Q} \left[ \log \left( \frac{p(x, z; \theta)}{Q(z)} \right) \right]
$$
$$
\log p(x; \theta) = \log \sum_z Q(z) \cdot \frac{ p(x, z; \theta)}{Q(z)} \geq \sum_z Q(z) \cdot \log \frac{p(x, z; \theta)}{Q(z)} \tag{11.7}
$$
* Now, for any distribution $Q$, the formula (11.7) gives a lower-bound on $\log p(x; \theta)$.
	* We will make the lower-bound tight at that value of $\theta$. I.e., we will make the inequality above hold with equality at our particular value of $\theta$.
* To make the bound tight for a particular value of $\theta$, we need for the step involving Jensen's inequality in our derivation above to hold with equality. For this to be true, we know it is sufficient that the expectation be taken over a "constant"-valued random variable. I.e., we require that
$$
\frac{p(x, z; \theta)}{Q(z)} = c
$$
for some constant $c$ that does not depend on $z$. This is easily accomplished by choosing
$$
Q(z) \propto p(x, z; \theta).
$$
* Actually, since we know $\sum_z Q(z) = 1$ (because it is a distribution), this further tells us that
$$
\begin{align*} Q(z) &= \frac{p(x, z; \theta)}{\sum_z p(x, z; \theta)} \ &= \frac{p(x, z; \theta)}{p(x; \theta)} \ &= p(z|x; \theta) \end{align*}
$$
* Thus, we simply set the $Q$'s to be the posterior distribution of the $z$'s given $x$ and the setting of the parameters $\theta$.
* Indeed, we can directly verify that when $Q(z) = p(z|x; \theta)$, then equation (11.7) is an equality because
$$
\begin{align*} \sum_z Q(z) \log \frac{p(x, z; \theta)}{Q(z)} &= \sum_z p(z|x; \theta) \log \frac{p(x, z; \theta)}{p(z|x; \theta)} \\ &= \sum_z p(z|x; \theta) \log \frac{p(z|x; \theta)p(x; \theta)}{p(z|x; \theta)} \\ &= \sum_z p(z|x; \theta) \log p(x; \theta) \\ &= \log p(x; \theta) \sum_z p(z|x; \theta) \\ &= \log p(x; \theta) \quad \text{(because } \sum_z p(z|x; \theta) = 1\text{)} \end{align*}
$$
* We call this expression the **evidence lower bound (ELBO)**. If we sum the above over all examples:
$$
\ell(\theta) =  \sum_{i=1}^n \log p(x^{(i)}; \theta)\geq \sum_i \text{ELBO}(x^{(i)}; Q_i, \theta) = \sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})}
$$
* In the E-step, we set $Q_i(z^{(i)}) = p(z^{(i)} | x^{(i)}; \theta)$, and in the M-step, we maximize the ELBO with respect to $\theta$.

## Algorithm
* Repeat until convergence:
	1. E-step: For each $i$, set
$$
   Q_i(z^{(i)}) := p(z^{(i)} | x^{(i)}; \theta).
   
$$
	1. M-step: Set
$$
   \theta := \arg \max_\theta \sum_{i=1}^n \text{ELBO}(x^{(i)}; Q_i, \theta) = \arg \max_\theta \sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})}.
$$
* By iterating, EM ensures the likelihood increases monotonically:
$$
\ell(\theta^{(t+1)}) \geq \ell(\theta^{(t)}).
$$
