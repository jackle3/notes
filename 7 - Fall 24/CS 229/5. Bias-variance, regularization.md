
![[Pasted image 20241014120505.png]]


# High Variance
* In the high variance case, you tend to overfit to small datasets.
	* As $n$ grows, training error **increases** because it can't fit every data point as well.
	* As $n$ grows, test error **decreases** because your model generalizes better. This suggests that larger training sets will help performance.
* When you have high variance, **test error is much higher than training error.**
	* Large gap between test and training error.
![[Pasted image 20241014122052.png|700]]
* The desired performance might be something like human level performance. Suggests that if you **extrapolate further, test error will eventually reach desired**.

# High Bias
* In the high bias case, you tend to underfit.
* When you have high bias, **train error is almost the same as test error**, with both high.
![[Pasted image 20241014121737.png|500]]

