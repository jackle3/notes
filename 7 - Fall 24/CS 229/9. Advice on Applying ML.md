 
# ML Cycle
* Recall that a machine learning system involves both **writing code** and training on **data**.
	* You control the code but you don't know the data ⟶ makes this progress more iterative as you discover what works for your data.

* ML Cycle ⟶ the number of times you can go through this cycle often defines how well your model performs.

| 1. Start with an Idea ⟶ you get data and code.<br><br>2. Run the code to Experiment (this is usually the bottleneck)<br><br>3. Analyze your experiments to figure out improvements. | ![400](Pasted%20image%2020241023134234.png%5C) |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------- |

# Analysis
* Today, we'll focus on the analysis part of the ML Cycle
* Key ideas:
	1. Diagnostics for debugging learning algorithms.
	2. Error analysis and ablative analysis.
	3. How to get started on a machine learning problem.
		1. Premature (statistical) optimization.

# Debugging Learning Algorithms
![Pasted image 20241023134703](../../attachments/Pasted%20image%2020241023134703.png)

## Fixing the Learning Algorithm
* The first approach is to arbitrarily change the model (i.e. change code or data).
	* While this might work, it's very time-consuming ⟶ largely **a matter of luck** whether you end up fixing what the problem really is because your choices of algorithm improvements are guesswork.
![Pasted image 20241023134910](../../attachments/Pasted%20image%2020241023134910.png)

## Diagnostic for Bias vs. Variance
* You can make informed decisions by looking at the bias and variance!
![Pasted image 20241023135003](../../attachments/Pasted%20image%2020241023135003.png)

* If you have high variance (overfitting), the train error will be much lower than test error.
	* As training set size increases, **train error increases** because it's harder to fit the entire set.
	* **Test error still decreasing** as $n$ increases. Suggests larger training set will help.
	* Large gap between training and test error.
![Pasted image 20241023135111](../../attachments/Pasted%20image%2020241023135111.png)

* If you have only high bias (underfitting), both train and test error will be bad.
	* Notice that the **test error is no longer decreasing** as $n$ increases.
	* Small gap between training and test error.
![Pasted image 20241023135224](../../attachments/Pasted%20image%2020241023135224.png)

* **In practice:** you look at the largest training set size you have, and compare the test error, train error, and the desired performance.

* If you have **both** high bias and high variance.
	* Collecting more data will reduce the variance (bring test error closer to train error), but it won't improve train error.
![Pasted image 20241023135634](../../attachments/Pasted%20image%2020241023135634.png)

### Revisiting the Fixes
![Pasted image 20241023135922](../../attachments/Pasted%20image%2020241023135922.png)
* Hypothesis space
![Pasted image 20241029220838](../../attachments/Pasted%20image%2020241029220838.png)

## Diagnostics for Optimization

**Is the algorithm (i.e. gradient descent for logistic regression) converging?**
![Pasted image 20241023140140](../../attachments/Pasted%20image%2020241023140140.png)

**Are you optimizing the right function?**
* For example, the equation below is what you care about
	* You care much more about ensuring real emails are not marked as spam.
	* The $w^{(i)}$ are kind of the weights for false positives vs false negatives.
	* The weighted accuracy of $\theta$ is sum of weights where you got the label right
		* Gives more weight to getting good emails right, less to getting spam emails right.
	![Pasted image 20241023140233](../../attachments/Pasted%20image%2020241023140233.png)

* Since $a(\theta)$ is not really optimizable, we try to optimize $J(\theta)$ as a **proxy**.
	![Pasted image 20241023140554](../../attachments/Pasted%20image%2020241023140554.png)

* Suppose your friend tries an SVM and finds that it outperforms you.
	* The parameters learned by SVM outperforms those of BLR
![Pasted image 20241023140922](../../attachments/Pasted%20image%2020241023140922.png)
* Your hypotheses are: maybe my objective function $J(\theta)$ is incorrect, or maybe my optimization algorithm is incorrect (e.g. I just need to run gradient descent for longer).

* To diagnose this, you should test whether $J(\theta_{SVM}) > J(\theta_{BLR})$:
![Pasted image 20241023141002](../../attachments/Pasted%20image%2020241023141002.png)

### Revisiting the Fixes
* Note that changing $\lambda$ changes $J(\theta)$ ⟶ changing your optimization objective.
![Pasted image 20241023141347](../../attachments/Pasted%20image%2020241023141347.png)

# Error Analysis
* For tasks that humans can do, **manually examining the data** can gain significant insights.
	* Look at the devset examples that your algorithm is doing poorly on to find problems/patterns.
	* Then categorize the examples into a spreadsheet (like below) to see what causes errors.
	* Allows you to be informed and focus on buckets that cause errors.
![Pasted image 20241023141456](../../attachments/Pasted%20image%2020241023141456.png)

## Error Analysis on Pipelines
* Since there a lot of steps, each step can be improved individually. Where do you focus?
![Pasted image 20241023142533](../../attachments/Pasted%20image%2020241023142533.png)
* To perform error analysis, we ask: **how much error is attributable to each of the components?**
	* To do so, **plug in the ground truth (from the development set) for each component** one at a time and see how the final accuracy changes.

| If we give the model perfect background removal, accuracy only goes up to 85.1%<br><br>Adding on perfect face detection, it goes up to 91%<br><br>Adding on perfect eye seg, it goes up to 95%<br><br>Adding on perfect nose seg, it goes up to 96%<br><br>Adding on perfect mouth seg, it goes up to 97%<br><br>Finally, giving perfect outputs gives us 100% | ![500](Pasted%20image%2020241023143057.png%5C) |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------- |
|                                                                                                                                                                                                                                                                                                                                                                |                                           |
* **Conclusion**: Most room for improvement in face detection and eyes segmentation.
	* If we add perfect face detection, accuracy increases by 6% from perfect preprocessing.
	* If we add perfect eye segmentation, accuracy increases another 4%.

* Adding perfect mouth segmentation only brought accuracy up 1% from perfect nose segmentation.
	* Indicates the ceiling of mouth segmentation work is 1% improvement in accuracy.

* Note that this analysis **has ordering** ⟶ the order of perfecting components matters, but likely will give similar results.

# Ablative Analysis
* **Error** analysis tries to explain the **difference between current** performance and **perfect** performance.
* **Ablative** analysis tries to explain the **difference between some baseline** (much poorer) performance and **current** performance.
* E.g., Suppose that you’ve build a good anti-spam classifier by adding lots of clever features to logistic regression:
	* Spelling correction.
	* Sender host features.
	* Email header features.
	* Email text parser features.
	* Javascript parser.
	* Features from embedded images.
* Question: How much did each of these components really help?
![Pasted image 20241023143633](../../attachments/Pasted%20image%2020241023143633.png)
* **Conclusion:** The email text parser features account for most of the improvement.
	* When we took it away, performance went from 98.9% to 95%.

# Getting Started on a Problem
**Approach #1: Careful design.**
* Spend a long term designing exactly the right features, collecting the right dataset, and designing the right algorithmic architecture.
* Implement it and hope it works.
* Benefit: Nicer, perhaps more scalable algorithms. May come up with new, elegant, learning algorithms; contribute to basic research in machine learning.

**Approach #2: Build-and-fix.**
* Implement something quick-and-dirty.
* Run error analyses and diagnostics to see what’s wrong with it, and fix its errors.
* Benefit: Will often get your application problem working more quickly. Faster time to market.

## Premature Statistical Optimization
* This occurs when you prematurely optimize things, thereby building a system that is **too complex.**
![Pasted image 20241023143828](../../attachments/Pasted%20image%2020241023143828.png)
![Pasted image 20241023143857](../../attachments/Pasted%20image%2020241023143857.png)

# Summary
* Time spent coming up with diagnostics for learning algorithms is time well spent.
	* It’s often up to your own ingenuity to come up with right diagnostics.
* Error analyses and ablative analyses also give insight into the problem.
* Two approaches to applying learning algorithms:
	* Design very carefully, then implement.
		* Risk of premature (statistical) optimization.
	* Build a quick-and-dirty prototype, diagnose, and fix.
