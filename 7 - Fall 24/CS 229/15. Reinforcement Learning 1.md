
# RL Properties
**RL applies in situations where:**
* We have to make a **sequence of decisions** ⟶ decisions with long term effects

**For every decision:**
* We don't know the right answer (don't know the best possible action).
* None or little supervision

**In order to learn:**
* Learn from a designed reward function that models the real world scenario

**During the course of the program:**
* We can collect more data iteratively

# Markov Decision Processes
* Suppose we have a robot moving along a 1-D line.
![Pasted image 20241119134833](../../attachments/Pasted%20image%2020241119134833.png)

**The model has:**
* $S$: set of states ⟶ all possible configurations of the robot, $S = \{1, \dots,8\}$
* $A$: set of actions ⟶ in this case, $A = \{L, R\}$
* $P_{s,a}$: dynamics/transitions ⟶ probability distribution
In state $s \in S$, applying action $a \in A$, the distribution of the next state $s' \in S$ is given by:
$$
P_{s,a}(s') = Pr[s' | s, a]
$$
* $P_{s, a}$ is a distribution ⟶ $\sum_{s' \in S} P_{s, a}(s') = 1$
* $P_{s, a}(s')$ is the probability of moving to state $s'$ if we take action $a$ in state $s$.

**Example:**
* Suppose we are in state 7. The $L$ action succeeds with probability 0.9.
	* Model this as a probability distribution of taking $L$ action from state $7$.
$$
\begin{align*}
P_{7,L}(6) &= 0.9 \tag{moving left}\\
P_{7,L}(7) &= 0.1 \tag{staying still}\\
P_{7,L}(s') &= 0 \quad \forall s' \neq 6,7 \tag{all other states}\\
\end{align*}
$$
* Once you get to state 6, stay there.
	* We can model this by enumerating all actions.
$$
\begin{align*}
P_{6,L}(6) &= 1 \tag{left action does nothing}\\
P_{6,R}(6) &= 1 \tag{right action does nothing}\\
P_{6,a}(s') &= 0 \quad \forall a, s' \neq 6 \tag{all other states}\\
\end{align*}
$$

## Sequential Process
1. Let $S$ be the set of states and $A$ be the set of actions. We start at initial.
$$
s_{0}\in S \tag{initial state}
$$
2. **Algorithm** chooses $a_0 \in A$ as first action.
3. The **environment** decides next state.
$$
s_{1}\sim P_{s_{0}, a_{0}} \tag{choose next state from distribution}
$$
4. **Algorithm** chooses $a_1 \in A$ as second action.
5. The **environment** decides next state.
$$
s_{2}\sim P_{s_{1}, a_{1}} \tag{choose next state from distribution}
$$
## Reward Function
* The reward function is a real value quantity attached to every state.
	* When you enter state $s$, you get reward $R(s)$
$$
R: S\to\mathbb{R}
$$
* The goal of the **algorithm** is to design a strategy that maximizes the total reward.

* Reward could also be a function of **both state and action**.
	* When you leave state $s$ via action $a$, you get reward $R(s, a)$
$$
R:S, A \to \mathbb{R}
$$

**Example:**
* Suppose we want the algorithm to get to state 6 as quickly as possible:
$$
\begin{align*}
R(6) &= 1 \tag{reward for getting to 6}\\
R(s) &= -0.1 \quad \forall s \neq 6 \tag{penalize going to other states}
\end{align*}
$$
* Using the reward function allows us to get to state 6 by visiting as few other states as possible.

## Total Payoff
* **Trajectory/episode** is a sequence of state/action pairs denoting the decisions.
![Pasted image 20241119154013](../../attachments/Pasted%20image%2020241119154013.png)
* The **total payoff** of a trajectory is defined as the sum of rewards:
	* Notice that this sum is unbounded ⟶ can get to infinity.
$$
\text{TP} = R(s_{0}) + R(s_{1}) + R(s_{2}) + \dots
$$
* We usually use the **discounted total payoff** to upper bound it.
	* Let the discount factor be: $0 <\gamma < 1$
$$
\text{DTP} = R(s_{0}) + \gamma R(s_{1}) + \gamma^2 R(s_{2}) + \dots
$$
	* Suppose $R(s) \in [-M, M]$
	* Then the discounted total payoff is bounded by $\left[ \frac{-M}{1-\gamma} , \frac{M}{1-\gamma} \right]$
	* This implies that short term rewards are more valued than long term rewards.

## Expected Payoff
* **Infinite Horizon:** The MDP has no predetermined stopping time.
* **Finite Horizon:** The MDP stops at time $T$.
* Recall that $s_1, s_2, s_3, \dots$ are random variables.

* The **expected reward/payoff** in the infinite horizon is:
$$
\mathbb{E}[R(s_{0}) + \gamma R(s_{1}) + \gamma^2 R(s_{2}) + \dots]
$$
* The **expected reward/payoff** in the finite horizon is:
$$
\mathbb{E}[R(s_{0}) + \gamma R(s_{1}) + \gamma^2 R(s_{2}) + \dots + \gamma^T R(s_{T})]
$$

## MDP Formulation
### Markov Property
* **Markov Property:** the process is memoryless.
* At time $t$, given state $s_{t}$, all future states $s_{t+j}$ do not depend on past states/actions
	* All the history is self-contained within the current state.
* This means the optimal action at time $t$ depends **only** on the current $s_t$

### Definition
* An MDP is defined as:
$$
\text{MDP}(S, A, \{P_{s, a}\}_{s\in S, a\in A}, \gamma, R, s_{0})
$$
* The **goal** is to design an algorithm to choose actions that maximize the **expected discounted payoff**.

## Policies
### Deterministic Policy
* The best strategy for the algorithm can be expressed as a **policy**.
$$
\pi : S \to A \tag{$\pi$ is a deterministic policy}
$$
* The policy defines what action to take at each state.

* Recall the example with the robot moving along a 1-D line. We want to get to 6.
![Pasted image 20241119134833](../../attachments/Pasted%20image%2020241119134833.png)
* The optimal policy is:
$$
\begin{align*}
\pi(s)&=R \quad \forall s \leq 6 \\
\pi(s)&=L \quad \forall s \geq 7
\end{align*}
$$

### Randomized Policy
* We now define a probability distribution over actions given state.
$$
\pi(a \in A | s \in S)
$$
* It turns out that there **always exists a deterministic optimal policy** for MDP.

## Value Function
* Given a policy $\pi$, the value function for that policy is:
$$
V^\pi: S \to \mathbb{R}
$$
* For every state $s \in S$
$$
V^\pi (s) = \text{Expected discounted payoff of executing policy $\pi$ starting from $s$}
$$
* This means that:
$$
s_{0}, a_{0}=\pi(s_{0}) \to s_{1}, a_{1}=\pi(s_{1}) \to \dots
$$
$$
V^\pi(s) = \mathbb{E}[R(s_{0}) + \gamma R(s_{1}) + \gamma^2 R(s_{2}) + \dots | s_{0} =s, \pi]
$$
### Computing the Value Function
* If we had a fixed policy $\pi$, how good is it ⟶ $V^\pi (s)$ is a measuring of how good it is

* Recall the example with the robot moving along a 1-D line. We want to get to 6.
	* $R(6) = 1$. Once we get to $6$, we stop moving.
![Pasted image 20241119134833](../../attachments/Pasted%20image%2020241119134833.png)
* In the case of the end state, we have this:
![Pasted image 20241119161126](../../attachments/Pasted%20image%2020241119161126.png)

* In the general case, we have a recursive structure:
	* $V^\pi (s)$ = (expected discounted payoff of running policy $\pi$ starting from $s$)
	* $V^\pi (s)$ = $R(s)$ + (expected discounted payoff of running policy $\pi$ from $s_1$, where $s_1$ is sampled from $P_{s, \pi(s)}$)
![Pasted image 20241119161616](../../attachments/Pasted%20image%2020241119161616.png)
* Expanding the expectation, we have the **Bellman Equation** for $V^\pi(s)$
![Pasted image 20241119161854](../../attachments/Pasted%20image%2020241119161854.png)

### Linear System
* Think of $V^\pi(s)$ for $s\in S$ as variables.
	* We have $|S|$ variables ($s\in S$) with $|S|$ equations (each $V^\pi(s))$
* This means that given $\pi$ and the dynamics $P_{s, a}$ ⟶ we can solve linear system to retrieve
$$
V^\pi(s)\quad \forall s \in S
$$

## Finding Best Policy
* We want to find:
$$
V^\star(s) = \max_{\pi}V^\pi(s)
$$
$$
\pi^\star = \arg\max_{\pi} V^\pi(s)
$$
### Bellman Equation for $V^*$
$$
\begin{align*}
V^\star(s) &= \max_{\pi}V^\pi(s) \\
&= \max_{\pi}\left(R(s) + \gamma \sum_{s'}P_{s, \pi(s)}(s') \cdot V^\pi(s')\right)\\
&= R(s) + \max_{\pi} \left(\gamma \sum_{s'}P_{s, \pi(s)}(s') \cdot V^\pi(s')\right) \\
&= R(s) + \max_{a} \left(\gamma \sum_{s'}P_{s, a}(s') \cdot \max_{\pi}V^\pi(s')\right)
\end{align*}
$$
* In the last step:
	* By taking max over $\pi$ for $V^\pi(s')$, we know reward for all future states.
	* We should take the action $a$ that maximizes the expected value given that.
$$
V^*(s) = R(s) + \gamma \left(\max_{a} \sum_{s'}P_{s, a}(s') \cdot V^*(s')\right)
$$
![Pasted image 20241119163820](../../attachments/Pasted%20image%2020241119163820.png)

### Bellman Operator
* We can view the Bellman equation as a Bellman operator: $\mathbb{R}^{|S|} \to \mathbb{R}^{|S|}$
* The optimal policy $V^*$ is a fixed point of this operator.
![Pasted image 20241119163926](../../attachments/Pasted%20image%2020241119163926.png)

# Algorithms for MDP

## Value Iteration
1. Initialize $V \in \mathbb{R}^{|S|}$ ⟶ $V = 0$ such that $V(s) = 0$ $\forall s \in S$
2. Loop:
	1. Apply Bellman operator $V = B(V)$.
$$
V(s) = R(s) + \gamma \left(\max_{a} \sum_{s'}P_{s, a}(s') \cdot V(s')\right)
$$
* This algorithm iteratively gets closer and closer to the optimal $V^*$

* Once we have the optimal values, the **optimal policy** can be computed from this:
$$
\pi^*(s) = \arg\max_{a} \sum_{s' \in S}P_{s, a}(s') \cdot V^*(s')
$$
