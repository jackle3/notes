
# RL Properties
**RL applies in situations where:**
* We have to make a **sequence of decisions** ⟶ decisions with long term effects

**For every decision:**
* We don't know the right answer (don't know the best possible action).
* None or little supervision

**In order to learn:**
* Learn from a designed reward function that models the real world scenario

**During the course of the program:**
* We can collect more data iteratively

# Markov Decision Processes
* Suppose we have a robot moving along a 1-D line.
![[Pasted image 20241119134833.png]]

**The model has:**
* $S$: set of states ⟶ all possible configurations of the robot, $S = \{1, \dots,8\}$
* $A$: set of actions ⟶ in this case, $A = \{L, R\}$
* $P_{s,a}$: dynamics/transitions ⟶ probability distribution

In state $s \in S$, applying action $a \in A$, the distribution of the next state $s' \in S$ is given by:
$$
P_{s,a}(s') = Pr[s' | s, a]
$$
* $P_{s, a}$ is a distribution ⟶ $\sum_{s' \in S} P_{s, a}(s') = 1$
