
# Discrimative vs. Generative
![[Pasted image 20241029221208.png]]
## Discriminative
So far, we've talked about **discriminative algorithms** that learn
$$
p(y |x ; \theta)
$$
* E.g. logistic regression modeled $p(y|x;\theta)$ as $h_{\theta}(x)=\sigma(\theta^T x)$
* These algorithms try to find a decision boundary that separates the two classes.

## Generative
For **generative algorithms**, we attempt to learn
$$
p(x|y; \theta) \text{ and } p(y)
$$
* We learn:
	* a model of what each class $y$ looks like ⟶ $p(x | y)$.
	* class priors for each class ⟶ $p(y)$
	* To classify, we match the new datapoint $x$ against our models of $y$.
* E.g. if $y$ indicates whether an example is a dog (0) or an elephant (1), then $p(x|y = 0)$ models the distribution of dogs’ features, and $p(x|y = 1)$ models the distribution of elephants’ features
* Once we have learned both, we can use Bayes' to derive the posterior prediction ⟶ label given data
$$
p(y|x) = \frac{p(x|y)p(y)}{p(x)}
$$
* If we wanted to make a prediction, we can ignore the denominator.
![[Pasted image 20241029141113.png]]
## Comparison
![[Pasted image 20241029140924.png]]

# Gaussian Discriminant Analysis
* In this model, we will assume that for $x\in \mathbb{R}^d$, the distribution of features of each class $p(x | y)$ is distributed according to a multivative gaussian.
![[Pasted image 20241029221444.png]]
## Multivariate Gaussian
* Parameterized by a mean vector $\mu \in \mathbb{R}^d$ and a covariance matrix $\Sigma \in \mathbb{R}^{d\times d}$ where $\Sigma$ is PSD.
$$
x\sim \mathcal{N}(\mu, \Sigma)
$$
![[Pasted image 20241029141354.png]]
![[Pasted image 20241029141434.png]]

### Example Plots
* The mean $\mu$ defines where the peak is.
* As $\Sigma$ becomes larger, the Gaussian becomes more spread-out from the mean.
	* The higher peak for smaller $\Sigma$ is because it has to integrate to equal 1.

| ![[Pasted image 20241029141907.png]]                                        | ![[Pasted image 20241029141536.png]]     | ![[Pasted image 20241029141544.png]] |
| --------------------------------------------------------------------------- | ---------------------------------------- | ------------------------------------ |
| $\mu = \begin{bmatrix}-1 & -1.5\end{bmatrix}^T$ and covariance $\Sigma = I$ | Mean zero and covariance $\Sigma = 0.6I$ | Mean zero and $\Sigma=2I$            |

* As we put values in the off-diagonal, the distribution concentrates around the $y = x$ line.
![[Pasted image 20241029141741.png]]

## GDA Model
* We model $p(x|y)$ as a multivariate gaussian:
$$
\begin{align*}
y &\sim \text{Bernoulli}(\phi) \\
x|y = 0 &\sim \mathcal{N}(\mu_{0}, \Sigma) \\
x|y = 1 &\sim \mathcal{N}(\mu_{1}, \Sigma)
\end{align*}
$$
![[Pasted image 20241029142137.png]]
* The parameters of our model are:
	* $\phi$ a scalar in $(0, 1)$
	* $\mu_0, \mu_1$ vectors in $\mathbb{R}^d$
	* $\Sigma$ matrix in $\mathbb{R}^{d\times d}$

* Given a training set $\{x^{(i)}, y^{(i)}\}_{i=1}^n$, the log likelihood is:
![[Pasted image 20241029142345.png]]
* Recall that for a discriminative model, the likelihood was simply:
$$
\ell(\theta) = \log \prod_{i=1}^n p(y^{(i)} | x^{(i)}; \theta)
$$
* By maximizing the log likelihood with respect to the model parameters, the MLE estimate is:
	* $\phi$ ⟶ the prior probability of $y = 1$ of each class is simply the average of $y$
	* $\mu$ ⟶ the average of datapoints $x^{(i)}$ where $y^{(i)}$ is the desired class (0 or 1)
![[Pasted image 20241029142747.png]]

### Decision Boundary
* Notice that the contours of the two Gaussians are the same, except that they have different means.
	* This is because they share a covariance matrix $\Sigma$.
	* For GDA, if we use the same $\Sigma$ for $p(x | y = 0)$ and $p(x | y = 1)$:
		* we will get a linear decision boundary ⟶ because $p(x | y = 0) p(y = 0) = p(x | y = 1) p(y = 1)$ will be linear.
		* the likelihood function will be convex ⟶ one global maxima.
![[Pasted image 20241029142955.png]]

* The GDA explained so far is technically **Linear Discriminant Analysis** ⟶ using same $\Sigma$ to get a linear decision boundary.
![[Pasted image 20241029143258.png]]

* If we use multiple $\Sigma$ we can get quadratic decision boundaries.
![[Pasted image 20241029143313.png]]

## GDA Vs Logistic Regression
![[Pasted image 20241029221507.png]]
* If $p(x | y)$ is a multivariate gaussian with shared $\Sigma$, then $p(y | x)$ follows the logistic function.
	* The converse is not true ⟶ $p(y | x)$ being a logistic function does not imply gaussian distribution.
	* GDA makes stronger modeling assumptions about the data than does logistic regression.
		* When these modeling assumptions are correct, then GDA will find better fits to the data, and is a better model. Specifically, when $p(x|y)$ is indeed gaussian (with shared $\Sigma$), then GDA is asymptotically efficient (i.e., requires less training data to learn “well”)
		* Informally, this means that in the limit of very large training sets (large n), there is no algorithm that is strictly better than GDA (in terms of, say, how accurately they estimate $p(y|x)$).
	* In contrast, by making significantly weaker assumptions, logistic regression is also more robust and less sensitive to incorrect modeling assumptions.
		* There are many assumptions that would lead to $p(y|x)$ taking the form of a logistic function.
			* E.g. if $x|y = 0 ∼ Poisson(λ_0)$, and $x|y = 1 ∼ Poisson(λ_1$), then $p(y|x)$ will be logistic.
			* E.g. if $x|y = 0 ∼ ExpFam(\eta_{0})$, and $x|y = 1 ∼ ExpFam(\eta_{1}$), then $p(y|x)$ will be logistic.
		* Logistic regression will also work well on Poisson data like this. But if we were to use GDA on such data—and fit Gaussian distributions to such non-Gaussian data—then the results will be less predictable, and GDA may (or may not) do well.
![[Pasted image 20241029143502.png]]
![[Pasted image 20241029143514.png]]
* This is exactly the form that logistic regression used to model $p(y = 1 | x)$.


# Naive Bayes
![[Pasted image 20241029221531.png]]
* This learning algorithm is used when the feature vector entries $x_j$ are discrete.
* For example with spam classification, the feature vector can be the presence of words based on the vocabulary.
	* Each $x^{(i)}$ is a vector in $\{0, 1\}^{|V|}$
![[Pasted image 20241029144121.png]]
* We want to model $p(\vec{x}|y)p(y)$.

> [!NOTE] Naive Bayes Assumption
> Every $x_i$ is conditionally independent given $y$. This means if the vocab size was $|V| = d$:
![[Pasted image 20241029144336.png]]

## Parameters
![[Pasted image 20241029144455.png]]

## Joint Likelihood
* The expansion comes from the definition of conditional probabilities.
![[Pasted image 20241029144521.png]]

## MLE
![[Pasted image 20241029144555.png]]

## Prediction
![[Pasted image 20241029144634.png]]

# Laplace Smoothing
* Suppose you want to train a Naive Bayes model but you encounter **unseen words**.
* For example, suppose the word "neurips" was the 35000th word in the vocabulary.
	* Since it has never seen the word before, the MLE estimate is zero for both types.
![[Pasted image 20241029144815.png]]
![[Pasted image 20241029144839.png]]

## Definition
* In the original formulation, given a set of n independent observations $z^{(i)}$, the MLE is:
	* This means if we never see $z^{(i)} = j$, then $\theta_j = 0$.
![[Pasted image 20241029144927.png]]
* In Laplace smoothing, we replace the above estimate with:
	* This means if we never see $z^{(i)} = j$, then $\theta_j = \frac{1}{k+n}$ where $k$ is the number of classes (e.g. $|V|$)
![[Pasted image 20241029145005.png]]

* In the context of Naive Bayes, the model with Laplace Smoothing would be:
![[Pasted image 20241029145216.png]]

# Multinomial Event Model
* The Multinomial Event Model is specifically for text classification.

## What Did Naive Bayes Use?
* Naive Bayes used the **multivariate Bernoulli event model**:
	* Works well for many classification problems but not as well as MEM for text classification.
	* In this model, we assumed that the way an email is generated is that:
		1. It is randomly determined (according to the class priors $p(y)$) whether a spammer or non-spammer will send you your next
		2. Then, the person sending the email runs through the dictionary, deciding whether to include each word $j$ in that email independently and according to the probabilities $p(x_j = 1|y) = \phi_{j|y}$.
		3. Thus, the probability of a message was given by $p(y | x) \propto p(x | y)p(y) = p(y) \prod_{j=1}^d p(x_j | y)$
		4. Note that $p(x_j | y)$ is a Bernoulli.

## Definition of Multinomial Event Model
* Let $x_j$ denote the identity of the $j$-th word in the email.
	* This means now that $x_j \in \{1, \dots, |V|\}$
	* And that each $\vec{x}$ has length equal to the length of the email
	* Each $x_j$ is independent but identically distributed, drawn from $p(x_j | y)$
	* Note that $p(x_j | y)$ is a Multinomial ⟶ its same for all values of $j$
![[Pasted image 20241029145841.png]]

## Parameters
![[Pasted image 20241029150222.png]]

## Likelihood
![[Pasted image 20241029150258.png]]

## MLE
* The inner summation is the number of times word $k$ appears in $x^{(i)}$
![[Pasted image 20241029150322.png]]

## Laplace Smoothing
![[Pasted image 20241029150329.png]]

# Log Prediction
* Sometimes we will need to take the log of our probabilities due to numerical instability. Recall the NB assumption:
![[Pasted image 20241029150516.png]]

![[Pasted image 20241029150520.png]]
