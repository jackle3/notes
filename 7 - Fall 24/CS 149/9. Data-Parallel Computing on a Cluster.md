# Last Lecture: Data Parallel Thinking
* We've been accustomed to parallel programming from a thread pov ⟶ "what do threads do?"
* **Data parallel thinking** ⟶ parallel algorithms in terms of operations on sequences of data.
	* `map, filter, fold/reduce, scan/segmented scan, sort, groupBy, join, partition/flatten`
* Main idea: high-performance parallel implementations of these primitive operations exist.
	* Programs written in terms of these can run efficiently on parallel machines (if you can avoid being bandiwdth bound).

# Today's Theme
* How do you use data parallel programming to program with thousands of cores?
* We want to make data parallel operations:
	* scalable (**clusters** with 100k+ cores)
	* fault tolerant (don't lose data when something fails)
	* efficient (optimize system performance with efficient memory usage)
* We focus on **low arithmetic intensity** data processing applications.
	* Recall that arithmetic intensity is the ratio of computation to communication.
		* Low intensity ⟶ less computation, more communication.
		* Programs with **low** arithmetic intensity are **memory bandwidth bound**
	* Example: machine learning applications, etc.

# Why Use a Cluster?
* Want to process 100 TB of log data (e.g. at large companies like Meta)
* Suppose our bandwidth is 50 MB/s:
	* Using 1 node would take us 23 days
	* Using 1000 nodes would take us 33 minutes
* **Issue:** as number of nodes increase, hard to fully utilize them all.
	* Hard to program for all the cores.
	* Something breaks every hour
	* Need **efficient and reliable** and usable framework.

## Warehouse-Scale Computers (WSC)
**Nodes have standard architecture:**
* Cluster of commodity Linux nodes (multicore x86)
* **Private memory ⟶ each node has separate address space and separate OS**
* Communication via an Ethernet network ⟶ bandwidths >10-40 Gb/s

**WSCs are relatively cheap**
* Built from commodity processors, networks, and storage
	* You can build 1000s of nodes for < $10 M
* **Many WSCs now used optimized networks ⟶ customized and expensive.**
	* Produces much better performance than commodity networks.
	* Use supercomputer networking ideas to provide high bandwidth across the datacenter.

**How to organize computations on this architecture?**
* We want to address issues like load balancing and failures.

## Organization: Message Passing Model
* Originally, threads communicate by using a **shared memory address space** with synchronization.
[[6. Locality + Communication + Contention#Message Passing Model (abstraction)]]
* **Message passing:** Distributed memory communication **without shared memory**
	* Threads operate within their own private address spaces
	* Threads communicate by sending/receiving messages
		* `send`: specifies recipient, buffer to be transmitted, and optional message identifier (“tag”)
		* `receive`: sender, specifies buffer to store data, and optional message identifier
		* Sending messages is the only wayto exchange data between threads 1 and 2
![[Pasted image 20241103215248.png]]
### Synchronous/Blocking Send/Receive
* **Synchronization**: Provides inherent synchronization as the sender and receiver block until the communication is complete, reducing the need for additional synchronization mechanisms.
* **Deadlocks**: Can still occur if processes are mutually waiting for each other to send or receive messages, leading to a deadlock situation.
	* Waiting to receive something that was never sent.
	* Everyone is sending (and waiting for `ack`), no one is able to receive.
### Asynchronous/Non-Blocking Send/Receive
* **Synchronization**: Allows independent send and receive operations, significantly reducing the need for synchronization mechanisms.
* **Deadlocks**: Minimizes the risk of deadlocks since processes do not block each other directly, but care is still needed to avoid other synchronization issues.

## WSC Nodes/Servers
* **Racks** contain multiple nodes/servers.
* Each node/server has multiple cores, independent storage, and DRAM.
	* It has fast communication with **nodes in same rack**, slower with **nodes in other racks**.
	* In large scale WSCs, bandwidth to other nodes/racks may equal bandwidth to get to a node's own SSD.
* The **top-of-rack switch** connects different racks together.
![[Pasted image 20241103220525.png]]
