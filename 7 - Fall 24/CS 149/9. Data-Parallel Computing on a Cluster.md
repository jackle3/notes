# Last Lecture: Data Parallel Thinking
* We've been accustomed to parallel programming from a thread pov ⟶ "what do threads do?"
* **Data parallel thinking** ⟶ parallel algorithms in terms of operations on sequences of data.
	* `map, filter, fold/reduce, scan/segmented scan, sort, groupBy, join, partition/flatten`
* Main idea: high-performance parallel implementations of these primitive operations exist.
	* Programs written in terms of these can run efficiently on parallel machines (if you can avoid being bandiwdth bound).

# Today's Theme
* How do you use data parallel programming to program with thousands of cores?
* We want to make data parallel operations:
	* scalable (**clusters** with 100k+ cores)
	* fault tolerant (don't lose data when something fails)
	* efficient (optimize system performance with efficient memory usage)
* We focus on **low arithmetic intensity** data processing applications.
	* Recall that arithmetic intensity is the ratio of computation to communication.
		* Low intensity ⟶ less computation, more communication.
		* Programs with **low** arithmetic intensity are **memory bandwidth bound**
	* Example: machine learning applications, etc.

# Why Use a Cluster?
* Want to process 100 TB of log data (e.g. at large companies like Meta)
* Suppose our bandwidth is 50 MB/s:
	* Using 1 node would take us 23 days
	* Using 1000 nodes would take us 33 minutes
* **Issue:** as number of nodes increase, hard to fully utilize them all.
	* Hard to program for all the cores.
	* Something breaks every hour
	* Need **efficient and reliable** and usable framework.

## Warehouse-Scale Computers (WSC)
**Nodes have standard architecture:**
* Cluster of commodity Linux nodes (multicore x86)
* **Private memory ⟶ each node has separate address space and separate OS**
* Communication via an Ethernet network ⟶ bandwidths >10-40 Gb/s

**WSCs are relatively cheap**
* Built from commodity processors, networks, and storage
	* You can build 1000s of nodes for < $10 M
* **Many WSCs now used optimized networks ⟶ customized and expensive.**
	* Produces much better performance than commodity networks.
	* Use supercomputer networking ideas to provide high bandwidth across the datacenter.

**How to organize computations on this architecture?**
* We want to address issues like load balancing and failures.

# Organization: Message Passing Model
* Originally, threads communicate by using a **shared memory address space** with synchronization.
[](7%20-%20Fall%2024/CS%20149/6.%20Locality%20+%20Communication%20+%20Contention.md#Message%20Passing%20Model%20(abstraction))
* **Message passing:** Distributed memory communication **without shared memory**
	* Threads operate within their own private address spaces
	* Threads communicate by sending/receiving messages
		* `send`: specifies recipient, buffer to be transmitted, and optional message identifier (“tag”)
		* `receive`: sender, specifies buffer to store data, and optional message identifier
		* Sending messages is the only wayto exchange data between threads 1 and 2
![Pasted image 20241103215248](attachments/Pasted%20image%2020241103215248.png)
## Synchronous/Blocking Send/Receive
* **Synchronization**: Provides inherent synchronization as the sender and receiver block until the communication is complete, reducing the need for additional synchronization mechanisms.
* **Deadlocks**: Can still occur if processes are mutually waiting for each other to send or receive messages, leading to a deadlock situation.
	* Waiting to receive something that was never sent.
	* Everyone is sending (and waiting for `ack`), no one is able to receive.
## Asynchronous/Non-Blocking Send/Receive
* **Synchronization**: Allows independent send and receive operations, significantly reducing the need for synchronization mechanisms.
* **Deadlocks**: Minimizes the risk of deadlocks since processes do not block each other directly, but care is still needed to avoid other synchronization issues.

# WSC Nodes/Servers
* **Racks** contain multiple nodes/servers.
* Each node/server has multiple cores, independent storage, and DRAM.
	* It has fast communication with **nodes in same rack**, slower with **nodes in other racks**.
	* In large scale WSCs, bandwidth to other nodes/racks may equal bandwidth to get to a node's own SSD.
* The **top-of-rack switch** connects different racks together.
![Pasted image 20241103220525](attachments/Pasted%20image%2020241103220525.png)

# Computer Component Reliability
* MTTF stands for mean time to failure
    * When there is one unit, the MTTF might be high. With many units, the MTTF is much lower.
* We need a system that can handle failures.
![Pasted image 20241104112508](attachments/Pasted%20image%2020241104112508.png)

# Storage Systems
* First order problem: if nodes can fail, how can we store data persistently?
* Answer: **distributed file systems.**
    * Provides global file namespace across all nodes.
    * Provides **fault tolerance** (if one node fails, the system can continue to operate).
    * Provides **scalability** (if we add more nodes, the system can continue to operate).
    * E.g. Google File System (GFS), Hadoop Distributed File System (HDFS).
* Typical usage pattern:
    * Huge files with 100s of GBs.
    * Data is rarely updated once written.
    * Reads and appends are much more common than modifications (e.g. log files where you append).
* **Key idea:**
    * **Data is stored redundantly** across many nodes.
    * **Each node stores only a fragment** of the file.

## HDFS
**Chunk servers**
* a.k.a DataNodes in HDFS.
* File is split into contiguous chunks (usually 64-256MB).
* Each chunk is replicated multiple times (usually 3x).
* Try to keep replicas in different racks to avoid losing all replicas in a rack failure.
**Master node**
* a.k.a NameNode in HDFS.
* Stores metadata ⟶ this data is usually replicated across multiple nodes.
    * Keeps track of all files and their locations (e.g. given filename, what are the chunks?)
    * Keeps track of which chunks are stored on which nodes
* Helps with load balancing by assigning requests to different nodes.
* Helps with fault tolerance by detecting failed nodes and replicas.
**Client library for file access**
* Talks to master to find out where chunks are stored.
* Talks directly to chunk servers to read and write data.
![Pasted image 20241104113128](attachments/Pasted%20image%2020241104113128.png)
* **When you read:**
	* Client contacts NameNode first to get DataNode locations for chunks
	* NameNode provides addresses of DataNodes that store replicas
	* Reading process:
		* Client reads data directly from one of the DataNodes
		* If that DataNode fails, client tries another replica
		* Continues until successful read or all replicas exhausted
* **When you write:**
    * Client contacts NameNode first to get DataNode locations for replicas
    * NameNode provides addresses of DataNodes that will store replicas
    * Writing process:
        * Client writes data to first DataNode
        * First DataNode forwards to second DataNode
        * Second DataNode forwards to third DataNode
    * Acknowledgment flows back through chain:
        * Third DataNode → Second DataNode → First DataNode → Client
    * Pipeline approach ensures data consistency and efficient network usage

* **Extremely hard to lose data in HDFS because chance of losing all replicas in a rack failure is low.**

# Large Distributed Files
* Suppose we have a log file `cs149log.txt` of page views ⟶ this can get super large.
* We store this in a distributed file system:
	* Cluster of 4 nodes, each node with a 10 TB SSD
	* Contents of file are distributed evenly in blocks/chunks across the cluster.
![Pasted image 20241104120151](attachments/Pasted%20image%2020241104120151.png)
* Suppose we want to ask: "What type of phones are visitors using?"
    * Using messages passing model, we can ask each node to count the number of page views from each type of phone.
    * Then, we can sum up the counts from all nodes to get the total number of page views from each type of phone.
    * But this is not efficient because we have to send data all over the network; might also lead to deadlocks.

## MapReduce
* MapReduce is a programming model and software framework that processes large amounts of data in parallel on a cluster
### Map
* Higher order function (a function that takes a function as an argument)
* It applies side-effect free unary function `f :: a -> b` to each element of the input sequence.
    * Takes a function `f` and a sequence `[x1, x2, … xn]`
    * Returns a new sequence `[f(x1), f(x2), … f(xn)]`
![600](attachments/Pasted%20image%2020241104113809.png)
### Reduce
* Apply binary operation `f` to each element and an accumulated value:
    * The type signature is `reduce :: ((b, a) -> b) -> seq a -> b`
    * Takes a function `f`, a sequence `[x1, x2, … xn]`
    * Returns a single value `reduce f [x1, x2, … xn] = f(f(…f(f(x1, x2), x3), …), xn)`
* Similar to `fold`, except we don't have an initial value.
![Pasted image 20241104114519](attachments/Pasted%20image%2020241104114519.png)

### Program Example
* The code below computes the count of page views by each type of mobile phone.
* Observe that we create:
	* A `mapper` that constructs intermediate key-value pairs.
	* A `reducer` that aggregates the key value pairs (sums up the counts).
* We use the `runMapReduceJob` function to run the map and reduce functions over the input.
![Pasted image 20241104120833](attachments/Pasted%20image%2020241104120833.png)

## Implementing MapReduce
* We first map to create key-value word counts, then group-by to send those pairs to the appropriate reducer, then reduce to get the final sums.
![Pasted image 20241104121105](attachments/Pasted%20image%2020241104121105.png)
### Step 1. Running the Mapper Function
* The mapper function is called once for each line of the input file.
* **Question**: How do we know which lines go to which node?
    * Idea 1: use work queue of a list of input blocks to process. Dynamic ⟶ free nodes takes next available block.
        * This is kinda of like work stealing ⟶ most optimal load balancing.
        * Though this requires a lot of communication between nodes.
    * Idea 2: data distribution assignment ⟶ each node processes lines in blocks of input file that is already stored on it.
        * E.g. node 0 processes blocks 0 and 1, node 1 processes blocks 2 and 3, etc.
        * Data was already distributed evenly across nodes when the file was created, so this is efficient communication.
![Pasted image 20241104122035](attachments/Pasted%20image%2020241104122035.png)

### Step 2: Gathering Data for Reducer
* This is the group-by step, where we send the appropriate keys to the reducer.
* Question: How do we know which keys go to which node?
    * We can use a hash function to determine which node a key should be sent to.
    * E.g. node 0 gets all keys that hash to 0, node 1 gets all keys that hash to 1, etc.
![Pasted image 20241104122356](attachments/Pasted%20image%2020241104122356.png)

### Step 3: Running the Reducer Function
* The reducer function is called once for each unique key in the intermediate data.
* It takes in a key and a sequence of values associated with that key, and returns a single value.

## Job Scheduler Responsibilities
**Exploit data locality ⟶ move computations close to data**
* Run mapper jobs on nodes that contain input data blocks.
* Run reducer jobs on nodes that already have most of the data for a particular key.

**Handling node failures**
* Job scheduler must detect when a node has failed and move its jobs to other nodes.
	* This is possible since inputs reside in persistent storage.
	* Scheduler can duplicate jobs on multiple machines (to reduce overall processing latency incurred by node failures).

**Load balancing**
* Scheduler can duplicate jobs on multiple machines.
	* Underloaded nodes can finish jobs before overloaded nodes.

## MapReduce Limitations
**Permits only very simple program structures.**
* Programs must be expressed in terms of: map followed by reduce by key.

**Not suitable for iterative algorithms.**
* Need to load data from disk for each iteration.
    * Because MapReduce is stateless; stores results in distributed file system between iterations.
![Pasted image 20241104134122](attachments/Pasted%20image%2020241104134122.png)

**To address these limitations, we want to use in-memory processing ⟶ Spark**
* Memory is much faster than disk; many working sets on big data clusters can fit in memory.

# Spark
* Spark is an in-memory, fault tolerant, distributed data processing engine.

**Comparison:**
* In MapReduce, we kept intermediate results on the disk (in the file system), then read it back in the next step.
    * Slow and bandwidth limited.
    * There's no locality between steps.
* In Spark, we keep intermediate results in memory.
    * Faster than disk.
    * Can pipeline operations and avoid materializing intermediate results.

**The goals are:**
* Cluster-scale computations where there is significant reuse of data across computations.
    * E.g. iterative algorithms, graph algorithms, machine learning
    * E.g. interactive data mining: load data into memory, perform multiple queries
* Don't want to incur inefficiency of writing intermediate results to disk (keep all in memory).
    * **Challenge**: efficiently implementing fault tolerance for in-memory computations.

## Fault Tolerance for In-memory Calculations

* Recall that in the MapReduce model, fault tolerance is achieved by:
    * Checkpoints after each map/reduce step (writing to file system)
    * Logs in the form of scheduler's list of outstanding jobs.
    * Functional structure allows programs to restart at granularity of a single map or reduce call.

**Replicate all computations**
* All tasks are executed on multiple nodes.
* Though this is expensive: decreases throughput by a factor of the replication factor.

**Checkpoint and rollback**
* Periodically save the in-memory state of tasks to persistent storage.
* Can restart tasks from checkpointed state if nodes fail.
* Though checkpointing is expensive ⟶ checkpointing takes time and takes up space.

**Maintain log of updates**
* Maintain a commands and data.
* Can replay the log to reconstruct the state of the data structures if a node fails.
* Though there is high overhead to maintaining the log.

## Resilient Distributed Datasets (RDDs)
* Spark's abstraction for distributed fault tolerant sequences of data.
* **RDDs are immutable** ⟶ RDDs are a read-only ordered collection of records.
    * Can only be created by deterministic **transformations** on data in persistent storage or other RDDs.
    * **Actions** on RDDs return data to applications.
    * Note: transformations are functional (stateless and no side effects) while actions are imperative (stateful and may have side effects).
* **RDDs are fault tolerant** ⟶ if a node fails, the RDD can be reconstructed from the original data.
    * RDDs are stored in memory of the nodes in the cluster.
    * RDDs are replicated across multiple nodes in the cluster.
    * If a node fails, the RDD can be reconstructed from the replicated data.
* **RDDs are in-memory** ⟶ RDDs are stored in memory of the nodes in the cluster.

> [!NOTE] Main Property of RDDs
> The Spark program defines HOW to compute each element of each RDD, but it does not specify WHEN those elements must be computed


### Transformations and Actions
* Transformations are applied together to create a **lineage**: a sequence of RDD operations needed to compute the output.
![Pasted image 20241104135510](attachments/Pasted%20image%2020241104135510.png)
### Examples
* In terms of length of RDDs: `lines` > `mobileViews` >`safariViews`
![Pasted image 20241104135302](attachments/Pasted%20image%2020241104135302.png)
* In this case we chain multiple transformations together to compute the final result.
	* `map` takes a sequence of `string` and outputs a sequence of pairs `(string, 1)`
	* `reduceByKey` finds all pairs with the same **key**, and performs the reduction (in this case `x + y`) on all the **values**.
![Pasted image 20241104135402](attachments/Pasted%20image%2020241104135402.png)

### Persistence
* We can use the `persist()` method to retain the RDD's contents in memory after an action.
    * `persist(RELIABLE)` stores contents in durable storage (e.g. disk).
    * Allows us to reuse an RDD for multiple actions without recomputing it.
* This creates a DAG of RDD operations.
![Pasted image 20241104135639](attachments/Pasted%20image%2020241104135639.png)

## Implementing RDDs

### How to Store RDDs in Memory?
* **Materialization** ⟶ store the actual data in memory.
* If we treated RDDs as arrays (i.e. materializing it), we would **need a huge amount of memory**
![Pasted image 20241107141727](attachments/Pasted%20image%2020241107141727.png)

### Loop Fusion
* Recall the idea of loop fusion from [](7%20-%20Fall%2024/CS%20149/6.%20Locality%20+%20Communication%20+%20Contention.md#Loop%20Fusion) where we fuse independent operations from two loops into a single loop.

### RDD Partitioning and Dependencies
* The RDD only tells you what operations you need to perform to go from `block` to `mobileviews`.
	* Each line/block is independent from other lines and blocks.
	* RDDs never explicitly define scheduling, allocation, or implementation ⟶ we can parallelize it or stream it however we want for efficiency (**its an abstraction**)
![Pasted image 20241107141951](attachments/Pasted%20image%2020241107141951.png)

* Spark can **fuse RDD operations** into streaming tasks to make it more memory efficient.
	* Keep track of the lineage (sequence of transformations) needed to compute the RDD.
	* Example: `map().filter().reduce()` can be fused into a single streaming task that only keeps **one line** in memory at a time.
![Pasted image 20241107142150](attachments/Pasted%20image%2020241107142150.png)

### Wide Dependencies
* Not all operations can be fused due to complex dependencies or randomness (e.g. shuffling).
* **Wide dependencies** ⟶ child RDD might depend on multiple parent RDDs in order to group
* **Narrow dependencies** ⟶ filter and map are narrow; each parent referenced by one child RDD
* E.g. `groupByKey` might say "I want all elements of RDD_A that begin with the key `k`"
![Pasted image 20241107142536](attachments/Pasted%20image%2020241107142536.png)
* `join` is another example of wide dependencies:
	* To create partition 0 for RDD_C, it needed part 0 of RDD_A and RDD_B and part 3 of RDD_B.
![Pasted image 20241107142749](attachments/Pasted%20image%2020241107142749.png)
* If you partition it correctly, you can avoid wide and only create **narrow dependencies**
![Pasted image 20241107142943](attachments/Pasted%20image%2020241107142943.png)
![Pasted image 20241107143011](attachments/Pasted%20image%2020241107143011.png)

### Scheduling Spark Computations
* Notice that `RDD_B` has wide dependencies on `RDD_A`. This means `A` has to be materialized in memory in order for `B` to correctly compute.
	* Note that data flows down. Arrows denote `from` depends on `to`.
![Pasted image 20241107143130](attachments/Pasted%20image%2020241107143130.png)

## Resilience
* RDD transformations are bulk, deterministic, and functional.
	* **Main Idea:** scheduling can construct contents of RDD from its lineage (the sequence of transformations used to create it)
		* The linear is a **log** of the transformations ⟶ records bulk data-parallel operations (low overhead)
* When a node fails, we can simply give those blocks to another node and recompute via lineage.
![Pasted image 20241107143541](attachments/Pasted%20image%2020241107143541.png)
