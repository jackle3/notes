# More Advanced CUDA Scheduling
## More Threads than Contexts
* In the program, each **thread block uses 256 CUDA threads**, as well as a `support` shared variable.
* Assuming that the GPU core (the SM) only has **128 execution contexts** (4 warps)
![Pasted image 20241028105535](Pasted%20image%2020241028105535.png)
* CUDA **will not** let you run this program on this core ⟶ **fail at compile time** because the thread block needs 256 threads but there are only 128 execution contexts.
* Why not just run threads 0-127 to completion, then run threads 128-255 to completion in order to execute the entire thread block?
	* Because of the call to `syncthreads` ⟶ leads to deadlock
	* Recall that `__syncthreads()` is a barrier that waits for **all threads in the block to reach it** ⟶ i.e. no thread can pass it until all 256 reach it.

## Implementation of Abstractions
![Pasted image 20241028105625](Pasted%20image%2020241028105625.png)

## Independence
* The code below is not necessarily independent ⟶ however, it is okay to schedule blocks in any order.
![Pasted image 20241028105836](Pasted%20image%2020241028105836.png)

* In this code, it is not okay to schedule blocks in any order. If block 1 goes first, we have a deadlock.
![Pasted image 20241028110102](Pasted%20image%2020241028110102.png)

## CUDA Summary
![Pasted image 20241028110229](Pasted%20image%2020241028110229.png)


# Data-Parallel Thinking
* Focused on writing algorithms that are more parallelizable.
![Pasted image 20241028111038](Pasted%20image%2020241028111038.png)

* **Main Idea:** high performance parallel implementations of these operations exist. So programs written in terms of these primitives can often run efficiently on parallel machines (if you can avoid being bandwidth bound)

* **Motivation:** applications need to expose large amounts of parallelism
	* GPUs have thousands of execution contexts, many cores, SIMD processing, etc

## Dependencies
![Pasted image 20241028111256](Pasted%20image%2020241028111256.png)

## Data-parallel Model
* Organized computation as operations on sequence of elements (order of elements matter)
	* E.g. perform same function on all elements of a sequence.
	* E.g. in NumPy, $C = A + B$ on vectors does element-wise addition on all elements in vectors

## Key Data Type: Sequences
![Pasted image 20241028111530](Pasted%20image%2020241028111530.png)

# Operations
* Unlike arrays, programs access elements of a sequence only through specific **operations**
	* It cannot be accessed via direct element access/indexing
# Map
* `map` is a higher order function (function that takes a function as an argument)
	* Given a function $f$, it applies $f$ on every element of $A$ to produce $B$.
	* $f$ is a function that takes as input a variable of type $a$ and outputs a variable of type $b$
![Pasted image 20241028111608](Pasted%20image%2020241028111608.png)
* In C++, the `transform` function is map ⟶ takes start and end of input iterator and outputs into output iterator after applying `unary_op`
* In Haskell, `f` is a function that takes a value `a` and produces a value `b`.
	* When we do `map f a`, we map `f` onto the sequence `a`, producing a sequence of `b`.
	* Read as:
		* `map`: This is the name of the function. It has **two arguments**.
		* `::`: This symbol specifies the type of map. In Haskell, `::` is used to denote type annotations.
		* `(a -> b)`: This represents a function that takes a value of type `a` and produces a value of type `b`. map takes this function as its first argument.
		* `->`: This arrow denotes that map is a function that takes multiple arguments in sequence.
		* `seq a`: This represents a sequence (or list) of elements of type a. map takes a sequence of type a as its second argument.
		* `seq b`: This is the return type of map, which is a sequence of type b. After applying the function `(a -> b)` to each element in the sequence of `a`s, map returns a sequence of `b`s.
![Pasted image 20241028111612](Pasted%20image%2020241028111612.png)
* Notice that `map` only guarantees that it takes a seq of `a` and makes a seq of `b`.
	* It guarantees that work done by `f` is independent ⟶ bc `f` is side-effect free.
	* It does not guarantee ordering of operations; could be parallel, etc.

## Parallelizing Map
![Pasted image 20241028112134](Pasted%20image%2020241028112134.png)
* In this simple parallel implementation:
	* We break the sequence `s` onto smaller subsequences.
	* Run map independently on smaller subsequences.
	* Then concatenate the resulting subsequences
![Pasted image 20241028112139](Pasted%20image%2020241028112139.png)

# Fold (fold left)
* Applies binary operation `f` to each element and an accumulated value, seeded by initial value of type `b`.
* The function `f` takes `(b, a)` and outputs `b`.
	* E.g. it takes in the initial value `10` (type b) and input `3` (type a) to produce `13` (type b).
![Pasted image 20241028113422](Pasted%20image%2020241028113422.png)
* The function `fold` takes three arguments and outputs `b`
	* Starting value `b`
	* Folding function `f` that takes `(b, a)` and outputs `b`
	* Input sequence `seq a`
![Pasted image 20241028113431](Pasted%20image%2020241028113431.png)
* In the example: the initial element is `10`. The function is `+`. The input sequence is red.
![Pasted image 20241028112752](Pasted%20image%2020241028112752.png)
## Parallelizing Fold
* We cannot parallelize `fold` is we know nothing about `f` ⟶ `f` might not be commutative and associative.
* If `f` is associative, we can parallelize it by breaking input sequence into chunks and divide and conquer.
	* The starting element has to now be the identity of `f` ⟶ `f(identity, x) -> x
![Pasted image 20241028113253](Pasted%20image%2020241028113253.png)

# Scan
* The `i`th output element (green) is the scan-inclusive operator applied to all input elements up to and including `i`.
* As opposed to `fold` which produces a value of type `b`, `scan` produces a `seq a`.
![Pasted image 20241028113750](Pasted%20image%2020241028113750.png)

## Parallelizing Scan

**Data-parallel scan**
![Pasted image 20241028114029](Pasted%20image%2020241028114029.png)

**Data-parallel inclusive scan**
* One implementation is to sum all pairs in each step.
	* In the first step we perform operation $N-1$ pairs.
	* In the second step we perform operation on $N-2$ pairs.
	* In the third step we perform operation on $N - 4$ pairs.
![Pasted image 20241028114205](Pasted%20image%2020241028114205.png)

* Each step is $O(n)$ work. There are $O(\log n)$ steps.
	* The total work is $O(n \log n)$
	* The span is $O(\log n)$.
		* Span is the longest chain of dependencies — minimum sequence of dependent steps
		* If you have infinite processors, minimum runtime is the span bc of dependencies.
![Pasted image 20241028114617](Pasted%20image%2020241028114617.png)

## Work-efficient Parallel Exclusive Scan
* This implementation has $O(n)$ work.
	* There are two phases:
		* In the first phase we combine pairs but only half at a time.
			* Step 1 has $N/2$ pairs, step 2 has $N/4$ pairs, etc
			* This converges to $O(n)$ work total.
		* In the second phase, it distributes the partial sums back through the array.
![Pasted image 20241028114911](Pasted%20image%2020241028114911.png)
![Pasted image 20241028115514](Pasted%20image%2020241028115514.png)
* This algorithm is **massively parallelly** but doesn't have that great data locality.
	* Also the span is technically $2 \log n$, but the work is asymptotically less.
## Two Cores
* Suppose we only had two cores to run the exclusive scan algorithm.
	* This one is less parallel but would be perfectly fine on a two core system.
![Pasted image 20241028115633](Pasted%20image%2020241028115633.png)
![Pasted image 20241028115715](Pasted%20image%2020241028115715.png)

## SIMD Exclusive Scan
* In this case, the programmer knows that groups of 32 threads (warps) are executed by a 32-wide SIMD instruction.
	* `lane >= 1` ⟶ 31 of the 32 threads run the first 31 pairwise summations
	* `lane >= 2` ⟶ 30 threads run the second 30 pairwaise summations.
	* `lane >= 4` ⟶ 28 threads run the third 28 pairwise summations.
* `log(32) = 5` ⟶ so in 5 SIMD operations of the CUDA program (each of the `lane >= n`), we performed a scan of the 32-element array.
* This algorithm has the minimum span (span = $\log n$)
	* But it does more work (it does the $n\log n$ version of work), though we're not worried about it because $n = 32$.
![Pasted image 20241028120137](Pasted%20image%2020241028120137.png)
![Pasted image 20241028121330](Pasted%20image%2020241028121330.png)

* Since we have a really good 32-wide segmented scan that has good SIMD utilization, we can just repeat it!
	* Break up the array into chunks and just use the 32 wide scan on each chunk.
	* Then run scan on the results to build the `base` array.
	* Then add the base to each warp to get result.
		* E.g. add base $a_{0-31}$ to all elements in warp 1 $a_{32-63}$ to get result $a_{0-32}, \dots ,a_{0-63}$
![Pasted image 20241028134826](Pasted%20image%2020241028134826.png)
![Pasted image 20241028134916](Pasted%20image%2020241028134916.png)
* This code can do a scan on large arrays very quickly. Suppose we have a 1024 size block.
	* 5 instructions for each warp ⟶ 32-wide scan producing partial prefix sums.
	* 5 more instructions to scan the partials together to create the bases.
	* 1 instruction per thread to add the base back to the element.
![Pasted image 20241028135059](Pasted%20image%2020241028135059.png)

# Parallel Segmented Scan
![Pasted image 20241028135343](Pasted%20image%2020241028135343.png)
* If you parallelize over vertices (without accounting for edges), you might have terrible work imbalance. This is where segmented scan comes in.
![Pasted image 20241028135855](Pasted%20image%2020241028135855.png)
* You can represent a sequence of sequences purely as two sequences:
	* One is just a flat `data` sequence.
	* The other is a `flag` sequence that tells you the start of subsequences.
![Pasted image 20241028135903](Pasted%20image%2020241028135903.png)
* You can parallelize this similar to before, but we now just pass the flag along.
	* If you see a `flag == 1`, then don't add the `bases`/accumulation in.

![Pasted image 20241212150513](Pasted%20image%2020241212150513.png)
![Pasted image 20241212150519](Pasted%20image%2020241212150519.png)

# Sparse Matrix Multiplication Example
![Pasted image 20241028140507](Pasted%20image%2020241028140507.png)
* You can store sparse matrices as a sequence of sequences:
	* Store the nonzero elements and their column locations in the row.
	* `row_starts` denotes the index inside `values` that each row starts, assuming that it was stored as just a regular sequence (i.e. `data = [3, 1, 2, 4, …]`)
![Pasted image 20241028140511](Pasted%20image%2020241028140511.png)

* You can perform sparse matrix multiplication using `scan` and `map` ⟶ completely parallel.
	* You could've also used segmented parallel `fold` since we only care about the last one in the segment.
![Pasted image 20241028140527](Pasted%20image%2020241028140527.png)

# Gather/scatter
![Pasted image 20241028140858](Pasted%20image%2020241028140858.png)![Pasted image 20241028140903](Pasted%20image%2020241028140903.png)
![Pasted image 20241028141002](Pasted%20image%2020241028141002.png)
![Pasted image 20241028141014](Pasted%20image%2020241028141014.png)

# More Sequence Operations
![Pasted image 20241028141051](Pasted%20image%2020241028141051.png)

# Important Example
* Suppose we want to have a **grid of particles** data structure on a large parallel machine (e.g. a GPU).
* **Problem:** there are 1M point particles placed in a 16-cell uniform grid.
	* We want to build a structure that maps cell IDs to the count of particles in the cell and the particle ID.
	* Build an array of all the cells, for every item in array we have a list of particle ids in that cell.
![Pasted image 20241106164626](Pasted%20image%2020241106164626.png)
![Pasted image 20241106164730](Pasted%20image%2020241106164730.png)
**Analogy to Assignment 3 Render**
* If given a data structure where for different regions of the screen, there was a list of triangles/circles that might overlap with that cell, we would trivially be able to solve the renderer.
* The assignment boils down to building this array of lists.

## Solution 1: Parallelize over Particles
* This would have essentially serial execution time due to contention for lock.
![Pasted image 20241106165001](Pasted%20image%2020241106165001.png)

## Solution 2: Use Finer-granularity Locks
* Similar to before, but we have one lock per cell.
* Still not probably gonna perform well because in a GPU where there are thousands of threads, contention can be very high.
![Pasted image 20241106165031](Pasted%20image%2020241106165031.png)

## Solution 3: Parallelize over Cells
* In this case, there is no synchronization ⟶ making the list for each cell is done independently.
* Issue:
	* This is doing a lot of extra work ⟶ "is p in cell" is done for every cell. Before, it was just "compute c containing p".
![Pasted image 20241106165133](Pasted%20image%2020241106165133.png)
* Some solutions for assignment 3 might be using this.

## Solution 4: Compute Partial Results + Merge
* Divide particles into partial grids and compute it in parallel.
* Then merge all the partial grids together into one.
![Pasted image 20241106165324](Pasted%20image%2020241106165324.png)

## Solution 5: Data-parallel Approach
* This is extremely parallelizable with no global synchronization.

**Step 1: In parallel, compute cell containing each particle**
![Pasted image 20241106165440](Pasted%20image%2020241106165440.png)

**Step 2: Use a parallel sort to sort both lists by the grid_cell values**
* As a result, particles in the same cell are now next to each other in the `particle_index` array.
![700](Pasted%20image%2020241106165500.png)

**Step 3: Compute start and end of cell indices**
* If we wanted to jump directly to the particles in cell `c`, we need to know the beginning and end of the cell in the `particle_index` array.
* The code steps are:
	* If `this_cell` is different than the cell for the last particle, this point is where the new sublists starts.
![Pasted image 20241106165638](Pasted%20image%2020241106165638.png)
