# What Are Caches?

* A lot of memory movement is done manually by the programmer.
* However, cache's attempt to do this automatically to improve locality.
  * It gives both temporal and and spatial locality
    * temporal ⟶ repeated access to same address
      * if we access index 0, we're likely to access it again
    * spatial ⟶ loading data in cache line preloads the data needed for subsequent **accesses to different addresses in the same line**
      * if we access index 0, we're likely to access index 1, etc.
        ![[Pasted image 20241111224650.png]]

## Example

* Suppose we have:
  * 16 bytes of memory
  * 8 byte cache
  * cache reads in 4-byte cache lines (so 2 lines fit in the cache)
    * easier to load in a bigger memory
    * primarily for spatial locality
  * cache follows an LRU policy
    ![[Pasted image 20241111225944.png|400]]
* **cold miss** ⟶ first time accessing this memory location, so it's not in cache yet
* **hit** ⟶ requested data is already present in cache
* **capacity miss** ⟶ cache is full and we had to evict this item earlier due to limited cache size
* **conflict miss** ⟶ cache had space but this item was evicted earlier because multiple addresses mapped to same cache location

### Example 1

![[Pasted image 20241111230106.png]]

### Example 2

![[Pasted image 20241111230123.png]]

# Cache Associativity

* By limiting where the cache can store data (using smaller N-way set associativity), we increase the number of potential cache misses but reduce cost since fewer comparators are needed to check cache locations.

### Direct Mapped (1-way Set associative)
* Each memory address can only go to ONE specific location in the cache
* The cache location is determined by the memory address (usually by taking address % cache size)
* Simplest to implement but can lead to more conflicts since addresses that map to same location will evict each other
* Example: If address 0x100 maps to cache location 5, it can ONLY ever be stored there

### Fully Associative

* A memory address can be stored in ANY location in the cache
* Most flexible mapping but requires checking every cache location when looking for data
* More expensive to implement since comparators needed for each cache location
* Best for avoiding conflicts but hardware cost is high
* Example: Address 0x100 could be stored anywhere in the cache
* **Note:** the lowest miss rate you can see in a cache is in a fully associative cache.

### N-way Set Associative

* Compromise between direct mapped and fully associative
* Cache is divided into sets, each set has N locations (ways)
* Memory address maps to a specific set but can go in any of the N ways within that set
* Common values: 2-way, 4-way, 8-way set associative
* Example: In 4-way set associative, address 0x100 might map to set 2 and could be stored in any of the 4 ways in that set

# Cache Design

![[Pasted image 20241111231334.png]]

* Each cache line has:
	* A dirty bit
	* Line state
	* Tag
	* Cache data (from byte 0 of line to byte N-1 of line)
* Write back cache ⟶ only write back to memory when cache line is being evicted (we mark the dirty bit to note that the cache line has been changed)
* Write through cache ⟶ write back to memory immediately after writing to cache line (next level is always up to date)
* Write allocate ⟶ On a write miss, the cache first loads the entire cache line from memory before performing the write. This ensures the cache has a complete copy of the data.
* No write allocate ⟶ On a write miss, the write goes directly to memory without loading the cache line. The data is not cached, saving the overhead of loading the line when only writing.

## Example: Write-allocate, Write-back Cache on a Write Miss

![[Pasted image 20241111231849.png]]

* Let's walk through what happens when a processor executes `int x = 1` with a write-allocate, write-back cache:
  1. Processor attempts to write value 1 to memory address of `x`, but discovers it's not in cache (cache miss)
  2. Cache controller:
     * Determines where this new line should go based on the address
     * If that location contains a dirty line (previously modified data), writes that old line back to memory first
  3. Cache fetches the entire cache line containing `x`'s address from main memory
  4. Cache updates just the portion containing `x` with the value 1, while preserving rest of the cache line
  5. Sets the dirty bit to indicate this line has been modified and will need to be written back to memory eventually

# Cache Hierarchy

* L1 is lowest latency but smallest, then L2 then L3.
* L1 has 8-way set associative
  * Given a cache line, we can instantly find what set it belongs to.
  * 8-way means there are 8 locations in the set, and a cache line can be placed in any of these locations.
  * In other words, 8 different places where you can find a cache line.
    ![[Pasted image 20241111232156.png]]

# Shared Address Space Model

![[Pasted image 20241111232736.png]]

## Shared Memory Multi-Processor

* When there are multiple processors:
  * When we read a value at address `X`, it should be the last value written to `X` by any processor.
    ![[Pasted image 20241111232804.png]]

## Cache Coherence Problem

* Modern processors replicate contents of memory in local caches.
* Cache coherence problem ⟶ processors can observe different values for the same memory location because it reads from local cache.
  ![[Pasted image 20241111233059.png]]

### Example

* Suppose `X` is initially 0.
  * P1 loads `X` and P2 loads `X` ⟶ `X=0` goes into their cache
  * Then if P1 writes `X=1` to memory, P2 still has `X=0` in its cache
  * If P3 loads `X`, it will also get `X=0`.
  * When P1 loads `Y`, it needs to evict `X` from its cache. Since its a write-back cache, it writes `X=1` to memory.
* Now all processors see different values for `X`
  ![[Pasted image 20241111233240.png]]

### Solution

![[Pasted image 20241111233753.png]]
![[Pasted image 20241111233816.png]]
![[Pasted image 20241111233825.png]]
