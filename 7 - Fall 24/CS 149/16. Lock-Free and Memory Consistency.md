
# Blocking Algorithms/Data Structures
* A blocking algorithm **allows one thread to prevent other threads from completing operations** on a shared data structure indefinitely.
* Example:
	* Thread 0 takes a lock on a node in our linked list.
	* Thread 0 is swapped out by the OS, or crashes, or just very slow
	* Now, no other threads can complete operations on the data structure (thread 0 not making progress, nothing else making progress either)
* **Key Idea:** an algorithm that uses locks is **blocking** regardless of the implementation of the lock.
# Lock-free Algorithms
* Non-blocking algorithms are lock-free is **some** thread is guaranteed to make progress ("systemwide progress")
	* In lock-free case, it is not possible to preempty one thread such that progress is prevented system-wide.
	* **Note:** this does not prevent starvation of threads, just that at least one thread is making progress.

## Circular Buffer Queue
* Suppose one thread is `push` and one thread is `pop`
* Notice that `push` updates `head`, while `pop` updates `tail`.
	* No two threads are accessing the same thing (as long as the `head` is not equal to the `tail`)
	* This means we don't need any synchronization.
![[Pasted image 20241126144523.png]]

## Unbounded Queue
* The main idea is that the pusher:
	* Allocates new things in the queue.
	* Deletes the nodes marked reclaim.
* The popper removes stuff from the queue.
	* It does not **delete** anything, just marks it as reclaim.
* Only the pusher allocates and deletes memory (thread-safe).
![[Pasted image 20241126144720.png]]
![[Pasted image 20241126144846.png]]

## Lock-free Stack
* When we push:
	1. Take the old top of the stack.
	2. Connect the new node to the top of the stack.
	3. Before updating `s->top`, check if the `s->top` is the same as the one we saw in step 1.
		* If so, update `s->top` and return.
		* If not, try again in the while loop (push to top again).
* Same thing in pop using `compare_and_swap`.
* Comparison to lock:
	* With lock, you don't begin operation on data structure until you're sure no one else is working on it.
	* In this case, you just go for it. Before you commit your update, check if somebody else modified it, and if so, retry.
* **Note:** this solution **does not work** because of the ABA problem.
![[Pasted image 20241126145302.png]]

### ABA Problem
* Suppose thread 0 starts `pop`, but has not made `B` the new top.
* Suppose thread 1:
	* starts and finishes an entire `pop` operation.
	* pushes a new node onto the stack
	* pushes the value that it popped back onto the stack
* Now when thread 0 commits, it sees that `old_top = new_top`, so it commits ⟶ corrupted structure.
![[Pasted image 20241126145720.png]]

### Solution 1: Counter
* Not only do we check if the top is the same, we also check if the `pop_count` is the same (ensuring no other structure modified it)
![[Pasted image 20241126150204.png]]

## Lock-free Linked List
![[Pasted image 20241126150537.png]]
![[Pasted image 20241126150546.png]]

## Summary
* For some data structures, lock-free does not necessarily mean faster.
* Good fine-grained locking implementations can be as fast and much easier to implement.
![[Pasted image 20241126150740.png]]
![[Pasted image 20241126150757.png]]


# Shared Memory Behavior
* Intuition says loads should return latest value written.
	* What does **latest** mean?
	* **Coherence:** maintains this property for ==one memory location==.
		* P1 writes to X, P2 reads X, P2 sees P1's write
	* **Consistency:** ordering of writes to ==all memory locations==
		* P1 writes X and Y, P2 writes Z
* This affects:
	* **Programmability:** how programmers reason about program behavior
		* Allowed behavior of multithreaded programs executing with shared memory
	* **Performance:** limits the HW/SW optimizations that can be used
		* E.g. Reordering memory operations to hide latency
# Coherence vs. Consistency
![[Pasted image 20241126152355.png]]
* The goal of cache coherence is to **ensure the memory system** in a parallel system behaves as if the caches were not there.
	* A system without caches would not need coherence.
* Memory consistency defines the **allowed behavior of loads and stores** to **different** addresses in a parallel system.
	* This needs to be specified regardless of whether or not caches are present.

# Memory Operation Ordering
![[Pasted image 20241126152547.png]]

## Sequential Consistency
![[Pasted image 20241126152943.png]]
* **Sequential Consistency:** Each thread's operations happen in program order.
	* You can put all the operations of each thread on a timeline.
	* If one thread views a sequence of operations, all other operations must agree that their observations are consistent with that.
![[Pasted image 20241126153039.png]]

# Relaxed Memory Operation Ordering
## Relaxation: Allow Reads to Move ahead of Writes
* In original case, a write to a value `X` must finish before a read to value `Y`.
	* Notice that these are **independent instructions**.
	* **Relaxation**: allow read `Y` to happen before write `X` finishes.
		* i.e. remove the $W_x \to R_y$ requirement.
![[Pasted image 20241126153356.png]]

### Optimization: Write Buffer
* The instructions in each processor is independent. **Suppose writing takes a long time.**
* Write buffer: asynchronously write to a buffer, and begin the read before the write finishes.
![[Pasted image 20241126195932.png]]
* Write buffers change memory behavior
	* In the case below, the instructions for each processor can be reordered (because they are ILP).
	* Proc 0 can run instruction 2 before 1 ⟶ this means `r1 = r2` can be `0`.
![[Pasted image 20241126202722.png]]
![[Pasted image 20241126200219.png]]
* Write buffers are much more performant.
* Every modern processor uses them write-buffers
	* Intel x86, ARM, SPARC
* **Con:** Need a weaker memory model
	* TSO: Total Store Order
	* Slightly harder to reason about than sequentially consistent
	* x86 uses an incompletely specified form of TSO

### TSO and PC
![[Pasted image 20241126200552.png]]

### Consistency and Coherency
* Relaxed consistency involves what is **perceived by different processors**, not the same processor.
* If a **write** of variable `X by thread T1 on processor P1` comes before a **read** of `X by T1 on processor P1`, then the read will reflect that write.
	* This is because the thread respects program order within itself.
	* This must be the case, otherwise the processor isn't executing the program correctly.
	* This is unrelated to memory consistency.
* If a **write** of variable `X by thread T1 on processor P1` comes sufficient time before a **read** of `X by T2 on different processor P2`, then **MEMORY COHERENCE** guarantees the read with reflect the write.
	* This is because we're talking about reads and writes to the **same variable**.
* Memory consistency governs the relationship between reads/writes to **different addresses**.
	* TSO relaxes read after write, meaning `P1 can perform a later read of Y` BEFORE `P1 has finished a prior write X`.
	* In other words, P1 cannot assume other processors can observe the effect of the write to X when P1 reads Y.
* TSO is strict in one way: it provides the guarantee that once some other processor P2 observes the write to X, then it can assume all processors have also observed the write to X.
	* In other words when P2 reads X and gets an updated value 1, it can assume that reads of X on any processor at this point will return 1.
	* In other models like processor consistency (PC), even that is not guaranteed.

![[Pasted image 20241126200704.png]]

## Relaxation: Allow Writes to Be Reordered
* Consider a write to `X` followed by a write to `Y`.
	* These are independent instructions, so technically we can reorder them.
![[Pasted image 20241126201212.png]]

## Why Are Relaxations Useful?
* All of these optimizations are perfectly valid on single-thread systems.
![[Pasted image 20241126201505.png]]
* If we throw all reorderings away:
	* There is guarantee that one thread might see memory operations in the same order as another thread.
	* Some ARM systems does this ⟶ very good single-threaded performance, but requires **synchronizations** for parallel programs/
![[Pasted image 20241126201546.png]]


## Synchronization
* There are additional instructions like **memory fence** to allow programmers to ensure memory consistency.
![[Pasted image 20241126201659.png]]
![[Pasted image 20241126201749.png]]

## Data Races
* Data races happen when **multiple threads access the same memory location**, where at least one thread writes.
* Data races can lead to inconsistent results due to:
	* Different processors potentially seeing memory operations in different orders
	* Write buffers causing delays between when a write happens and when other processors see it
	* Compiler optimizations reordering memory operations
* Examples of data races:
	* One thread writing to a variable while another thread reads it without synchronization
	* Multiple threads incrementing a counter without atomic operations
	* Two threads writing to the same memory location concurrently
* To prevent data races, we need proper synchronization:
  * Using locks to ensure mutual exclusion
	* Memory fences to enforce ordering of operations
	* Atomic operations for simple operations like increments
	* Following a consistent memory model like TSO or sequential consistency

![[Pasted image 20241126201758.png]]
![[Pasted image 20241126201817.png]]

## Synchronized Program
* Programs that are synchronized yield sequentially consistent results.
	* We did synchronization via locks, etc.
	* Locks are often implemented with memory fences to ensure consistency
![[Pasted image 20241126201912.png]]

## Summary: Relaxed Consistency
![[Pasted image 20241126202052.png]]

# Compiler Optimizations
* Sometimes compliers might optimize useless instructions out.
![[Pasted image 20241126202211.png]]
* But this leads to inconsistency in multithreaded programs.
	* Green is if thread 2 did not exist. Red is if both threads are running.
![[Pasted image 20241126202233.png]]

## Language-level Memory Models
![[Pasted image 20241126202330.png]]

# Summary: Memory Consistency Models
![[Pasted image 20241126202413.png]]

# Domain Specific Languages
* **Main motivation:** Why can't the compiler do most of the parallelization for us?
	* Create an abstraction that gets performance and productivity (easy to write)
	* We can give up generality (ability to write arbitrary programs)
![[Pasted image 20241126203001.png]]

> [!NOTE] DSL Hypothesis
> If you stay within the DSL, it is possible to write one program and also run it efficiently on a range of heterogeneous parallel systems (the compiler/abstraction can take care of it for you).

* **Domain Specific Languages (DSLs)**
	* Programming language with **restricted expressiveness** for a particular domain
	* High-level, usually declarative, and deterministic
![[Pasted image 20241126203218.png]]

## Example: Halide
* Halide is a DSL for image processing ⟶ used to implement camera processing pipelines
	* HDR+, aspects of portrait mode, industry usage, etc

### Naive Convolution Code
* This code does a 3x3 image blur
![[Pasted image 20241126203407.png]]

### Optimization: Two-pass Blur
* In the case of blur, a 2D separable filter (such as a box filter) can be evaluated as two 1D filtering operations.

* **Pro: we now do much less work.**
![[Pasted image 20241126203523.png]]

* **Con: we pay via the extra storage (we actually reduce our arithmetic intensity)**
![[Pasted image 20241126203722.png]]

### Optimization: Chunking and Fusing
* For each row output:
	* We compute 3 rows of horizontal blurs ⟶ creates `3 x N` tensor
	* We then compute a vertical blur on that ⟶ creates `1 x N` tensor (i.e. row of output)
* This program does **more work** than original version, but our arithmetic intensity is higher (more work, same memory usage)
![[Pasted image 20241126203906.png]]

### Optimization: Using a Bigger Chunk
* We use we load in `CHUNK_SIZE + 2` rows to create `CHUNK_SIZE` rows of output.
![[Pasted image 20241126204147.png]]

### Fully Optimized C++ Code
* This is extremely hard to read, but its very efficient.
![[Pasted image 20241126204335.png]]

### Halide DSL
* This is a loop-free language that can be used to describe operations on images (e.g. blurs)
* `blurx(x, y)` is the horizontal pass, and `blury(x, y)` is the vertical pass.
	* You can think of these as RDDs ⟶ only materialized when we call `out.realize`
![[Pasted image 20241126204356.png]]
* **Main Idea:** the code looks a lot like how you would intuitively think about the operation.
![[Pasted image 20241126204729.png]]
![[Pasted image 20241126204735.png]]

* If we thought of Halide as C++ code with arrays, we would basically have temp buffers for each RDD step.
![[Pasted image 20241126204844.png]]

* Halide allows programmers to easily express their programs, and have the system decide on an efficient implementation.
![[Pasted image 20241126204911.png]]
### Halide Scheduling
* Halide code also offers scheduling primitives to allow programmers to sketch out how the computer should schedule it.
![[Pasted image 20241126205014.png]]

* We can also use tell it to **not fuse**
![[Pasted image 20241126205211.png]]

* We could also tell it to **fuse it at a specific point**
![[Pasted image 20241126205227.png]]
![[Pasted image 20241126205259.png]]

### Philosophy of Halide
![[Pasted image 20241126205509.png]]
