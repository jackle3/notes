
# Blocking Algorithms/Data Structures
* A blocking algorithm is one where **one thread can prevent other threads from making progress indefinitely** on a shared data structure.
* Example:
	* Thread 0 acquires a lock on a node in a linked list
	* Thread 0 gets swapped out by the OS, crashes, or becomes very slow
	* No other threads can make progress since they need the lock held by Thread 0
* **Key Point:** Any algorithm using locks is inherently blocking, regardless of lock implementation

# Lock-free Algorithms
* A non-blocking algorithm is considered lock-free if **at least one thread** is guaranteed to make progress ("systemwide progress")
	* Even if threads are preempted, some thread will always be able to make progress
	* Note: Individual threads may still experience starvation, but the system as a whole continues to make progress

## Circular Buffer Queue
* Consider a queue with one thread doing `push` and another doing `pop`
* `push` updates `head`, while `pop` updates `tail`
	* As long as `head != tail`, the threads operate on different parts of the queue
	* This means no synchronization is needed in the non-full/empty cases
![Pasted image 20241126144523](attachments/Pasted%20image%2020241126144523.png)

## Unbounded Queue
* Key design principles:
	* Push thread:
		* Handles all memory allocation
		* Deletes nodes marked for reclamation
	* Pop thread:
		* Marks nodes for reclamation but doesn't delete them
	* Memory safety achieved by having only one thread (pusher) handle allocation/deallocation
![Pasted image 20241126144720](attachments/Pasted%20image%2020241126144720.png)
![Pasted image 20241126144846](attachments/Pasted%20image%2020241126144846.png)

## Lock-free Stack
* Push operation:
	1. Read current top of stack
	2. Point new node to current top
	3. Use compare-and-swap to update top:
		* If top hasn't changed, update succeeds
		* If top has changed, retry operation (push to new top)
* Pop uses similar compare-and-swap approach
* Contrast with locks:
	* Locks: Wait for exclusive access before starting
	* Lock-free: Attempt operation, verify no interference before committing
* **Important:** This basic implementation has the ABA problem
![Pasted image 20241126145302](attachments/Pasted%20image%2020241126145302.png)

### ABA Problem
* Scenario that breaks the basic implementation:
	1. Thread 0 starts pop, reads top pointing to A
	2. Thread 1 completes:
		* Pops A
		* Pushes new node B
		* Pushes A back
	3. Thread 0 resumes, sees top still points to A
	4. Thread 0 incorrectly succeeds compare-and-swap, corrupting structure
![Pasted image 20241126145720](attachments/Pasted%20image%2020241126145720.png)

### Solution 1: Counter
* Add a counter that increments with each modification
* Compare-and-swap checks both pointer and counter
* Ensures detection of intermediate modifications
![Pasted image 20241126150204](attachments/Pasted%20image%2020241126150204.png)

## Lock-free Linked List
![Pasted image 20241126150537](attachments/Pasted%20image%2020241126150537.png)
![Pasted image 20241126150546](attachments/Pasted%20image%2020241126150546.png)

## Summary
* Lock-free doesn't automatically mean better performance
* Well-implemented fine-grained locking can match performance while being simpler to implement
![Pasted image 20241126150740](attachments/Pasted%20image%2020241126150740.png)
![Pasted image 20241126150757](attachments/Pasted%20image%2020241126150757.png)


# Shared Memory Behavior
* Intuition suggests loads to an address should return the latest written value
	* What defines "latest" in a parallel system?
	* **Coherence:** Maintains consistency for a single memory location
		* P1 writes to X, P2 reads X, P2 sees P1's write
	* **Consistency:** Defines ordering of operations across all memory locations
		* Handles ordering between operations on different variables (P1 writes X and Y, P2 writes Z and reads X)

This affects two key aspects:
* **Programmability:** How developers reason about parallel program behavior
* **Performance:** What optimizations are allowed in hardware/software

# Coherence vs. Consistency
![Pasted image 20241126152355](attachments/Pasted%20image%2020241126152355.png)
* Cache coherence ensures the memory system behaves as if caches didn't exist
	* Only needed in parallel systems with caches
* Memory consistency defines allowed behaviors for loads/stores to different addresses
	* Required in any parallel system, with or without caches

# Memory Operation Ordering
![Pasted image 20241126152547](attachments/Pasted%20image%2020241126152547.png)

## Sequential Consistency
* Defines the strictest memory model:
	* All operations from each thread appear to execute in program order
	* All threads observe the same global ordering of operations
![Pasted image 20241126152943](attachments/Pasted%20image%2020241126152943.png)
![Pasted image 20241126153039](attachments/Pasted%20image%2020241126153039.png)

# Relaxed Memory Operation Ordering
## Relaxation: Allow Reads to Move ahead of Writes
* Each thread still executes its own instructions in program order.
* However, other threads may observe memory operations in a different order than program order.
* Example: When a thread writes to `X` and then reads `Y`:
	* The thread itself may reorder these operations if they are independent instructions (ILP), but it will appear to execute them in program order from its own perspective
	* But other threads may observe the read of `Y` before they see the write to `X` complete
	* This is because these are **independent instructions**
	* **Relaxation**: From other threads' perspective, the read of `Y` may appear to happen before write to `X` finishes
		* i.e. remove the $W_x \to R_y$ ordering requirement between threads
![Pasted image 20241126153356](attachments/Pasted%20image%2020241126153356.png)

### Optimization: Write Buffer
* The instructions in each processor is independent. **Suppose writing takes a long time.**
* Write buffer: asynchronously write to a buffer, and begin the read before the write finishes.
![Pasted image 20241126195932](attachments/Pasted%20image%2020241126195932.png)
* Write buffers change memory behavior
	* In the case below, the instructions for each processor can be reordered (because they are ILP).
	* Proc 0 can run instruction 2 before 1 ⟶ this means `r1 = r2` can be `0`.
![Pasted image 20241126202722](attachments/Pasted%20image%2020241126202722.png)
![Pasted image 20241126200219](attachments/Pasted%20image%2020241126200219.png)
* Write buffers are much more performant.
* Every modern processor uses them write-buffers
	* Intel x86, ARM, SPARC
* **Con:** Need a weaker memory model
	* TSO: Total Store Order
	* Slightly harder to reason about than sequentially consistent
	* x86 uses an incompletely specified form of TSO

### TSO and PC
![Pasted image 20241126200552](attachments/Pasted%20image%2020241126200552.png)

### Consistency and Coherency
* Relaxed consistency involves what is **perceived by different processors**, not the same processor.
* If a **write** of variable `X by thread T1 on processor P1` comes before a **read** of `X by T1 on processor P1`, then the read will reflect that write.
	* This is because the thread respects program order within itself.
	* This must be the case, otherwise the processor isn't executing the program correctly.
	* This is unrelated to memory consistency.
* If a **write** of variable `X by thread T1 on processor P1` comes sufficient time before a **read** of `X by T2 on different processor P2`, then **MEMORY COHERENCE** guarantees the read with reflect the write.
	* This is because we're talking about reads and writes to the **same variable**.
* Memory consistency governs the relationship between reads/writes to **different addresses**.
	* TSO relaxes read after write, meaning `P1 can perform a later read of Y` BEFORE `P1 has finished a prior write X`.
	* In other words, P1 cannot assume other processors can observe the effect of the write to X when P1 reads Y.
* TSO is strict in one way: it provides the guarantee that once some other processor P2 observes the write to X, then it can assume all processors have also observed the write to X.
	* In other words when P2 reads X and gets an updated value 1, it can assume that reads of X on any processor at this point will return 1.
	* In other models like processor consistency (PC), even that is not guaranteed.

![Pasted image 20241126200704](attachments/Pasted%20image%2020241126200704.png)

## Relaxation: Allow Writes to Be Reordered
* Consider a write to `X` followed by a write to `Y`.
	* These are independent instructions, so technically we can reorder them.
![Pasted image 20241126201212](attachments/Pasted%20image%2020241126201212.png)

## Why Are Relaxations Useful?
* All of these optimizations are perfectly valid on single-thread systems.
![Pasted image 20241126201505](attachments/Pasted%20image%2020241126201505.png)
* If we throw all reorderings away:
	* There is guarantee that one thread might see memory operations in the same order as another thread.
	* Some ARM systems does this ⟶ very good single-threaded performance, but requires **synchronizations** for parallel programs/
![Pasted image 20241126201546](attachments/Pasted%20image%2020241126201546.png)


## Synchronization
* There are additional instructions like **memory fence** to allow programmers to ensure memory consistency.
![Pasted image 20241126201659](attachments/Pasted%20image%2020241126201659.png)
![Pasted image 20241126201749](attachments/Pasted%20image%2020241126201749.png)

## Data Races
* Data races happen when **multiple threads access the same memory location**, where at least one thread writes.
* Data races can lead to inconsistent results due to:
	* Different processors potentially seeing memory operations in different orders
	* Write buffers causing delays between when a write happens and when other processors see it
	* Compiler optimizations reordering memory operations
* Examples of data races:
	* One thread writing to a variable while another thread reads it without synchronization
	* Multiple threads incrementing a counter without atomic operations
	* Two threads writing to the same memory location concurrently
* To prevent data races, we need proper synchronization:
  * Using locks to ensure mutual exclusion
	* Memory fences to enforce ordering of operations
	* Atomic operations for simple operations like increments
	* Following a consistent memory model like TSO or sequential consistency

![Pasted image 20241126201758](attachments/Pasted%20image%2020241126201758.png)
![Pasted image 20241126201817](attachments/Pasted%20image%2020241126201817.png)

## Synchronized Program
* Programs that are synchronized yield sequentially consistent results.
	* We did synchronization via locks, etc.
	* Locks are often implemented with memory fences to ensure consistency
![Pasted image 20241126201912](attachments/Pasted%20image%2020241126201912.png)

## Summary: Relaxed Consistency
![Pasted image 20241126202052](attachments/Pasted%20image%2020241126202052.png)

# Compiler Optimizations
* Sometimes compliers might optimize useless instructions out.
![Pasted image 20241126202211](attachments/Pasted%20image%2020241126202211.png)
* But this leads to inconsistency in multithreaded programs.
	* Green is if thread 2 did not exist. Red is if both threads are running.
![Pasted image 20241126202233](attachments/Pasted%20image%2020241126202233.png)

## Language-level Memory Models
![Pasted image 20241126202330](attachments/Pasted%20image%2020241126202330.png)

# Summary: Memory Consistency Models
![Pasted image 20241126202413](attachments/Pasted%20image%2020241126202413.png)

# Domain Specific Languages
* **Main motivation:** Why can't the compiler do most of the parallelization for us?
	* Create an abstraction that gets performance and productivity (easy to write)
	* We can give up generality (ability to write arbitrary programs)
![Pasted image 20241126203001](attachments/Pasted%20image%2020241126203001.png)

> [!NOTE] DSL Hypothesis
> If you stay within the DSL, it is possible to write one program and also run it efficiently on a range of heterogeneous parallel systems (the compiler/abstraction can take care of it for you).

* **Domain Specific Languages (DSLs)**
	* Programming language with **restricted expressiveness** for a particular domain
	* High-level, usually declarative, and deterministic
![Pasted image 20241126203218](attachments/Pasted%20image%2020241126203218.png)

## Example: Halide
* Halide is a DSL for image processing ⟶ used to implement camera processing pipelines
	* HDR+, aspects of portrait mode, industry usage, etc

### Naive Convolution Code
* This code does a 3x3 image blur
![Pasted image 20241126203407](attachments/Pasted%20image%2020241126203407.png)

### Optimization: Two-pass Blur
* In the case of blur, a 2D separable filter (such as a box filter) can be evaluated as two 1D filtering operations.

* **Pro: we now do much less work.**
![Pasted image 20241126203523](attachments/Pasted%20image%2020241126203523.png)

* **Con: we pay via the extra storage (we actually reduce our arithmetic intensity)**
![Pasted image 20241126203722](attachments/Pasted%20image%2020241126203722.png)

### Optimization: Chunking and Fusing
* For each row output:
	* We compute 3 rows of horizontal blurs ⟶ creates `3 x N` tensor
	* We then compute a vertical blur on that ⟶ creates `1 x N` tensor (i.e. row of output)
* This program does **more work** than original version, but our arithmetic intensity is higher (more work, same memory usage)
![Pasted image 20241126203906](attachments/Pasted%20image%2020241126203906.png)

### Optimization: Using a Bigger Chunk
* We use we load in `CHUNK_SIZE + 2` rows to create `CHUNK_SIZE` rows of output.
![Pasted image 20241126204147](attachments/Pasted%20image%2020241126204147.png)

### Fully Optimized C++ Code
* This is extremely hard to read, but its very efficient.
![Pasted image 20241126204335](attachments/Pasted%20image%2020241126204335.png)

### Halide DSL
* This is a loop-free language that can be used to describe operations on images (e.g. blurs)
* `blurx(x, y)` is the horizontal pass, and `blury(x, y)` is the vertical pass.
	* You can think of these as RDDs ⟶ only materialized when we call `out.realize`
![Pasted image 20241126204356](attachments/Pasted%20image%2020241126204356.png)
* **Main Idea:** the code looks a lot like how you would intuitively think about the operation.
![Pasted image 20241126204729](attachments/Pasted%20image%2020241126204729.png)
![Pasted image 20241126204735](attachments/Pasted%20image%2020241126204735.png)

* If we thought of Halide as C++ code with arrays, we would basically have temp buffers for each RDD step.
![Pasted image 20241126204844](attachments/Pasted%20image%2020241126204844.png)

* Halide allows programmers to easily express their programs, and have the system decide on an efficient implementation.
![Pasted image 20241126204911](attachments/Pasted%20image%2020241126204911.png)
### Halide Scheduling
* Halide code also offers scheduling primitives to allow programmers to sketch out how the computer should schedule it.
![Pasted image 20241126205014](attachments/Pasted%20image%2020241126205014.png)

* We can also use tell it to **not fuse**
![Pasted image 20241126205211](attachments/Pasted%20image%2020241126205211.png)

* We could also tell it to **fuse it at a specific point**
![Pasted image 20241126205227](attachments/Pasted%20image%2020241126205227.png)
![Pasted image 20241126205259](attachments/Pasted%20image%2020241126205259.png)

### Philosophy of Halide
![Pasted image 20241126205509](attachments/Pasted%20image%2020241126205509.png)
