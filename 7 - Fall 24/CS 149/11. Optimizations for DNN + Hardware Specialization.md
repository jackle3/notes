	
# Fusion Optimizations
* The direct implementation was:
	* For all images in batch, for all pixels in image, for all output channels (filters), for each dimension of the convolution, compute the convolution.
![Pasted image 20241110160827](attachments/Pasted%20image%2020241110160827.png)
* We can improve this as a dense matrix-matrix multiplication, which is highly parallelizable.
![Pasted image 20241110160930](attachments/Pasted%20image%2020241110160930.png)
* The pitfall is that **the dense matrix is much larger than the input data**.
	* Implicit GEMM ⟶ when people implement this, they think of it as dense matrix multiplication but never actually materialize the matrix.
		* Materialize only sub-blocks of matrices at a time on the GPU's shared memory.
	* Intel and NVIDIA have high performance low-level implementations of key DNN layers
![Pasted image 20241110161411](attachments/Pasted%20image%2020241110161411.png)

## Memory Traffic Between Operations
![Pasted image 20241110161824](attachments/Pasted%20image%2020241110161824.png)
![Pasted image 20241110161903](attachments/Pasted%20image%2020241110161903.png)
* If we wanted to fuse a **max pool** operation with this layer, we can:
	* Max pool is max of 2x2 blocks of output. matrix
	* We would modify the yellow loops to go through 2x2 blocks, compute it for that block, and write the max of the computed values.

## Transformers Attention
* **Matrix multiplication** is at the heart of the attention blocks ⟶ we can do the fusion trick to compute attention.
* Input is a sequence, output is another sequence.
	* E.g. input is a sentence, output is sentence + next word.
![Pasted image 20241110162106](attachments/Pasted%20image%2020241110162106.png)

### Attention Module
* Transformer blocks are just a set of matrix multiplications on the input sequences.
* $d$ is the dimension of the embedding of each token.
* $N$ can be pretty large when the sequences are long.
![Pasted image 20241110162200](attachments/Pasted%20image%2020241110162200.png)
* Suppose the inputs are $Q$ and $K$. This is $O(n)$ storage.
* Once we multiply them to make $S$, we have $O(n^2)$ storage.
* Then to make $P$, we need to run a softmax on each row.
	* This is a wide dependency ⟶ softmax needs to materialize the entire row.
* Then you multiple by $v$ to make the result vector $O$ in $O(n)$ storage.
![Pasted image 20241110162340](attachments/Pasted%20image%2020241110162340.png)

* **Wide dependency** ⟶ how to efficiently compute this without having the materialize the entire array?
* **Solution:** compute softmax in chunks. If we know the max $m$ of the first half and the second half, we trivially know the max of the vector.
![Pasted image 20241110162616](attachments/Pasted%20image%2020241110162616.png)
* We can do fused attention so that we never materialize the entire matrix.
![Pasted image 20241110162828](attachments/Pasted%20image%2020241110162828.png)

## Compiler Fusion
* Nowadays, compiler generates new implementations that fuse multiple operations into a single node that executes efficiently (without overhead of storing or communication intermediate results in memory).
* CUDNN can fuse a convolution with any two pointwise operators.

## Another Trick: Use of Low Precision Values
![Pasted image 20241110163223](attachments/Pasted%20image%2020241110163223.png)

# Hardware Specialization
* Computing is constrained by energy (power x time)
	* Supercomputers are energy constrained
		* Due to shear scale of machine
		* Overall cost to operate (power for machine and for cooling
	* Data centers are energy constrained
		* Reduce cost of cooling
		* Reduce physical space
	* Mobile devices are energy constrained
		* Limited battery life
		* Heat dissipation without fan
* In **Dennard scaling**, as transistors shrink, they require less voltage and current, which offsets the increased number of transistors on a chip. This keeps the **total power consumption constant**, even as chips become more densely packed.
	* If we want to improve performance, we have to make it more energy efficient.
	* To do so, we can do **specialization** ⟶ specialize the hardware to make it more energy efficient (get rid of extraneous hardware)
![Pasted image 20241110165007](attachments/Pasted%20image%2020241110165007.png)

* For specialized hardware, there is a fundamental tradeoff between energy efficiency and programmability.

## General-purpose Processor
* These multi-core CPUs and GPUs that are able to execute any combination of instructions are "inefficient".
* Suppose your operation is to `add r1 r2 r3`.
	* The actual arithmetic is only ~6% of the energy consumption.
	* The rest is overhead.
![Pasted image 20241110165727](attachments/Pasted%20image%2020241110165727.png)

### SIMD Improvement
* One of the ways of increasing the green component was to do SIMD execution:
	* This goes through the same process of instruction and data supply, but you do more arithmetic (SIMD vector)
![Pasted image 20241110165916](attachments/Pasted%20image%2020241110165916.png)

## ASIC
* To improve performance even further, you can take your algorithm and burn it directly onto your silicon ⟶ fixed function ASIC.
* In the figure below ASIC is the implementation burned onto chip.
![Pasted image 20241110170052](attachments/Pasted%20image%2020241110170052.png)

### Anton Supercomputer
* Used for molecular dynamics, simulates time evolution of proteins (protein folding)
	* You figure this out via an N-body simulation
* Anton was ASIC for computing particle-particle interactions (512 of them in machine)
	* Throughput-oriented subsystem for efficient fast-fourier transforms
	* This chip/machine can only execute a single algorithm
	* Custom, low-latency communication with network designed for communication patterns of N-body simluations
* Distributed machine that is very capable but can only be used for protein folding.

### TPUs
* Google's Tensor Processing Unit ⟶ accelerates deep learning operations
* Specialized processors (sometimes ASIC) for evaluating deep networks


## Digital Signal Processors (DSPs)
* Domain specific programmable processors ⟶ focused on a particular application area (in this case, signals)
* Simpler instruction control paths
* Usually uses  VLIW ⟶ very long instruction word
	* A complex instruction that can do a lot of work in a single cycle
* Because of how difficult it is to comprehend these instructions:
	* Writing software for these processors are usually done at low-level assembly code
	* Super hard to make a compiler for languages like C that produces these optimized instructions.
![Pasted image 20241110171916](attachments/Pasted%20image%2020241110171916.png)

## FPGAs (Field Programmable Gate Arrays)
* Middle ground between an ASIC and a processor
* FPGA chip provides array of logic blocks, connected by interconnect
* Programmer-defined logic implemented by FPGA
![Pasted image 20241110193919](attachments/Pasted%20image%2020241110193919.png)
![Pasted image 20241110193932](attachments/Pasted%20image%2020241110193932.png)

### Modern FPGAs
* A lot of area now is devoted to hard gates. These can be:
	* Dense memory blocks (SRAM)
	* DSP blocks (multipliers/arithmetic units)
![Pasted image 20241110194050](attachments/Pasted%20image%2020241110194050.png)
* Downsides of FPGA compared to ASIC:
	* Less energy efficient ⟶ its more programmable (array of units can be reconfigured)
	* Less performant than a fixed-function ASIC; programs need to be very optimized
* Advantages of FPGA:
	* Easier to program ⟶ you don't need to wait for the frabrication costs and time associated with burning in a fixed function ASIC

## Efficiency Benegits of Compute Specialization
![Pasted image 20241110194818](attachments/Pasted%20image%2020241110194818.png)
## Summary
![Pasted image 20241110194933](attachments/Pasted%20image%2020241110194933.png)

# Hardware for DNN
**Why might a GPU be a good platform for DNN evaluation?**
* Consider arithmetic intensity, SIMD, data-parallelism, memory bandwidth requirements
* In order to implement a DNN efficiently, you need:
	* Highly data-parallel implementations of dense matrix multiplication
	* Improving locality using fusion of operations
![Pasted image 20241110195309](attachments/Pasted%20image%2020241110195309.png)

**Why might a GPU be a sub-optimal platform for DNN evaluation?**
* Many functions are not necessarily for DNN evaluation ⟶ DNN usually only does arithmetic and matrix multiplications.
* We can do hardware specialization (e.g. TPUs) to create much more efficient DNNs

## Special Instruction Support
* **Goal:** do more arithmetic for each instruction that you fetch ⟶ decreases overhead
	* Complex instructions have lower overhard because they do more per fetch
![Pasted image 20241110195548](attachments/Pasted%20image%2020241110195548.png)

## Numerical Data Format
* For machine learning, the higher the floating point precision, the smaller the accuracy.
![Pasted image 20241111000338](attachments/Pasted%20image%2020241111000338.png)
* Each of these precisions have different energy costs.
	* However notice that the arithmetic costs are much slower than reading from RAM
	* By using smaller data types, you save both energy and silicon area
![Pasted image 20241111000405](attachments/Pasted%20image%2020241111000405.png)

## Hardware Acceleration for DNN
* The above is the motivation for having hardware acceleration
![Pasted image 20241111000558](attachments/Pasted%20image%2020241111000558.png)
* They all focus on the idea of **improving matrix multiply capability**
![Pasted image 20241111000712](attachments/Pasted%20image%2020241111000712.png)
