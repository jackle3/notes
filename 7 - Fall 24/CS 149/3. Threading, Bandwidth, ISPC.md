
# Accessing Memory
* Accessing memory leads to a lot of unavoidable latency.

## Cache
* Memory is just a big array of bytes that are accessible via address.
* Caches reduce length of stalls (reduce memory access latency) **when accessing data that have been recently accessed.**
![[Pasted image 20241001151608.png]]

## Prefetching
* Modern processors perform **data prefetching** (hides the latency)
	* It guesses what data will be accessed based on memory access patterns and pre-fetches it into the cache.
![[Pasted image 20241001151935.png]]

# Multi-threading
* **Interleave** processing of multiple threads on the same core to hide stalls.
	* If you can't make progress on the current thread (because of IO), work on another.

* For example, you can make one core have four hardware threads, so it has 4 register files.

| ![[Pasted image 20241001152340.png\|250]] | ![[Pasted image 20241001152348.png\|250]] | ![[Pasted image 20241001153557.png\|350]] |
| ----------------------------------------- | ----------------------------------------- | ----------------------------------------- |

* With this, we can run four threads and interleave them once each thread stalls.
	* This allows the processor to have 100% utilization -> at every point in time, there is a thread running.
![[Pasted image 20241001152504.png]]

## Trade-offs
* One major tradeoff is throughput. You are increasing the time for one thread to complete, in exchange for increasing overall system throughput.
![[Pasted image 20241001152612.png]]

* Another tradeoff is resources. The on-chip storage for registers/execution context (usually the L1 cache) is finite.

## Utilization

**Case 1:** We perform three real instructions followed by 12-cycle latency from memory load.
* Suppose we have a single-core CPU each with varying amounts of hardware threads.

* The utilization of the core with **one thread** is 20%.
![[Pasted image 20241001153334.png]]

* The utilization of the core with **two threads** is 40%.
![[Pasted image 20241001153353.png]]

* We need **five threads** to reach 100% utilization. Additional threads yield no benefit.
![[Pasted image 20241001153412.png]]

**Case 2:** We perform **six real instructions** followed by 12-cycle latency.
![[Pasted image 20241001153938.png]]
* **Key Idea:** the ratio of math instructions to memory latency affects the number of threads needed for latency hiding.

## Takeaway

**Point 1:** A processor with multiple hardware threads can avoid stalls by performing instructions from other threads when one thread must wait for a long latency operation.
* With multiprocessing and SIMD, we added resources to **increase the number of instructions we can do per clock**.
* With multithreading, we added resources (in the form of registers) to **better utilize our existing resources**.
	* Note that we do not change the latency of memory; we just make it so that it no longer causes reduced processor utilization.

**Point 2:** A multi-threaded processor hides memory latency by performing arithmetic from other threads.
* Programs that feature more arithmetic per memory access need fewer threads to hide memory stalls.

## Summary
We've been talking about **hardware supported multi threading**
* Core manages execution contexts for multiple threads.
	* Same number of ALU resources; we now use them more efficiently to hide latency.
* Interleaved multi-threading
	* Each clock, the core chooses a thread and runs an instruction from the thread.
* Simultaneous multi-threading
	* Each clock, core chooses instructions from multiple threads to run on ALUs
	* E.g. Intel Hyper-threading (2 threads per core)

![[Pasted image 20241001154651.png]]
* The processor has 16 cores, 8 SIMD ALUs per core, and 4 threads per core.
	* There are 16 **simultaneous** instruction streams (one per core)
	* There are $4 * 16 = 64$ total **concurrent** instruction streams (via threading).
		* 64 threads active but only 16 making progress.
	* Each instruction stream can run 8-wide vector instructions (via SIMD).
	* The total number of work that can be done is $4 * 16 * 8 = 512$ pieces.

**To utilize modern parallel processors efficiently, an application must:**
![[Pasted image 20241001155313.png]]

---
# Bandwidth/Latency
* Suppose only one car can be on a lane of a highway at once. A new car can enter only once the previous car completes.
* The **latency** is an amount of time -> in this case, 0.5 hours for driving from SF to Stanford.
* The **bandwidth/throughput** is a rate -> in this case, 2 cars can arrive per hour.
![[Pasted image 20241001191829.png]]

* To improve throughput, you can drive faster or build more lanes.
	* In the first case, **latency** is now 0.25 hours.
	* In the second case, notice that **latency does not change** but **throughput increases**.
![[Pasted image 20241001192051.png]]

* You can also use the highway more **efficiently**.
	* In either case, the **latency still has not changed** (since the cars are still at 100kph)
	* To **calculate throughput**, think of it as "how frequently does a car arrive at Stanford"?
		* Since cars are spaced by 1km traveling at 100kph, we get 1 car per 1/100 hour.
![[Pasted image 20241001192257.png]]

 ## Memory Bandwidth
* The rate at which the memory system can provide data to the processor.
	* This is really the throughput of the memory system.

* Example:![[Pasted image 20241001192742.png]]
	* Each item has a latency of 2 seconds to transfer from memory to processor.
	* Each lane has a bandwidth of 4 items per second. Having two lanes gives us 8 items per second.

## Laundry Example
* In the example below, we have a throughput of 0.5 loads per hour.![[Pasted image 20241001192940.png]]

* If we double our resources, we can double our throughput.![[Pasted image 20241001193003.png]]

* To increase throughput but not resources, we can pipeline it.![[Pasted image 20241001193137.png]]
	* Reading off the graph, we see that we finish a new load every hour.
		* Note that **throughput assumes steady state** if we were to repeat the job millions of times.
		* Notice how if we only consider the first load of laundry, the throughput is still 0.5 per hour; however it gradually grows to 1 per hour.
	* Furthermore, the dryer and washer both have 100% utilization.
		* However, the washer is outrunning the dryer (gap between washer and dryer is growing).

* The pipeline throughput is limited by the slowest component of the pipe.
![[Pasted image 20241001193454.png]]

## Applying to a computer
![[Pasted image 20241001193708.png]]

* Suppose it takes 8 clocks to transfer data, and there can be 3 outstanding requests.
	* In the diagram below, memory (the blue region) is 100% utilized.
	* Even with memory latency hiding (the gray bar), once we run out of outstanding requests, the program **stalls**.
![[Pasted image 20241001193801.png]]
![[Pasted image 20241001194102.png]]

## Thought Experiment
![[Pasted image 20241001194916.png]]
* For every math operation (`A[i] * B[i]`), it needs to:
	* load two values (load `A[i]` and `B[i]`)
	* and write one value (write `C[i]`)
* The ratio of memory instructions to math instructions is 3:1.
* The ratio of bytes moved to math instructions is 12:1 (assuming `A[i]` is 4 byte float).

* A modern GPU can go 5120 floating point MULs per clock (@ 1.6Ghz)
	* This is $5120 * (1.6 * 10^9 \text{ ops per second}) = 8192 * 10^9 \text{ ops per second}$
	* Since each operation needs to move 12 bytes of data, we need a memory bandwidth of $(8192 * 10^9 \text{ ops per second}) * (12 \text { bytes}) = 98.3 \text{ TB per second}$

## Key Point
* **Key Point:** Even though this computation is very parallel friendly, it's **bandwidth limited**.
	* If processors request data at too high a rate, the memory system cannot keep up.
![[Pasted image 20241001195012.png]]

# Abstractions
* **Semantics** is a form of abstraction
	* What do the operations provided by a program mean?
	* What does it compute?
* In this class we think about **implementation (aka scheduling)**
	* How will the answer be computed on a parallel machine?
	* When (i.e. in what order) and what part (e.g. what thread) of a computer does those operations?
* The goal is to be able to **trace** through what each part of a parallel computer does given a parallel computer program.

# ISPC
* Intel SPMD Program Compiler
	* Compiler designed to supports SPMD programming for parallel programs.
	* A minimal extension for C to allow you to not have to think about SIMD instructions but have the code generate good SIMD code.
* SPMD -> single program multiple data.

## Traditional Program
* Consider the program to compute $sin(x)$ using Taylor expansions for each element of an array. Suppose `main()` calls that function.
![[Pasted image 20241001201331.png]]
* Traditional programs have one thread of control/sequence of instructions.
	* `main` calls `sinx` and transfers control to it.
	* Once `sinx` finished, it returns control to `main`

## ISPC Program
* ISPC is essentially the same as C code, with some very minor changes.
* The most important one is **`programCount`** and **`programIndex`**, which are provided by the ISPC compiler.

* `programCount`: number of simultaneously executing instances in the gang (uniform value) 
* `programIndex`: id of the current **instance** in the gang (a non-uniform value: “varying”) 
* `uniform`: A type modifier. **All instances have the same value for this variable**. Its use is purely an optimization. Not needed for correctness.

| ![[Pasted image 20241005155241.png]] | ![[Pasted image 20241005155246.png]] |
| ------------------------------------ | ------------------------------------ |

### SPMD model
![[Pasted image 20241005162907.png]]

* If we had a `programCount = 8`, we essentially spawn 8 **instances** of `ispc_sinx`.
![[Pasted image 20241001201701.png|600]]

## Abstraction vs. Implementation

**SPMD programming abstraction**:
* Calls to an ISPC function spawns a gang of **program instances** --> all instances run **simultaneously** on **one thread** with SIMD instructions.
	* The **# of instances** is the **SIMD** width of the hardware.
	* Instruction `i` across all instances is executed using a single SIMD instruction.
		* The difference between each instance is that the array indices being worked on are different.
* Upon return of function, all instances have completed.

**ISPC compiler generates SIMD implementation:**
* Number of instances in a gang is the SIMD width of the hardware (or a small multiple of SIMD width)
* ISPC compiler generates a C++ function binary (.o) containing SIMD instructions.


# ISPC Interleaving

## Interleaved assignment
* The ISPC compiler uses an **interleaved** assignment of array elements to each instance.
* It creates `programCount` program instances.
	* When going through list, it skips the loop by `programCount` with an offset of  `programIndex`.

![[Pasted image 20241001202007.png]]

## Blocked assignment
* To create a program that is not interleaved, you can use the program on the right.
	* This one does work on chunks. Each instance does work on **count** consecutive indices.
![[Pasted image 20241005155804.png]]

| Interleaved![[Pasted image 20241005155246.png]] | Blocked![[Pasted image 20241005155659.png]] |
| ----------------------------------------------- | ------------------------------------------- |


## Memory loading
* Recall that it is much more efficient to **load memory in contiguous chunks**
* During ISPC execution, **all instances are executed at each time step** via single thread using a vector instruction.

* Using interleaved, the memory used by each program instance is **contiguous**.
![[Pasted image 20241005160145.png]]

* Using blocked, the memory used at each time step is separated in memory.
![[Pasted image 20241005160404.png]]

* Therefore, **interleaved assignment** is generally faster and more efficient than **blocked assignment**.

# ISPC abstraction
* ISPC gives us the **`foreach`** abstraction that automatically computes the assignments.
	* It says "the entire gang of program instances" carries out these operations.
	* Because of the memory loading, `foreach` implements **interleaved assignment**.
	* Basically does "for every element in array, potentially in parallel, do smth"

![[Pasted image 20241005160536.png]]

* This abstraction allows us to write programs as if it were a sequential program.

## Issue: Independent Iterations
* To write an ISPC program with `foreach`, the **order of execution** of the loop **must not matter** --> each loop iteration needs to be independent.
![[Pasted image 20241005161501.png]]

## Issue: Cross-instance dependence
* Consider the program for computing the sum of an array below. Both are incorrect.

| ![[Pasted image 20241005161535.png]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | ![[Pasted image 20241005161541.png]]                                                                                                                                                                                                                                                                                                                                                               |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| - There is one unique `sum` per program instance.<br><br>- Because of how ISPC works, you will have 8 different `sum` variables, each stored in a lane of an 8-wide SIMD register.<br><br>- At the end of the loop, `sum` contains the sum of all array elements assigned for each instance.<br> <br>- In other words, the value of `sum` is undefined because we don't know what element was assigned.<br><br>- When we return, we return multiple copies of `sum` from each instance.<br><br>- Compile time type error. | - There is now one `sum` across all program instances.<br><br>- However, we now have multiple program instances trying to update a shared variable at the same time. This is a **race condition**.<br><br>- The issue is the `sum += x[i]` line. <br><br>- If all program instances run this at once, the result of `sum` becomes undefined because loading the initial value of `sum` is unclear. |


* The correct way to do this is to accumulate the partial sums of each instance into a regular `float`.
	* This means that the partial sum of each instance is stored in a lane of the 8-wide register that holds `partial`.
* At the end, we can use `reduce_add` to sum up all the lanes.
![[Pasted image 20241005162529.png]]

## ISPC cross program instance operations
![[Pasted image 20241005162625.png]]

# ISPC Tasks
* The ISPC gang abstraction is implemented by **SIMD instructions** that execute within **one thread** running on **one x86 core** of a CPU.
	* All the code seen so far would have executed on **only one of the four cores**.

* ISPC contains another abstraction: **a “task” that is used to achieve multi-core execution**.
	* Each task runs an ISPC function, which takes up a thread on a core.
		* Each ISPC function executes the SIMD instructions on those cores.
	* If you have 8 cores, you need to create **at least** 8 tasks to occupy all those cores.