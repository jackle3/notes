# 1 Convolutional Neural Networks
## 1.1 Dependency Graphs
![600](../../attachments/Pasted%20image%2020241107144319.png)

## 1.2 Deep Neural Networks
* A deep neural network is **simply a dependency graph**.
![Pasted image 20241107144358](../../attachments/Pasted%20image%2020241107144358.png)
* We can put many of these units together to form a network.
	* A **fully connected layer** can be thought of as a matrix-vector product ⟶ each row is a neuron of the output.
![Pasted image 20241107144425](../../attachments/Pasted%20image%2020241107144425.png)
* $w_{ij}$ is the weight from $x_j$ to neuron $i$ with bias $b_i$
![Pasted image 20241107144453](../../attachments/Pasted%20image%2020241107144453.png)

## 1.3 2D Convolution
* In terms of networks, you think of it as: for every pixel in the output, there are 9 pixels affecting it.
	* In this case, the weights for each pixel simply averages the 3x3 convolution.
![Pasted image 20241107144754](../../attachments/Pasted%20image%2020241107144754.png)
* We can define the weights to pick up **different patterns**
	* In the horizontal one, it detects change in pixel value in the horizontal direction. giving more weight to stuff in my current row (the middle)
![Pasted image 20241107145007](../../attachments/Pasted%20image%2020241107145007.png)

* You can also **apply many filters at once**
![Pasted image 20241107145114](../../attachments/Pasted%20image%2020241107145114.png)
![Pasted image 20241107145134](../../attachments/Pasted%20image%2020241107145134.png)
* Modern CNNs is just a lot of these layers bunched together.
![Pasted image 20241107145202](../../attachments/Pasted%20image%2020241107145202.png)

# 2 Implementing Convolution Layers
## 2.1 Direct Implementation
* The `i, j` goes through every pixel. The `f` goes through each filter.
	* For each filter, we do the `3x3` (or `FILTER_Y x FILTER_X`) convolution and storing the result in `output[j][i][f]`
	* More specifically, we also go through a **batch** of images `IMAGE_BATCH_SIZE`, so the output is for each image in the batch `output[img][j][i][f]`
![Pasted image 20241107145400](../../attachments/Pasted%20image%2020241107145400.png)

## 2.2 Matrix Vector Product
* We can rephrase the convolution as a matrix-vector product.
	* Define $\vec{w}$ a vector of our weights.
	* We copy each convolution stride into a row of our matrix sized $(W \cdot H) \times 9$
	* The output column vector is the resulting image after applying weights, sized $(W \cdot H) \times 1$
![Pasted image 20241107145706](../../attachments/Pasted%20image%2020241107145706.png)

## 2.3 Matrix Matrix Product
* You can extend the above and have each set of filters be a column of weights.
	* This will give us a matrix sized $(W \cdot H) \times \text{num\_filters}$
![Pasted image 20241107145917](../../attachments/Pasted%20image%2020241107145917.png)

* If we have multiple channels (e.g. for RGB), we can make our matrix more complex.
	* The left matrix is replicated 9 times for each channel.
	* The right matrix now has different sets of weights for each channel.
![Pasted image 20241107150021](../../attachments/Pasted%20image%2020241107150021.png)
* There exists very efficient matrix multiplication libraries!
## 2.4 GEMM
* GEMM stands for general matrix multiplication.
![Pasted image 20241107150723](../../attachments/Pasted%20image%2020241107150723.png)

# 3 Dense Matrix Multiplication
![Pasted image 20241107150808](../../attachments/Pasted%20image%2020241107150808.png)
* How much work do I do?
	* The size of the output array is $O(N^2)$
	* There's three `for`-loops so we have an $O(N^3)$ algorithm.
	* We do $(N^3)$ work on $(N^2)$ data ⟶ arithmetic intensity is:
$$
\text{Arithmetic Intensity} = \frac{\text{amount of computation}}{\text{amount of communication}} = \frac{\text{work}}{\text{data}} = \frac{O(N^3)}{O(N^2)} = O(N)
$$
* This is pretty good arithmetic intensity! We need to implement it right to reach it though.

* The issue with this the implementation above is: **low arithmetic intensity** bc of bad temporal locality (many cache misses) when accessing to A and B
	* **Access pattern** ⟶ to compute each row of C, we use the same row of A but different columns of B.
	* If the entire row of $A$ does not fit in cache, we're basically at SAXPY level of intensity (around 1/2 or 1/3 intensity).

## 3.1 Blocked Dense Matrix Multiplication
* Work on sub-blocks of $C$ at a time such that each block fits into our cache.
	* The inner three for loops do the submatrix multiply. The outer for loops move through the blocks.
	* The size out the output block is $\text{block\_size}^2$, and it reads roughly $2\cdot \text{block\_size}^2$ data from $A$ and $B$ to compute it.
* The arithmetic intensity is now:
$$
\text{Arithmetic Intesity} = \frac{O(\text{block\_size}^3)}{O(\text{block\_size}^2)}=O(\text{block\_size})
$$
* The advantage here is that every access to a block in $A$ and $B$ will be cache hits.
![Pasted image 20241107152235](../../attachments/Pasted%20image%2020241107152235.png)
* We want `block_size` to be as big as possible:
	* At least until we are compute bound, or until our cache runs out of space.
	* In the naive example our `block_size` was 1.

## 3.2 Implementations
* Recall that in this code, we access a block of matrix $A$ and a block of matrix $B$.
	* Before running this, we did some stuff to copy data from the original input tensor into the matrices `A` and `B` to multiply.
	* **Optimization:** instead of copying data from input to `A`, we could replace `A[jblockj+j]` with the actual indexing math that would index directly into the input.
![Pasted image 20241107153354](../../attachments/Pasted%20image%2020241107153354.png)
* Instead of creating the actual matrix and materializing it, we dynamically calculate the index and pull from the `input`.
![Pasted image 20241107153337](../../attachments/Pasted%20image%2020241107153337.png)
![Pasted image 20241107153705](../../attachments/Pasted%20image%2020241107153705.png)

# 4 GPU Work
* Suppose you work with a `256x256` image with `128` channels. At a batch size of `32`, that's `1 GB` of data.
![Pasted image 20241107153844](../../attachments/Pasted%20image%2020241107153844.png)
