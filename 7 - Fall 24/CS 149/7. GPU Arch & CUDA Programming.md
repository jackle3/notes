
# Basic GPU Architecture
* GPUs are similar to CPUs ⟶ chips are just a bit bigger and SIMD will be wider.
![[Pasted image 20241020144913.png]]
* GPUs were designed for graphics ⟶ they had many SIMD, multi-threaded cores to provide efficient execution of shader programs.
	* Some shader function `shader()` would be executed on every pixel on the screen in parallel

## GPU-based Scientific Computation
* Early on, used GPUs in a hacky way to do data ⟶ They changed the shader function to do some computation.
![[Pasted image 20241020145528.png]]

## Brook Streaming Programming Language
* New language that generalized usage of GPUs for data-parallel processing.
* Defined kernel functions that take in an input array, and runs **some function on every element** of that array.
![[Pasted image 20241020145855.png]]

## GPU Compute Mode
* NVIDIA then began providing a general compute interface instead of a graphics-specialized interface.
![[Pasted image 20241020150032.png]]
![[Pasted image 20241020150056.png]]
![[Pasted image 20241020150106.png]]

# CUDA Abstractions
* Introduced in 2007 with NVIDIA Tesla architecture
* “C-like” language to express programs that run on GPUs using the compute-mode hardware interface
* Relatively **low-level**: CUDA’s abstractions closely match the capabilities/performance characteristics of modern GPUs (design goal: **maintain low abstraction distance**)

## CUDA Threads
* A CUDA thread presents a similar abstraction as a `pthread` in that both correspond to logical threads of control, but the implementation of a CUDA thread is very different

## Hierarchy of Concurrent Threads
* When we spawn work, we spawn `blocks` and define the number of `threadsPerBlock`.
	* To compare to ISPC, blocks are your tasks/gangs, threads are your program instances.
* The `id` of a particular thread consists of:
	1. What thread block am I in?
	2. What thread am I in that thread block?
* All the threads in a thread block run simultaneously. There is no guarantee that different blocks run simultaneously.
![[Pasted image 20241020150346.png]]
## Basic Syntax
* To run a kernel function with cuda, use `fn<<numBlocks, threadsPerBlock>>(*args)`.
![[Pasted image 20241020150446.png]]
* In addition, notice that the out-of-bounds check is necessary because the matrix size does not divide evenly with the number of threads per block.
![[Pasted image 20241020151437.png]]
## CUDA Execution Model
* Host code runs on the CPU. The device code runs on the GPU.
![[Pasted image 20241020150827.png]]
## CUDA Memory Model
![[Pasted image 20241020151723.png]]
* To move code between the host (CPU) and the device (GPU), we have to **explicitly move it**.
![[Pasted image 20241020151835.png]]

* In addition, **every block has its own address space** and **every thread** has its own space.
![[Pasted image 20241020151921.png]]


## CUDA Example: 1D Convolution
![[Pasted image 20241020152501.png]]
* In this program, we create **one thread per output element** to compute them in parallel.
	* Notice that each thread computes the result (via input), and writes to memory (output).
	* This requires every thread to access **device memory** ⟶ there are $3 \times 128$ loads for `input`.
![[Pasted image 20241020152517.png]]
* In this program, we do the same thing but **stage input data into per-block shared memory**.
	* The `__shared__` decorator indicates that there is one copy of this variable **per block**.
	* All blocks load from device memory into block memory (only $130$ loads).
	* Then each thread computes the elements using the shared per-block `support` ⟶ accesses faster memory.
![[Pasted image 20241020152619.png]]

## CUDA Synchronization
![[Pasted image 20241020152857.png]]

## Summary
* Execution: thread hierarchy
	* Bulk launch of many threads
	* Two-level hierarchy: threads are grouped into thread blocks
* Distributed address space
	* Built-in `memcpy` primitives to copy between host and device address spaces
	* Three different types of device address spaces
		* Per thread, per block (“shared”), or per program (“global”)
* Barrier synchronization primitive for threads in thread block
* Atomic primitives for additional synchronization (shared and global variables)

# CUDA Semantics
* When we run the program, we launch $(1024^2 / 128 \text{ blocks})\times (128 \text{ threads per block}) = 1 \text{ mil}$ threads.
![[Pasted image 20241020153714.png]]
* We have over 8K thread blocks, so also 8K instances of the shared variable `support`.
* When the program is **compiled**, the device binary includes:
	* Program text (instructions)
	* Information about required resources:
		* 128 threads per block
		* B bytes of local data per thread (for local variables like `index`)
		* 128+2=130 floats (520 bytes) of shared space per thread block (for `support`)

## Thread-block Assignment
* The implementation of GPU will map these thread-blocks (aka tasks) to cores (aka workers).
![[Pasted image 20241020153858.png]]
* In other words, we have a pool of workers: **GPU cores**.

# Sub-core
* The sub-core has a fetch and decode.
* It also has a bunch of ALUs for each type of implementation:
	* There are 16 `fp32` ALUs, 16 `int` ALUs, etc. These are used to implement SIMD.
![[Pasted image 20241020154052.png]]

## Registers and Execution Contexts
* Each sub-core has a bunch of execution contexts (aka threads)
	* In this case, each execution context is a vertical group of scalar registers in the blue region.
	* Notice that we have $128$ execution contexts in this diagram.
* In traditional CPUs, you would think of these registers as **one CPU execution context** with **32-wide vector registers**.
![[Pasted image 20241020154436.png]]

## Warps
* 32 **consecutive** CUDA threads are mapped to 32 consecutive hardware execution contexts.
	* These are mapped to one warp.
![[Pasted image 20241020154609.png]]
* Threads in a warp are executed in a SIMD manner **if they share the same instruction**
* NVIDIA calls this SIMT (single instruction multiple CUDA thread)
	* If the 32 CUDA threads do not share the same instruction, performance can suffer due to divergent execution.
	* This mapping is **similar to how ISPC runs program instances in a gang** (all instances in a gang run in SIMD).
		* But GPU hardware is **dynamically checking** whether the 32 independent CUDA **threads share an instruction** ⟶ if true, it executes all 32 threads in a SIMD manner.
		* The CUDA program is not compiled to SIMD instructions like ISPC gangs.
* A warp is not part of CUDA, but is an important CUDA implementation detail on modern NVIDIA GPUs.
* **Thread blocks** are made up of one or more **warps**

## Instruction Execution
![[Pasted image 20241020155322.png]]
* When they execute instructions, they interleave fetches and arithmetic.
* Remember, **entire warp** of CUDA threads is running this instruction stream.
* So each instruction is run by all 32 CUDA threads in the warp. Since there are 16 ALUs, running the instruction for the entire warp takes **two clocks** (notice how fp32 and int32 takes two clocks).

# SM Unit
* The streaming multiprocessor (SM) unit is like the core ⟶ each one has **four sub-cores.**
* The warps are interleaved across the sub-cores.
* The SM (core) can:
	* Run 4 instructions per clock (one per sub-core) ⟶ basically **four-way superscalar.**
	* Maintain 64 warps (i.e. $64 \times 32 = 2048$ execution contexts/threads)
* You can think of **each SM as a thread-block**. The shared memory/L1 cache is the **block memory**.
![[Pasted image 20241020155507.png]]
## Running a Thread Block
* When we run the code on the core, the GPU allocates four warps (for the threads) and shared memory.
![[Pasted image 20241020155730.png]]
* SM core operation each clock:
	* Each sub-core selects one runnable warp (from the 16 warps in its partition)
	* Each sub-core runs next instruction for the CUDA threads in the warp
		* This instruction may apply to all or a subset of the CUDA threads in a warp depending on divergence

# Programming Model Summary
The CUDA programming model consists of a hierarchy of execution units:

1. **Thread**
	* The finest level of parallelism
	* Executes individual operations like floating point math
	* Has private registers for local variables
	* Maps to a single hardware execution context

2. **Warp (32 Threads)**
	* **32 threads make a warp**
	* All threads in a warp execute same instruction in SIMD fashion
		* More accurately SIMT because threads can diverge
	* Executed by one sub-core of an SM
	* Performance suffers if threads in warp diverge

3. **Block (N Warps)**
	* **N warps make a block**
	* Group of warps that share resources
	* Has access to fast shared memory visible to all threads in block
	* Executed entirely on one SM
		* You can think of each SM as a thread-block (though an SM can have many thread blocks scheduled).
		* All the threads in a block are executed on the same SM, and the shared memory is shared across all the threads in the block.

4. **Grid (M Blocks)**
	* **M blocks make a grid**
	* Top level of hierarchy containing all blocks
	* M should be a multiple of number of SMs for load balancing
	* Blocks in grid can execute in any order
	* Enables scaling across different GPU sizes

# GPU Chip
* A modern GPU (in this case NVIDIA V100) has 80 SM's.
	* This effectively defines the number of blocks that can run simultaneously, and each block can run threads simultaneously.
![[Pasted image 20241020160047.png]]

* Throughput:
	* 1.245 GHz clock
	* 80 SM cores per chip
		* Each chip has four sub-cores, which each have 16 ALUs (16-wide)
		* This equals $80 \times 4 \times 16 = 5120$ fp32 ALUs for `mul-add`.
	* Recall that each `mul-add` is two operations.
		* This means our max throughput is: $5120 \times 2 \times 1.245 \text{ GHz} = 12.7 \text{ TFLOPS}$

* Warps:
	* Each SM has 64 warps.
	* This means we can have $80 \times 64 = 5120$ interleaved warps per chip.
	* This is equal to $163,840$ CUDA threads per chip.
	* If your code is not creating this many threads, **we are underutilizing the chip**.


# Running a CUDA Program on a GPU
* We created thread blocks, each of which has 128 threads.
	* Each block allocates a shared variable (an array of 128 floats plus 2 float boundary conditions) that was shared amongst the threads.
* The code runs `N / 128` blocks (could be thousands of blocks)
* This subcore is a simplfied version ⟶ has 1.5KB of shared memory and 12 warps
![[Pasted image 20241020160739.png]]
![[Pasted image 20241020160759.png]]

**Step 1: host sends CUDA device (GPU) a command (“execute this kernel”)**
* Notice that this program runs 1000 blocks ⟶ 128,000 threads, with 128 threads per block.
* These threads are different from CPP ⟶ the core streams thread blocks onto processors, and only creates threads when block runs.
![[Pasted image 20241020160847.png]]

**Step 2: scheduler maps block 0 to core 0
* This first scheduling reserves execution contexts for 128 threads and 520 bytes of shared storage
![[Pasted image 20241020160911.png]]

**Step 3: scheduler continues to map blocks to available execution contexts (interleaved mapping shown)**
* Notice that only two thread blocks fit on a core (third block won’t fit due to insufficient shared storage ⟶ 3 x 520 bytes > 1.5 KB)
![[Pasted image 20241020160949.png]]

**Step 4: thread block 0 completes core 0**
![[Pasted image 20241020161036.png]]

**Step 5: block 4 is scheduled on core 0
* This block is mapped to execution contexts 0-127
![[Pasted image 20241020161102.png]]

**Step 6: thread block 2 completes on core 0**
![[Pasted image 20241020161126.png]]

**Step 7: thread block 5 is scheduled on core 0 (mapped to execution contexts 128-255)**
![[Pasted image 20241020161142.png]]
