
# Roadmap
![Pasted image 20241111133633](attachments/Pasted%20image%2020241111133633.png)

## Review: Energy Efficiency Vs Programmability
![Pasted image 20241111134710](attachments/Pasted%20image%2020241111134710.png)

# Hardware Acceleration for DNNs

# Google TPU V1
* Focuses on making matrix multiply as fast as possible.
* It uses 8 bit integers as the data type ⟶ only supports 8 bit int inputs
	* Allows us to implement a 256x256 matrix multiply in a single cycle
![Pasted image 20241111140012](attachments/Pasted%20image%2020241111140012.png)
![Pasted image 20241111140131](attachments/Pasted%20image%2020241111140131.png)

## Systolic Array
* The key to implementing matrix multiplies for DNNs efficiently.
* We have a bunch of **Processing Elements**

**Suppose we wanted to do matrix-vector multiplication: $Y = wx$**
* The examples below is a **weight stationary algorithm**
	* I.e. we load up the PEs with the weights

| ![Pasted image 20241111140534](attachments/Pasted%20image%2020241111140534.png) | ![Pasted image 20241111140454](attachments/Pasted%20image%2020241111140454.png)                                                                                                   |
| ------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------- |
| ![Pasted image 20241111140546](attachments/Pasted%20image%2020241111140546.png) | ![Pasted image 20241111140633](attachments/Pasted%20image%2020241111140633.png)                                                                                                   |
| ![Pasted image 20241111140640](attachments/Pasted%20image%2020241111140640.png) | With a 4x4 W matrix, we need 16 multiplications (the systolic array will have 16 elements).<br><br>A lot of the PEs are underutilized. |
![Pasted image 20241111140941](attachments/Pasted%20image%2020241111140941.png)

* This makes the TPU very good at doing dense matrix multiplies.

# Scaling Law
* There is exponentailly more compute needed in order to take a constant step in accuracy.
![Pasted image 20241111142611](attachments/Pasted%20image%2020241111142611.png)

# NVIDIA H100
* Over time, NVIDIA chips has added more and more specialized capabilities for training
* The most modern model H100 has:
	* distributed shared mem, transformer engines, high bandwidth mem (HBM), etc
	* these all support training ML models ⟶ gives more performance and efficiency
* Notice that it has async copy (memory) and async exec
![Pasted image 20241111142957](attachments/Pasted%20image%2020241111142957.png)

## Memory Hierarchies
* Cluster is a new idea ⟶ allows you to combine multiple thread blocks together an have them cooperate
	* Defining a cluster of thread blocks allows you to compute multiple thread blocks on different SMs concurrently.
	* This introduces the **SM-to-SM** network.
![Pasted image 20241111143145](attachments/Pasted%20image%2020241111143145.png)

## SM Architecture
* There are now double as many `fp` execution units (yellow boxes)
	* Allows you to do mul-add on a warp (32 threads) in one clock cycle
	* Recall: V100 required two clocks because we had 16 ALUs for 32 threads
![Pasted image 20241111143326](attachments/Pasted%20image%2020241111143326.png)

## Tensor Memory Accelerator
* In order to keep tensor cores utilized, we need to feed data to it **at a very high rate**
	* Existing methods to feed data (from global memory to shared memory) is too slow
* With the TMA, you define a descriptor that defines a region in global memory.
	* You can tell the TMA to go fetch the data from global memory.
	* Makes the data movement much more asynchronous ⟶ this is essentially a direct memory access unit
	* The TMA does the memory address generation and movement for you; relieves the SM from having to do memory-related work.
![Pasted image 20241111143559](attachments/Pasted%20image%2020241111143559.png)

## The Whole GPU
* Most of the FLOPs are in the systolic arrays in the tensor core
![Pasted image 20241111143853](attachments/Pasted%20image%2020241111143853.png)

## Utlilization Rate
* Since the H100 chip is so much faster, existing algorithms have low utilization:
	* E.g. FlashAttention-2 had 70% utilization on A100s to 35% utilization on H100.
	* It took 2 years to come back to 65% utilization with FlashAttention-3
* To extract peak performance, we need to keep the tensor cores busy.
	* Tensor cores operate on 16x16 tiles ⟶ we should give it data in this shape or multiples of this shape
	* Use asynchrony
		* TMA to fire off asynchronous memory reads
		* Asynchronous executions on warps
	* Doing loads and stores through the SM is not enough ⟶ you have to use the TMA to be efficient.
![Pasted image 20241111144041](attachments/Pasted%20image%2020241111144041.png)

# ThunderKittens
* A domain specific language that is embedded in CUDA.
	* Designed for AI kernels
* Three key ideas:
	* The fundamental compute data type is a tile
	* Make maximum use of async (both compute and memory access can be done async)
	* Globally coordinate warp groups to process things efficiently in a pipeline
* They do this by:
	* Having a bunch of optimized Templated Data Types
	* Operations on the data types
![Pasted image 20241111214008](attachments/Pasted%20image%2020241111214008.png)
## Key Principles
![Pasted image 20241111220053](attachments/Pasted%20image%2020241111220053.png)
## Tile Processing Pipeline
* The producer loads tiles into shared memory.
* The consumer takes tiles and computes them with the tensor core.
* The finish stores tiles from shared memory back to global.
![Pasted image 20241111214238](attachments/Pasted%20image%2020241111214238.png)

## Matmul
* For the `a, b, c`, the data type is `bf16` and the minimum tile size is `64x64` for `a` and `64x256` for `b`.
![Pasted image 20241111214414](attachments/Pasted%20image%2020241111214414.png)
* In the producer, we tell one warp (or really one thread) to go tell the TMA to go launch loads.
![Pasted image 20241111214528](attachments/Pasted%20image%2020241111214528.png)
* Once load is done, we do the computation using `mma_async_wait`
	* This is where you place the execution on the tensor core.
![Pasted image 20241111214649](attachments/Pasted%20image%2020241111214649.png)

## Performance
* CuBLAS comes directly from NVIDIA so its pretty much as fast as you can get.
* ThunderKittens is better for small size matrices, and nears it for larger sizes.
* TK allows you to write a kernel that is competitive with CuBLAS pretty simply.
![Pasted image 20241111214758](attachments/Pasted%20image%2020241111214758.png)

# SambaNova
**Can we have efficiency and a simpler programming model?**
**Can we get asynchrony with a simpler programming model?**
* Hint: take a data centric view
## Reconfigureable Dataflow
* Intended for ML and DNN acceleration ⟶ fundamentally wants to do GEMM quickly.
* It has PCUs that look like systolic arrays
* The on-chip memory is distributed across the core on each of the PMUs.
* It allows you to reconfigure the chip to match the data flow of the algorithm.
	* The chip is basically a cluster of PCUs each with their own PMUs
	* Each PCU/PMU communicates with each other via smth similar to message passing
	*
![Pasted image 20241111220745](attachments/Pasted%20image%2020241111220745.png)
## Dataflow Programming
* Take the algorithm and turn it into data parallel patterns
	* This then allows you to tile things and convert them into operations separated by intermediate buffers
* In softmax, the intermediate tensors are `x, m, r, o`
	* You can then take this and map it onto the compute and have it communicate via the on-chip network.
	* Parts of the chip does different components of the execution pipeline.
![Pasted image 20241111221248](attachments/Pasted%20image%2020241111221248.png)

## Decoupled Access Execute
* If you are accessing a linked list, then you have to couple the load and stores together (i.e. to figure out where to follow next link, you need to load current node and move to the next node)
* If you are on a tensor, you know all the data you need to fetch beforehand.

## Metapipelining
* Converts a loop body into multiple stages, put buffers between the stages ⟶ allows you to run a loop in a pipelined manner
* Allows you to overlap executions of multiple loop executions at the same time.
![Pasted image 20241111215630](attachments/Pasted%20image%2020241111215630.png)

### Example
* First we load from outside memory into chip on the `row` buffer
	* We also load the `sub` buffer
* We then run the difference mapping ⟶ This is `Pipe 2`.
	* We store the result in the `diff` buffer.
![Pasted image 20241111221831](attachments/Pasted%20image%2020241111221831.png)
* We can have **each pipe running in parallel** ⟶ metapipelining with four stages.
	* We use double buffers so that the producer can be writing into one buffer while the consumer reads from the other buffer.
![Pasted image 20241111222132](attachments/Pasted%20image%2020241111222132.png)
## Matmul Metapipeline
* The outer pipeline loads tiles from the A matrix.
* The inner pipeline loads tiles from the B matrix and does the matmul.
![Pasted image 20241111222225](attachments/Pasted%20image%2020241111222225.png)
* We can use `row_par` to decide how many PCUs it should use to compute it.
* You can also insert explicit buffers (e.g. `c_tile = BUFFER(c)`)
![Pasted image 20241111222308](attachments/Pasted%20image%2020241111222308.png)

## FlashAttention
* Attention is broken into tiles.
	* Different parts of attention from different tiles can all be computed in parallel.
* It overlaps memory access and compute.
![Pasted image 20241111222455](attachments/Pasted%20image%2020241111222455.png)

## Tensor Parallel Llama Training
* With an NVIDIA GPU, we need to have 10 different kernels to implement each decoder.
* With SN40L chip, we can metapipeline it such that there is only one kernel running all the steps in parallel.
![Pasted image 20241111222730](attachments/Pasted%20image%2020241111222730.png)

### Kernel Looping
* Notice that there are 32 decoders ⟶ we can use the same kernel for all 32 decoders to save on the overhead costs of setting up the kernel.
* With NVIDIA, as we add more H100 chips:
	* Time spent on GEMM decreases bc we have more compute
	* But time on `AllReduce` increases (bc we need to communicate across more chips)
* When we use kernel looping:
	* The synchronization cost of SN40L becomes tiny.
![Pasted image 20241111223144](attachments/Pasted%20image%2020241111223144.png)
* SN40L has half the memory bandwidth of the H100
	* Inference on LLMs token generation is memory-bound.
	* By doing kernel looping, we can get near max HBM bandwidth the entire time (since we removed synchronization overhead)
![Pasted image 20241111223147](attachments/Pasted%20image%2020241111223147.png)
* **Drawback:** we need to be able to fit the entire kernel on the chip.

# Summary
![Pasted image 20241111223432](attachments/Pasted%20image%2020241111223432.png)
