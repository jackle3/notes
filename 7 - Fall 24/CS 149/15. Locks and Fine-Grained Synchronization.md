
# Review
## Deadlock
* **Deadlock** ⟶ state where system has outstanding operations to complete, but not operation can make progress.
	* Arises when each operation acquires a **shared resource** that another operation needs.
	* There is no way for any thread to make progress unless some thread gives up a resource (e.g. all the locks are taken, no one can make progress)
![300](../../attachments/Pasted%20image%2020241122123953.png)
### Deadlock in Systems
![Pasted image 20241122124227](../../attachments/Pasted%20image%2020241122124227.png)

### Conditions for Deadlock
![Pasted image 20241122124313](../../attachments/Pasted%20image%2020241122124313.png)

## Livelock
* System is **executing many operations**, but no thread is making meaningful progress.
	* E.g. cars keep moving back and forths to make space for others, but no one is able to actually progress

## Starvation
* Some threads are unable to make progress because of other threads.
![Pasted image 20241122124418](../../attachments/Pasted%20image%2020241122124418.png)

## Cache Coherence
* Suppose we have two processors, each with process-specific caches
	* Logically, each address corresponds to a single value in memory
	* With caches, we have copies of values at addresses
* Cache coherency ensures these copies are the same ⟶ if all caches obey by these rules, accessing value `X` will get the most recent value written to `X` by any process.
* **Main idea:** If one processor writes, no other processors can read.

### State Transition Diagram
* Every cache line is in one of three states:
	* Invalid ⟶ line not in cache
	* Modified ⟶ line in cache, but its been written to
		* If in modified, no other cache has a copy of this line.
		* This cache has exclusive access to the line.
		* Because its **exclusive**, we are guaranteed that no other cache is reading or writing to the line, so we can safely modify the line.
	* Shared ⟶ line in cache, but it has not been written to
		* If in shared, other caches can have a copy of that line.
		* If its in shared, its not possible for someone to have it in the exclusive state.
			* Guaranteed that no one else is writing to it.
* If a processor wants to write:
	* It brings the cache line into a modified state.
	* It invalidates the cache line in all other caches.
* If multiple processors want to write:
	* **The network serializes the transactions**
	* E.g. it lets processor 1 write first, then flush, then processor 2 reads and writes.
* If we have a cache hit, we don't need to tell others.

![Pasted image 20241122124700](../../attachments/Pasted%20image%2020241122124700.png)

**Example 1:**
* If line is invalid in my cache and I want to write (`PrWr`) to it:
	* I yell out `BusRdX` to everyone (**black**).
	* Anyone who hears `BusRdX` will move to the invalid state (**blue**).
	* Then I elevate to the modified state.
* If line is invalid in my cache and I want to read (`PrRd`) it:
	* I yell out `BusRd` to everyone (**black**)
	* If a cache in **shared** state hears `BusRd`, they stay in shared (**blue**)
	* If a cache in **modified** state hears `BusRd`, they **flush** and move to shared (**blue**)

**Example 2:**
![Pasted image 20241122130104](../../attachments/Pasted%20image%2020241122130104.png)

# Test-and-set Based Lock
* **Test-and-set instruction:** an atomic read, condition, write
	* Update the value at `addr` to 1 if and only if it is 0.
![Pasted image 20241122130208](../../attachments/Pasted%20image%2020241122130208.png)

* We can use this to implement a basic lock:
	* Atomitically read a value and conditionally update it.
	* Wait until value is 0 (meaning unlocked), then set to 1 to acquire lock
![Pasted image 20241122130242](../../attachments/Pasted%20image%2020241122130242.png)

## Coherence
* The test-and-set is a **write** because we might write to the address ⟶ `BusRdX`
* While one processor holds the lock:
	* The other processors continuously send `BusRdX` to test-and-set, but will fail because the lock is not free.
	* The cache line for the test-and-set `addr` is bouncing around.
* The dashed box is when that processor has the line in **modified** state.
	* Note that its impossible for a cache to have a line that is not up to date (if its not up to date, it would be in the invalid state, meaning not in cache)
![Pasted image 20241122130944](../../attachments/Pasted%20image%2020241122130944.png)

## Performance
* Notice that **there is a lot of cache contention** with the test-and-set lock.
* This makes the lock more inefficient the more processors we have (more contention)
![Pasted image 20241122131331](../../attachments/Pasted%20image%2020241122131331.png)

# Lock Performance
![Pasted image 20241122131357](../../attachments/Pasted%20image%2020241122131357.png)

# Test-and-test-and-set Lock
* While `lock` is 1 (meaning someone else has it), keep checking with a `BusRd`
* Once it becomes zero (released), we try to `test_and_set` with `BusRdX`.
![Pasted image 20241122131446](../../attachments/Pasted%20image%2020241122131446.png)

* There is **no cache contention** while someone else has the lock, because all other processors will have line is **shared** state.
![Pasted image 20241122132121](../../attachments/Pasted%20image%2020241122132121.png)

## Characteristics
![Pasted image 20241122132309](../../attachments/Pasted%20image%2020241122132309.png)

# Ticket Lock
* Main problem with test-and-set style locks: upon release, **all waiting processors** attempt to acquire lock using test-and-set ⟶ only one actually gets it.
* Solution: ticket ⟶ make processors wait their turn before they can acquire.
	* There is **one write** per lock ⟶ increment `next_ticket`
	* Then the lock spins on **reads** to `now_serving`
		* Almost every read will be a cache hit; we will have a cache miss once someone unlocks.
![Pasted image 20241122132936](../../attachments/Pasted%20image%2020241122132936.png)

# Compare and Swap
* Compare and swap is an **atomic** general purpose test-and-set
	* If `old` value equals `compare`, set it to the new `val`.
![Pasted image 20241122133531](../../attachments/Pasted%20image%2020241122133531.png)

## Atomic Min
* We can use this to implement `atomicMin`
	* `atomicCAS` succeeds if the value from memory is the same as what we read before.
	* This ensures that the value in `addr` is always the min.
	* Suppose:
		* thread 1 calls `*addr = 100` and `x = 50`.
		* thread 2 calls `*addr = 100` and `x = 10`.
		* when thread 1 enters the while loop, if thread 2 finishes before it, it needs to reread the value from memory.
		* basically, `atomicCAS` checks if thread 2 finished before it (is value same)
![Pasted image 20241122133542](../../attachments/Pasted%20image%2020241122133542.png)

## Spin Lock
![Pasted image 20241122133958](../../attachments/Pasted%20image%2020241122133958.png)
* In the efficient implementation (test-and-test-and-set):
	* While we don't have the lock just wait
	* Try to acquire the lock using compare and swap.
		* If we get it, return.
		* If not, repeat.
	* **Implications on cache coherence:** This avoids the ping-ponging of the cache line for the lock because we only go into read-exclusive in the if-statement.
		* The line can stay in shared state in all thread's cache for the majority of the time.
		* Only when it hits the if-statement does it get invalidated and one thread takes exclusive access.

# Using Locks

## Sorted Linked List
* The linked list is implicitly sorted.
	* When we insert, we walk forwards until we find correct position.
	* When we delete, we walk forwards until we find value.
![Pasted image 20241122134330](../../attachments/Pasted%20image%2020241122134330.png)
* If **two threads** are working on this linked list:
	* Both threads might try to delete the same value/node (double-free)
	* The sorted property could break if two threads insert at the same time.
	* Linked list might just fully break
	* We might lose the operations from one thread.
![Pasted image 20241122134543](../../attachments/Pasted%20image%2020241122134543.png)
![Pasted image 20241122134602](../../attachments/Pasted%20image%2020241122134602.png)

### Single Global Lock
* The simplest solution is to **lock** the read and writes to the list.
![Pasted image 20241122134744](../../attachments/Pasted%20image%2020241122134744.png)
![Pasted image 20241122134804](../../attachments/Pasted%20image%2020241122134804.png)

### Fine Grained Locking
* Each node has a lock, and we do **hand-over-hand locking**
	* Acquire the next lock before we release the current lock.

Suppose `Thread 0: delete(11)`
* We grab the lock on `3` and also grab the lock on `5`.
![Pasted image 20241122135151](../../attachments/Pasted%20image%2020241122135151.png)
* Then we can release the lock on `3` and move onto `10`
![Pasted image 20241122135210](../../attachments/Pasted%20image%2020241122135210.png)
* Then we can release the lock on `5` and move onto `11`
	* If **thread 0** deletes `11`, we **don't need a lock on 18**
	* Since **thread 0** holds lock on `10`, its impossible for another thread to delete `18` before thread 0 gives up the lock.
![Pasted image 20241122135223](../../attachments/Pasted%20image%2020241122135223.png)

Suppose now `Thread 1: delete (10)`
* It acquires the lock on `3` and `5` but can't move on until thread 0 finishes.
![Pasted image 20241122135348](../../attachments/Pasted%20image%2020241122135348.png)
* Once thread 0 finishes, then thread 1 can take the locks.
![Pasted image 20241122135423](../../attachments/Pasted%20image%2020241122135423.png)
![Pasted image 20241122135441](../../attachments/Pasted%20image%2020241122135441.png)
![Pasted image 20241122135455](../../attachments/Pasted%20image%2020241122135455.png)

#### Implementation
![Pasted image 20241122135513](../../attachments/Pasted%20image%2020241122135513.png)

#### Characteristics
![Pasted image 20241122135530](../../attachments/Pasted%20image%2020241122135530.png)
