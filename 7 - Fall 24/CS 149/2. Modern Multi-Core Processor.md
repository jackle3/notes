
# Logistics
* Partner matching form posted on Ed for programming assignments
* Written assignments -> short, meant to be so that you can be done in 2 hours
	* They are assigning the partners; two random people for every written assignment.

# Review

## What's a computer program?
* A program is a list of instructions. Instructions tell the computer to modify the program state (registers, memory, etc).

## What does a processor do?
* Processors execute instructions. Figures out what the instructions do, and doing it (manipulating state).
* There are three parts:
	1. Control (fetch/decode) -> determines what instruction to do next
	2. Execution unit (ALU) -> does stuff
	3. State (execution context) -> registers keeping program state

## Superscalar execution
* This is when the processor automatically figures out dependencies and runs **multiple independent instructions** at the same time.
	* Respects program order -> produces same output as serial execution.
	* This is a **hardware implementation detail**. The processor is given serial instructions and figures this out itself.

![600](../../attachments/Pasted%20image%2020240926125338.png)

* The processor now has multiple control and execution units managing the same state.
![400](../../attachments/Pasted%20image%2020240926125442.png)

## What is memory?
* Memory is like registers (allows you to store values) but **much bigger**
* Memory is just a linear indexable array of bytes.
	* Each byte is identified by its address (index).
* Program **state is both registers and memory**.

### Latency
* Processors have to request data from memory in order to use it!
	* Programs can use the `ld R0 <- mem[R2]` to load index `R2` of the memory into `R0`.
	* **Memory access latency** -> the amount of time it takes the memory system to provide data to the processor.

### Stalls
* Processors stall (can't make progress) when it cannot run the next instruction because future instructions depend on an **old instruction that is not yet complete**.
* Accessing memory is a source of stalls.
![Pasted image 20240926130426](../../attachments/Pasted%20image%2020240926130426.png)

### Cache
* On-chip storage that maintains a copy of a subset of values in memory for fast access in the future.
	* Processors can load from cache more quickly than from memory.
* If memory is papers in your garage, then the cache is papers on your desk -> quicker to use.
* Cache is always pulled in contiguous chunks called **cache lines**.

![Pasted image 20240926130637](../../attachments/Pasted%20image%2020240926130637.png)

* Cache policies include:
	* Most recently used, most frequently used, etc
* **For this class, assume the policy is LRU** (least recently used). To make room for new data, throw out the oldest data.`
	* cache of size N stores values for last N addresses accessed.

**Locality**
* Spatial locality -> loading data with a cache line preloads contiguous addresses; allows for cache hits for addresses in same line
* Temporal locality -> repeated access to same address results in hits.

**Example**
* First column is the addresses that the processor accesses.
![Pasted image 20240926131324](../../attachments/Pasted%20image%2020240926131324.png)
![Pasted image 20240926131528](../../attachments/Pasted%20image%2020240926131528.png)
* Cold miss -> never accessed before, loads into cache
* Capacity miss -> accessed before but we had to eject it
* Note: if the first address accessed was loading `0x1`, then it would've loaded the line `0x0` into the cache.

**Why is cache good?**
* It reduces the length of stalls (smaller memory access latency).

**Hierarchy of caches**
* The system has an L1 cache, an L2 cache, an L3 cache, and memory.
	* Each one is smaller than the next, but has lower access latency.
*  When the processors wants a memory address, it goes through each cache in order.
![Pasted image 20240926131921](../../attachments/Pasted%20image%2020240926131921.png)

* The latency is caused by **physical distance**. The data movement also has a high energy cost.
![Pasted image 20240926132042](../../attachments/Pasted%20image%2020240926132042.png)
![Pasted image 20240926132304](../../attachments/Pasted%20image%2020240926132304.png)

## Summary
* Single-thread-of-control performance is pleateaued -> improving slowly
* To run faster, we need multiple processing units or specialized hardware.


# Parallelization

**Today's example program**
* For today, the program we're working with takes in an input array `x` and uses Taylor expansion to compute `sin(x)` and stores it into `y`.
![Pasted image 20240926132505](../../attachments/Pasted%20image%2020240926132505.png)

* This program compiles the following sequence of instructions for each element of the array (inside the first loop).
	* Note that there is no ILP in the inner loop body because its just a chain of dependencies..
![Pasted image 20240926132804](../../attachments/Pasted%20image%2020240926132804.png)

## Multi-core Processor
* Rather than using transistors to increase sophistication of processor logic to accelerate a **single instruction stream**, we use transistors to **add more cores**.
* Each processor may be slower, but we now have multiple processors -> might give us a speedup.
![Pasted image 20240926132926](../../attachments/Pasted%20image%2020240926132926.png)
![Pasted image 20240926134329](../../attachments/Pasted%20image%2020240926134329.png)

## Parallel Program
 * Without multiple cores, the software now needs to give the processor two different instruction streams to do.
	 * Traditional C programs (like the one above) complies to an instruction stream that runs as one thread on one core.

### Threads
* In the example below, you split up the input `x` in half and use two threads to compute each half in parallel.
	* Main thread creates a second thread `my_thread = std::thread`.
	* Second thread computes first half of input `x`.
	* Main thread computes second half of input `x`.
	* The two threads are **independent** -> each item in the outer loop (for each input item) is independent.
* Generally, you want to create the same number of instruction streams as you have cores.
![Pasted image 20240926133457](../../attachments/Pasted%20image%2020240926133457.png)

## SIMD Processing
* We break our one ALU into 8 "smaller" scalar ALUs
	* In other words, make the ALU a vector ALU able to work on a vector of 8 scalars.
* We break our registers into vectors of 8 values.
	* If a processor now says add, it can take two registers (each with 8 pieces of data) and do a **vector add** to add 16 items at once.
* It is still one instruction per clock, but now that instruction is adding an 8-vector with another 8-vector.

* This amortizes the control (orange box) across many ALUs.
![Pasted image 20240926134919](../../attachments/Pasted%20image%2020240926134919.png)
![Pasted image 20240926135817](../../attachments/Pasted%20image%2020240926135817.png)

### Vector Program
![Pasted image 20240926135303](../../attachments/Pasted%20image%2020240926135303.png)
![Pasted image 20240926135556](../../attachments/Pasted%20image%2020240926135556.png)

* In this vector program, each iteration of the outer for-loop computes the sine for eight elements of the input.
	* Step through the input `x` in chunks of eight, compute each chunk in parallel.
* The intrinsic functions (in red) operate on **vectors of eight** 32-bit values (e.g. vector of 8 floats).
	* The `__m256` are fixed length on 256-bits. If we had 32-bit operations it'd be vectors of eight. If we had 64-bit operations it'd be vectors of four.

### Modern CPU examples
![Pasted image 20240926141444](../../attachments/Pasted%20image%2020240926141444.png)
![Pasted image 20240926141502](../../attachments/Pasted%20image%2020240926141502.png)

## Data-parallel expression
* Ideally, you would be able to simply declare that all loop iterations are independent.
* A compiler can use this to automatically generate both:
	1. threaded code to use multiple cores
	2. vector instructions to make use of SIMD processing
![Pasted image 20240926134752](../../attachments/Pasted%20image%2020240926134752.png)

# Conditional execution
* If a piece of code is **independent but has conditional execution**, some of the scalar ALUs may need more cycles to take the if-branch versus the else-branch.

![Pasted image 20240926140557](../../attachments/Pasted%20image%2020240926140557.png)
* In the yellow chunk, only 3/8 of ALUs did useful work.
* In white chunk, only 5/8 of ALUs did useful work.
* In worst case, only 1/8 of ALUs do useful work.

* Suppose the **if and else blocks had 3 instructions each** and the data was such that **50% of the time we go if and 50% of the time we go else**.
	* If 50% of the time we had 3/8 utilization, and 50% of the time we had 5/8 utilization, that would be 50% utilization on average.
	* If 50% of the time we had 1/8 utilization, and 50% of the time we had 7/8 utilization, that would be 50% utilization on average.
* If we wanted 1/8 utilization:
	* We need an input `x` such that 1/8 of the time we go down the if, and the if block was like 10000 instructions and the else is nothing.
	* Basically need all of our time to be spent running in 1/8 utilization.

# Stream Coherence
* **Instruction stream coherence ("coherent execution")**
	* Same instruction sequence applies to many data elements.
	* Coherent execution IS NECESSARY for SIMD processing resources to be used efficiently.
	* Coherent execution IS NOT NECESSARY for efficient parallelization across many cores
		* Each core has capability to fetch/decode a different instruction.
* **Divergent execution**
	* A lack of instruction stream coherence in a program.

* The 1/8 utilization conditional execution example above is an example of divergent execution.


# Summary
* We've seen three different forms of parallel execution.
![Pasted image 20240926141556](../../attachments/Pasted%20image%2020240926141556.png)

* You can mix and match these forms.
![Pasted image 20240926141627](../../attachments/Pasted%20image%2020240926141627.png)

* The myth machines have a quad-core processor with three-way superscalar and 8-wide SIMD.
![Pasted image 20240926141719](../../attachments/Pasted%20image%2020240926141719.png)

* Modern GPUs can do an insane amount of parallel execution.
![Pasted image 20240926141814](../../attachments/Pasted%20image%2020240926141814.png)

