
A parallel computer is a **collection of processing elements** that cooperate to solve problems **quickly**.
* We use *multiple processing elements* to get it.
* **quickly** means we care about performance and we care about efficiency.


# Speedup
We compute parallel processes to achieve a speedup!
$$
\text{speedup( using P processors )} = \frac{\text{execution time (using 1 processors)}}{\text{execution time (using P processors)}}
$$
There are some things that can limit the maximum speedup achieved:
1. **Communication** between processors can limit speedup.
	* Minimizing the cost of communication can improve speedup.
2. **Imbalance in work** assignments can limit speedup (some processors can run out of work while others are still working).
	* Improving the distribution of work can improve speedup.

To solve the workload imbalance, it may introduce overhead in communication

## Theme 1: Scale
We want to design and write parallels that scale.
1. Decomposing work into pieces that can safely be performed in parallel
2. Assigning work to processors
3. Managing comms/synchronization between processors **so that it does not limit speedup**

## Theme 2: Hardware
We need to know how parallel computers work in order to use it efficiently.
* Performance characteristics
* Design trade-offs: performance vs. convenience vs. cost

## Theme 3: Efficiency
Fast does not mean efficient. Fast programs may not be using the hardware efficiently.
* A 2x speedup on a computer with 10 processors is *not* a good result.

# Parallel Processing

**Historically**, parallel programming was not worth it because single-threaded CPU performance was doubling every 18 months.
* Processors kept increasing CPU clock frequency
* Processors exploited instruction-level parallelism (superscalar execution)

## Computer Program
Programs are recipes -> operations that manipulate some state.
![Pasted image 20240924142806](Pasted%20image%2020240924142806.png)

## Processors
![Pasted image 20240924142902](Pasted%20image%2020240924142902.png)
![Pasted image 20240924143045](Pasted%20image%2020240924143045.png)

## Computers
![Pasted image 20240924143106](Pasted%20image%2020240924143106.png)

# Instruction level parallelism (ILP)
To respect program order, we need to ensure **all dependencies are completed** before an instruction executes.
* If we don't violate dependencies, then the **output will be the same**.

![Pasted image 20240924143257](Pasted%20image%2020240924143257.png)
![Pasted image 20240924143457](Pasted%20image%2020240924143457.png)
![Pasted image 20240924143502](Pasted%20image%2020240924143502.png)

## Superscalar processor execution
* This is a **hardware detail** -> processor finds* independent instructions in a sequence and executes them in parallel on multiple execution units.
	* The compiler might also be finding independent instructions at compile time.

![Pasted image 20240924143704](Pasted%20image%2020240924143704.png)

# Plateau
* We are not getting any benefit from adding more transistors at this point (because power, ILP, and clock frequency plateaued)
	* However, most ILP is already exploited -- speedup plateaus pretty quickly.
	* In addition, processor clocks (frequency) can't keep growing because it consumes too much power, producing too much heat.
![Pasted image 20240924143908](Pasted%20image%2020240924143908.png)

* Architects are now building faster processors **by adding more execution units that run in parallel** (or units specialized for specific tasks)
	* Software must be written in parallel for performance gains!

# Goal
![Pasted image 20240924144145](Pasted%20image%2020240924144145.png)

## Efficiency
Software must be more than just parallel; it must be efficient.
1. Power creates heat, which slows down processors.
![Pasted image 20240924144224](Pasted%20image%2020240924144224.png)
	