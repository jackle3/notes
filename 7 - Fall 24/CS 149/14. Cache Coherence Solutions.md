
# Recall: Cache Coherence Problem
* The data in the cache can be out of sync in multiprocessing scenarios.
![Pasted image 20241112131247](../../attachments/Pasted%20image%2020241112131247.png)
* Intuitive behavior: reading value at address X should return the last value written to address X by any processor
* Memory coherence problem exists because there is both global storage (main memory) and per-processor local storage (cache).

# Coherence
* A memory system is coherent if:
	* **For each memory location**, there exists a hypothetical **serial order** of all program operations (executed by all processors) to that location that is consistent with the results of execution
	* Two key requirements:
		1. Memory operations issued by any one processor occur in the order issued by that processor
		2. The value returned by a read is the value written by the last write to that location according to the serial order
* In other words:
	* All processors must agree on the order of operations to the same memory location
	* The order must preserve program order for each individual processor
	* Reads must return the most recently written value based on this agreed-upon order
	* We want to **serialize** the accesses to any particular memory location
* Example chronology of operations on address X:
	1. P0 write: 5
	2. P1 read (it reads 5)
	3. P2 read (it reads 5)
	4. P0 read (it reads 5)
	5. P1 write: 25
	6. P0 read (it reads 25)

## Cache Coherence Invariants
* For any memory address `x`, at any given time period (epoch):
	* **Single-Writer, Multiple-Read (SWMR) Invariant**
		* **Read-write epoch:** there exists only a single processor that may write to x (and can also read it)
		* **Read-Only epoch:** some number of processors that may only read x
		* Basically at any epoch, we either have a single writer or multiple readers.
	* **Data-Value Invariant (write serialization)**
		* The value of the memory address at the start of an epoch is the same as the value of the memory location at the end of its last write epoch
		* i.e. when moving from a read-write epoch to a read-only epoch, the value of the memory address must be the same as the value at the end of the last write epoch
![Pasted image 20241112132137](../../attachments/Pasted%20image%2020241112132137.png)

## Types of Implementations
Two main approaches to implementing cache coherence:
1. Software-based solutions (coarse grain: VM page)
	* OS uses page-fault mechanism to propagate writes
    	* Write on top of virtual memory system, marking shared page as read only
        * When someone tries to write on it, it causes a page fault and the software takes over
	* Can be used to implement memory coherence over clusters of workstations
	* Performance problems:
		* Software is slow
		* False sharing (discussed later)
		* Works on the granularity of VM pages (8KB in a page)

2. Hardware-based solutions (fine grain: cache line)
	* "Snooping"-based coherence implementations
		* Looking at actions of all other processors and figuring out what to do
	* Directory-based coherence implementations
		* More scalable, now widely implemented
	* More efficient and finer grained ⟶ coherence at the granularity of cache lines

# Solution 1. Shared Caches
* This makes coherence very easy ⟶ one cache for all processors.
* Problems:
	* Cache is slower because its further away from processor
	* Need to share cache's bandwidth across different processors
	* Has both destructive but also constructive interference
* **Destructive interference** ⟶ contention (i.e. processors alternate kicking out the cache lines belonging to other processors)
* **Constructive interference** ⟶ loads by one processor might pre-fetch lines for another processor
![Pasted image 20241112135750](../../attachments/Pasted%20image%2020241112135750.png)

## Example: SUN Niagara 2
* Each processor has their own L1 cache, but processors share L2 caches
* Communication is done via a crossbar switch
	* Crossbar connects all processors to all of the L2 caches
	* Crossbar scales at $N^2$ so we can't have too many processors
![Pasted image 20241112140105](../../attachments/Pasted%20image%2020241112140105.png)

# Solution 2: Snooping Cache-Coherence
* **Main Idea:** all coherence-related activity is broadcast to all processors in the system via the interconnect
	* More specifically, broadcast is received by the processor's cache controllers
* Cache controllers monitor ("snoop") memory operations, and follow **cache coherence protocol** to maintain memory coherence
![Pasted image 20241112140328](../../attachments/Pasted%20image%2020241112140328.png)
## Write-through Coherence
* With write-through caches: **memory is always up to date** (as soon as we write to cache, we also write to memory)
* Coherence protocol:
	* When one processor writes, it invalidates the cache of all other processors
	* When other processors read, its a cache miss and they need to read from memory (which is up to date)
![Pasted image 20241112140647](../../attachments/Pasted%20image%2020241112140647.png)
![Pasted image 20241112140708](../../attachments/Pasted%20image%2020241112140708.png)
* Problems:
	* Bad performance ⟶ we have a lot of memory traffic
		* other processors have to go to memory for updated value
	* **Write through policy is inefficient**
		* Every write operation goes to memory (high bandwidth requirements)

## Write-back Coherence
* **We want to use a write-back policy**
	* Write-back policy abosrbs most write traffic as cache hits
	* Significantly reduces bandwidth requirements, but harder for coherence
![Pasted image 20241112141534](../../attachments/Pasted%20image%2020241112141534.png)
* The two important properties of a bus are:
	* Bus is a **broadcast** medium
		* everyone hears everything that is broadcast on the bus
		* i.e. all processors can observe all memory transactions happening on the bus
	* Bus **serializes** access
		* if you want to speak, you have to put your hand up and wait your turn
		* i.e. memory transactions occur one at a time in a well-defined order, preventing race conditions
* When we write to a write-back cache: **it marks the cache line as dirty**
	* Dirty bit ⟶ when a cache line is dirty, it means that the cache line has been modified and the new value is not yet in memory
* When a cache line is marked as dirty, it means this cache has exclusive ownership of that line:
	* Modified: This cache has the only valid copy of the line in the system
		* Only this cache can write to the line
		* Memory's copy is outdated
	* Owner: This cache must provide the data to other processors that request it
		* If another processor tries to read the line from memory, they would get stale data
		* The owner cache must intervene and provide the updated value
![Pasted image 20241112141949](../../attachments/Pasted%20image%2020241112141949.png)
* We can keep track of this state using the `line state` bits in the cache line
![Pasted image 20241112142059](../../attachments/Pasted%20image%2020241112142059.png)

# 2.1 MSI Write-back Invalidation Protocol
![Pasted image 20241112142143](../../attachments/Pasted%20image%2020241112142143.png)
* When ever a processor does a write (`PrWr`), the cache controller broadcasts a `BusRdX` to all other caches to read with intent to modify
* If a cache line is in the modified state, it means that the cache has the only valid copy of the line in the system and it can be written to (can do anything to it without initiating a bus transaction)
![Pasted image 20241112142236](../../attachments/Pasted%20image%2020241112142236.png)
* Read obtains block in "shared" ⟶ even if only cached copy
* We **obtain exclusive ownership** before writing
	* `BusRdX` causes other processors to invalidate their cache line
	* If in `M` in another cache, will cause other cache to writeback
* All caches carry out this logic independently to maintain coherence.

**Cases:**
* If we observe a `BusRdX` while in the **shared** state
	* This means another processor wants exclusive ownership of the line.
	* We must transition our own cache line to the **invalidate** state.
* If we observe a `BusRd[X]` while in the **modified** state
	* This means another processor wants to read our cache line (bc we have the latest copy of data)
	* We must initiate a **writeback** and move our line to the **shared** state (if no `X`) or **invalidate** state (if `X`).

## Example
1. When P1 reads `x`
	* we transact a `BusRd` and P1 cache goes into `shared` state
	* data comes from memory (no other processors in `modified` state yet)
2. When P3 reads `x`
	* we transact a `BusRd` and P3 cache goes into `shared` state
	* data comes from memory (no other processors in `modified` state yet)
3. When P3 writes `x`
	* we transact a `BusRdX` and P3 cache goes into `modified` state
	* P1 observes `BusRdX` and moves the P1 cache into `invalidate` state
4. When P1 reads `x`
	* we transact a `BusRd`
	* P3 observes `BusRd` and since it has `modified` state, it broadcasts a `BusWB` to say that it has the latest value.
	* P1 observes the `BusWB` and reads the value broadcasted from P3.
		* At the same time, this value is also written back to memory.
	* P1 and P3 cache both in `shared` state.
5. When P1 reads `x`
	* Data already in `shared`, so it does not need to send a bus transaction
6. When P2 writes `x`
	* we transact a `BusRdX` and P2 cache goes into `modified` state
	* P1 and P3 observes `BusRdX` and invalidates themselves
![Pasted image 20241112143553](../../attachments/Pasted%20image%2020241112143553.png)
* This is a **single writer, multiple reader** protocol.

## How Does MSI Satisfy Invariants?
1. Single-Writer, Multiple-Read (SWMR) Invariant
	* Only one processor/cache can be in `modified` state at a time
		* Because to get to `modified` state, we need to broadcast a `BusRdX` to all other caches to invalidate their cache line.
		* We can only modify the line when we are in `modified` state.
    * Multiple caches can be in read-only `shared` state ⟶ allows for multiple readers
2. Data-Value Invariant (write serialization)
	* On `BusRd` and `BusRdX`, data is provided by cache with line in M-state (via the `BusWB`)
	* Bus serializes all transactions

## MSI Summary
![Pasted image 20241112145849](../../attachments/Pasted%20image%2020241112145849.png)


# 2.2 MESI Invalidation Protocol
* Adds an `exclusive clean` state:
	* Processor has the only copy of the line in the system
	* Line is clean (i.e. not dirty)
	* Allows you to go from `exclusive clean` to `modified state` without telling anybody because no one else has the line.
![Pasted image 20241112150014](../../attachments/Pasted%20image%2020241112150014.png)
## State Transition Diagram
![Pasted image 20241112150203](../../attachments/Pasted%20image%2020241112150203.png)

# Solution 3: Directory Cache-Coherence
* Snooping schemes **broadcast** coherence messages to determine the state of a line in the other caches: **not scalable** and too restrictive
* Alternative idea: avoid broadcast by storing information about the **status of the line in one place** (a “directory”)
	* The directory entry for a cache line contains information about the **state of the cache line in all caches**.
	* Caches look up information from the directory as necessary
	* Cache coherence is maintained by point-to-point messages between the caches on a “need to know” basis
		* Instead of broadcasting to all caches, only talk to caches that contain the line
* Still need to maintain invariants
	* Single-writer, multiple-reader
	* Write serialization

## Example
* The L3 cache is inclusive ⟶ any line in L2 is resident in L3
* L3 keeps a directory:
	* For every cache line in L3, it knows which L2 cache has the cache line as well as its state (MESI)
	* Directory structured as `[line state][array of bits]
		* If state is `S`, any bit in array can be on.
		* If state is `M`, only **one bit** in array can be on.
		* If we see a request to write:
			* we tell the bits that are on to invalidate itself (turn off)
			* we turn on the writer
			* we set state to `M`
![Pasted image 20241112150436](../../attachments/Pasted%20image%2020241112150436.png)

# Implications of Cache Coherence

## Communication Overhead
* Communication time is a key parallel overhead
* Appears as increased memory access time in multiprocessor
![Pasted image 20241112151308](../../attachments/Pasted%20image%2020241112151308.png)

## False Sharing
* We can have unintended communication.
	* Recall that cache is stored in terms of lines, which can be more than what you want to communicate.
![Pasted image 20241112151837](../../attachments/Pasted%20image%2020241112151837.png)

### Example
![Pasted image 20241112151514](../../attachments/Pasted%20image%2020241112151514.png)
* Here we have a shared array across all threads.
	* This array will continuously bounce between the states as multiple threads attempt to read and modify the array.
	* This occurs even if threads access different indices, because a cache line might include multiple indices of array.

![Pasted image 20241112151520](../../attachments/Pasted%20image%2020241112151520.png)
* Here we ensure that each thread works on a different cache line.
	* This way, even if threads access different indices, they will not interfere with each other.
![Pasted image 20241112151805](../../attachments/Pasted%20image%2020241112151805.png)

### Impact of Cache Line Size on Miss Rate
* As the cache line size increases, notice that the miss rate due to false sharing increases.
![Pasted image 20241112152021](../../attachments/Pasted%20image%2020241112152021.png)

# Summary: Cache Coherence
![Pasted image 20241112152303](../../attachments/Pasted%20image%2020241112152303.png)
