
# Roadmap
* In this lecture, we do a case study on writing and optimizing a parallel program.
	* Use more cores/threads to get speedup.
* Demonstrated in two programming models:
	* data parallel
	* shared address space
![[Pasted image 20241005165221.png]]

## Steps
1. **Decomposition:** Identify work that can be parallelized
2. **Assignment:** Partition work (and associated data)
3. **Orchestration:** Manage data access, communication, and synchronization
4. **Mapping:** map the workers to hardware execution units.
![[Pasted image 20241005165233.png]]
# 1. Decomposition
* Break up problem into **tasks** that can be carried out in parallel.
* In general: create at least enough tasks to keep **all execution units** busy
* **Key challenge**: identify dependencies (or a lack of)
	* Dependencies can be data driven -- impossible to know at compile time.

## Amdahl's Law
* Dependencies limit the maximum speedup due to parallelism
* Let $S$ be the fraction of sequential execution (dependencies prevent parallel)
* The maximum speedup due to parallel (assuming infinite processors) is $\leq \frac{1}{S}$
![[Pasted image 20241005170109.png]]

## Example
* Suppose you want to do a two-step computation on an $N \times N$ image.
	1. Multiple brightness of all pixels by two
	2. Compute average of all pixels

* The sequential implementation takes $2N^2$ time
![[Pasted image 20241005165737.png]]

* **First strategy:** parallelize step 1 because there's no dependencies there.

| ![[Pasted image 20241005165810.png]] | ![[Pasted image 20241005165815.png]] | ![[Pasted image 20241005165829.png]] |
| ------------------------------------ | ------------------------------------ | ------------------------------------ |


* **Second strategy:** compute partial sums of image in parallel, combine results serially

| ![[Pasted image 20241005165921.png]] | ![[Pasted image 20241005165929.png]] | ![[Pasted image 20241005165935.png]] |
| ------------------------------------ | ------------------------------------ | ------------------------------------ |


# 2. Assignment
* Assigning **tasks** (things to do) to **workers** (threads, program instances, vector lanes, etc).
* Goal: achieve good workload balance and reduce communication costs.

* Programmers can perform **static assignments** or let complier do **static/dynamic assignment** (ISPC)
![[Pasted image 20241005170351.png]]

## ISPC Tasks: Dynamic Assignment
* If your machine has 8 hardware threads, ISPC creates 8 threads (workers) and begins assignment tasks to them.
	* Each **task is an independent** thing to do.
![[Pasted image 20241005170519.png]]


# 3. Orchestration
* Involves:
	* Structuring communication
	* Adding synchronization to preserve dependencies if necessary
	* Organizing data structures in memory
	* Scheduling tasks
* Goals: reduce costs of communication/sync, preserve locality of data reference, reduce overhead, etc. 
* Machine details impact many of these decisions
	* If synchronization is expensive, programmer might use it more sparsely

# 4. Mapping
* Mapping “threads” (“workers”) to hardware execution units
* Example 1: mapping by the operating system
	* e.g., map a thread to HW execution context on a CPU core
* Example 2: mapping by the compiler
	* Map ISPC program instances to vector instruction lanes
* Example 3: mapping by the hardware
	* Map CUDA thread blocks to GPU cores (discussed in a future lecture)
* Many interesting mapping decisions:
	* Place related threads (cooperating threads) on the same core (maximize locality, data sharing, minimize costs of comm/sync)
	* Place unrelated threads on the same core (one might be bandwidth limited and another might be compute limited) to use machine more efficiently

# Grid Solver Example
![[Pasted image 20241005170906.png]]
* Every dot on screen is updated with the average of its neighbors and itself.

## Sequential Algorithm
* Not only do we update each position `A[i, j]`, we also store it into a `diff` to calculate the total amount of change across the entire grid.
* If the average amount of change is small enough, we quit --> convergence.
![[Pasted image 20241005171035.png]]
## 1. Decomposition
### Identifying Dependencies
* If we go through the loop by column (`j` from 1 to n) and by row (`i` from 1 to n), the dependencies of each `A[i, j]` is the square above and to the left of it.
	* The squares above and to the left **must be computed first** before itself.
* In other words, we depend on **previous iterations of the i and j** loop.
![[Pasted image 20241005171434.png]]

* **There is independent work along the diagonals!**
	* The beginning and the end (a large fraction of the matrix) must be done in serial.
	* Memory access is not cache-friendly --> diagonals are not contiguous.
![[Pasted image 20241005171516.png]]
### Changing the Algorithm
* Sometimes, it is easier to **change the algorithm** to one that has more parallelism.
	* Change the order that grid cells are updated while ensuring it still converges.
![[Pasted image 20241005171914.png]]

## 2. Assignment
![[Pasted image 20241005172056.png]]

* Notice that in this program, there is a lot of inter-processor communication.
![[Pasted image 20241005172154.png]]

* Depending on the assignment, we might need to communicate a lot more.
	* In blocked assignment, P2 only needs **two rows** to compute its black cells.
	* In interleaved, P2 needs **six rows** to compute its black cells.
![[Pasted image 20241005172311.png]]

## Data-parallel solver
* Red cells are all independent, so we use `for_all` to assign them.
	* This code is run across many program instances who all cooperate on the loop.
	* `diff` is like a uniform variable, global across all instances.
		* We use `reduceAdd` to protect from race conditions of parallel updating shared var
![[Pasted image 20241005172904.png]]

## Shared address space solver
* Instead of using the `for_all` abstraction, we can also manually implement this via SPMD threads.
* Programmer is responsible for synchronization
* Common synchronization primitives:
	- Locks (provide mutual exclusion): only one thread in the critical region at a time
	- Barriers: wait for threads to reach this point
* We have **one piece of code** run **multiple times** by **multiple workers**
	* We use `getThreadId()` to compute the region of grid to work on.
	* We hand contiguous chunks of rows `(myMin, myMax)` to assign to each thread.
	* We **manually accumulate into `diff`** safely using a **lock** --> prevent race conditions
		* Without lock, `diff` may be less than it should be leading to early termination.

![[Pasted image 20241008132159.png]]

### Synchronization
* All threads **share** the same address space, so they can communicate via **shared variables**
![[Pasted image 20241008133546.png]]
### Locks
* We need to coordinate access to shared variables via locks.
![[Pasted image 20241008133658.png]]
* Operations on shared variables **must be atomic**
![[Pasted image 20241008134652.png]]

* Locks require communication between threads --> expensive.
	* Try to minimize using locks within parallel code -- accumulate into thread-specific variables then synchronize at the end.

| ![[Pasted image 20241008135504.png]]        | ![[Pasted image 20241008135518.png]] |
| ------------------------------------------- | ------------------------------------ |
| Lock is inside the loop ran by each thread. | Lock is outside the loop.            |

### Barrier
* Barriers divide computation into phases
	* All threads get held up at the call to `barrier(num_threads)` until enough threads has reached that call.
	* All computation by **all threads before the barrier** complete before any computation in any thread after the barrier begins
- In other words, all computations after the barrier are assumed to depend on all computations before the barrier

| ![[Pasted image 20241008135810.png]] | All three barriers are necessary here.<br><br>- Barrier 1 is needed because if one thread completes its work before the other threads get back to `diff = 0`, we might lose the work of that one thread.<br><br>- Barrier 2 is necessary because `diff` needs to be fully accumulated and all threads finished before we check for convergence.<br><br>- Barrier 3 is needed because all threads need to check `diff` before we reset it in the next iteration. |
| ------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

* We **can optimize this** use to **less barriers** ⟶ observe that the main phase of is per iteration of the outer while loop.
	* A thread can only be in one of three iterations: previous iteration, current, or next one
	* We just maintain three copies of `diff` and cycle them around.
![[Pasted image 20241008151143.png]]