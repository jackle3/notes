
# Programming for High Performance
**Key Goals**
* Balance workload onto available execution units
* Reduce communication (to avoid stalls)
* Reduce extra work (overhead)

**Tip #1:** Always implement the **simplest solution first**, then measure performance to determine if you need to do better.

# Balancing Workload
* Ideally, all processors are being used all the time during execution.
* In the example below, we have an imbalance — 20% of the work is serial.
![[Pasted image 20241008154115.png]]
* We can solve imbalance via different schemes of assignment

## Static Assignment
* Assignment of work to threads **does not depend on dynamic** behavior
	* The assignment is known when the amount of work and number of workers is known.
* Simple, essentially zero runtime overhead to perform assignment
* **Applicable when**:
	* work is predictable, but not all jobs have same cost (can figure out good assignment)
	* statistics about cost (execution time) is predictable (e.g. same cost on average)

## Semi-static Assignment
* We intermittently update the assignment as the program progresses.
![[Pasted image 20241008154857.png]]

## Dynamic Assignment
* Program determines assignment dynamically at runtime to ensure a well-distributed load.
* **Applicable when**: The execution time of tasks, or the total number of tasks, is unknown or unpredictable.

### Work Queue
* In the program below:
	* Each piece of work is testing one number `x[i]` for primality.
	* Because `i` is incremented during work (by a thread), we don't know which thread gets which iteration of `i` ⟶ threads gets work via a first-come-first-serve basis.
	* This program makes it so that all threads are busy all the time — even if one thread takes long, other threads pick up the work from queue.
![[Pasted image 20241008155028.png]]
* In dynamic assignment, you will **need synchronization** for the threads to communicate.
![[Pasted image 20241008155306.png]]

### Overhead
* The lock and unlock was not present in the serial version — this is overhead.
* Overhead can outweigh speedup if **each piece of work was too small** (e.g. if `test_primality` did not take any meaningful time).
	* If you made one task per element, you have good workload balance but **high synchronization overhead** because of the lock.
	* To test, comment out the work and compare time to **get lock** vs time to **get lock + work**.
![[Pasted image 20241008160321.png]]
* You can increase task granularity (more elements per task) to reduce overhead.
![[Pasted image 20241008160718.png]]

### Choosing Task Size
* When **dividing work into threads**, we generally create the same number of software threads as the number of hardware threads for maximum performance.
* With ISPC, we **divide work into tasks**. Suppose we have 8 threads.
	* If we only made 8 tasks, some tasks would finish quicker and not have any additional work.
	* If we made more than 8 tasks, threads that finish quicker can pick up additional work. This significantly **reduces idling caused by work imbalance**.
* Summary:
	* Choose more tasks than processors
		* This enables enable workload balance with dynamic assignment
		* Motivates small granularity tasks
	* Want as few tasks as possible
		* This minimizes overhead from synchronization
		* Motivates large granularity tasks

### Work Imbalance
* If the scheduler runs the long task last, we have huge load imbalance.
![[Pasted image 20241008155717.png]]
* One solution: divide work into **larger number of tasks**
	* This hopefully makes the long task shorter relative to others.
	* This may increase overhead, or just not possible to do (long task may need to be serial).
* Another solution: schedule **long tasks first**
	* Thread perform long task performs less overall work, but now its not imbalanced.
	* Requires knowledge of workload (some predictability of cost).

### Decreasing Synchronization
* We can have **distributed queues** — each worker uses own queue; once their queue is empty, steal from neighbors.
* This avoids the need for all workers to synchronize **on a single work queue**
![[Pasted image 20241008161241.png]]

### Work Dependencies
* Workers can submit new tasks (with optional explicit dependencies) to task system
* Ensures that tasks cannot be assigned until all dependencies are satisfied
![[Pasted image 20241008161454.png]]

## Summary
* Challenge: achieving good workload balance
	* Want all processors working all the time (otherwise, resources are idle!)
	* But want low-cost solution for achieving this balance
		* Minimize computational overhead (e.g., scheduling/assignment logic)
		* Minimize synchronization costs
* Static assignment vs. dynamic assignment
	* Really, it is not an either/or decision, there’s a continuum of choices
	* Use up-front knowledge about workload as much as possible to reduce load imbalance and task management/synchronization costs
	* If the system knows everything, use fully static assignment

# Common Parallel Patterns
* For every element of my array, please apply `foo` onto `A` and store into `B`
![[Pasted image 20241008161718.png]]
* For all threads, this is what the thread should do.
![[Pasted image 20241008161813.png]]

# Fork-join Pattern

## Divide and Conquer
* These algorithms give way to parallelism — subproblems are independent work.
![[Pasted image 20241008161934.png]]

## Cilk
* When you call a function, **caller** function stops. Thread enters **callee** and finishes it. Then exits back to **caller** and continues.

* With `cilk_spawn`, invoke **callee** `foo` but **caller** can continue executing at the same time.
	* Processor can execute the callee whenever it wants.
	* Difference between this and threads is that there is a **scheduler** for spawned calls.
![[Pasted image 20241008162311.png]]
* `cilk_sync` may only return once all the calls spawned has finished.
![[Pasted image 20241008162325.png]]

## C Vs `cilk`
* This is what a regular function in `C` looks like:
![[Pasted image 20241008162513.png]]

* This is what various permutations of a `cilk` function looks like:
![[Pasted image 20241008162538.png]]

* Notice that the `cilk_spawn` abstraction **does not specify how or when** spawned calls are scheduled to execute
	* Only that they **may be run concurrently** with caller (and with all other calls spawned by the caller)
* But `cilk_sync` does serve as a constraint on scheduling
	* All spawned calls must complete before `cilk_sync` returns

## Quicksort
* Revisiting **quick_sort**, this is how it would look under `cilk`.
	* We **spawn** on the **first half**. We do work on the **main thread** for the **second half**.
![[Pasted image 20241008162733.png]]

# Main Idea
* Expose independent work (potential parallelism) to the system using `cilk_spawn`
* *Some* scheduler is responsible for taking these calls and doing them in a way that is *good*.

# Spawning and Scheduling
## Basic Implementation
* Consider very simple scheduler:
	* Every time you see `cilk_spawn`, launch a `pthread` with `pthread_create` and run it.
	* Translate `cilk_sync` into appropriate `pthread_join` calls
* Problem:
	* Creating threads is expensive (more so than function calls)
	* We may create many more `pthread`s than cores — context switching overhead.

## `cilk` Implementation
* `cilk` maintains a pool of **worker threads** (exactly as many threads as execution contexts)
![[Pasted image 20241008163246.png]]

* Each thread maintains a **queue** of remaining work.
	* If one thread goes idle, it **steals work** from other busy threads, moves it into its own work queue, and begins executing it.
![[Pasted image 20241010141217.png]]

### Continuation Vs Child First
* At spawn, should the calling thread run the child or the continuation first?
![[Pasted image 20241010141418.png]]

**Continuation First**
* With this example program, we run continuation first. This means we every time we see `cilk_spawn`, we push it **onto the queue** and **continue the loop**.
![[Pasted image 20241010141507.png]]

**Child First**
* In this case, as soon as it sees `cilk_spawn` it runs the function. It pushes the continuation, which is the rest of the loop with `i` advanced by 1, back onto the queue.
* There's a ton of communication here — each iteration, it does one piece of work and kicks off the rest of the work to the queue.
![[Pasted image 20241010141647.png]]

### Work Stealing
#### Quicksort Example
* Suppose we are doing the run child first scenario. We work on the spawn, which is **the first half**, and we enqueue the continuation.
* We are doing quick sort on 0 to 200:
	1. We immediately spawn 0 to 100 and **work on it**, and kick 101 to 200 onto the queue.
	2. While working on 0 to 100, we spawn 0 to 50 and work on it, and kick 51 to 100 to the queue.
* By observation, the work queue has the same state as the regular call stack of a serial program.
![[Pasted image 20241010141923.png]]

#### Double-ended Queue
* We can implement the work queue for each thread as a double ended queue (deque)
* **Local** thread pushes/pops from the tail (**bottom** of queue)
* **Remote** threads steal from **head** (top)
![[Pasted image 20241010142321.png]]

* We need to make sure there is enough work so that **all the queues** stay full — to prevent stealing things back and forth.
* The **head** has the **biggest piece of work** — steal more work so we don't have to steal as often.
![[Pasted image 20241010142437.png]]

#### Actual Implementation
* Idle threads choose a random victim thread to steal from.
* It steals work from the **top** of the deque:
	* Steals largest amount of work (reduce num of steals)
	* Maximum work locality for each thread (when combined with run child first scheme)
	* Stealing thread and local thread do not contend for same elements of dequeue (efficient lock-free implementations of dequeue exist)

### Divide and Conquer
* `cilk` works super well in a divide and conquer mechanisms — the compiler actually breaks up traditional for loops into divide-and-conquer loops.
![[Pasted image 20241010143003.png]]

# Syncing
* In order to implement syncing, we need to know when the last continuation (in this case $i = 10$) has finished.
![[Pasted image 20241010143420.png]]

* When there is **no stealing**, we can sync for all calls signed within a **block**.
	* Since work is not stolen, it's basically serial. As soon as `foo(9)` is done, the block is done.
![[Pasted image 20241010143502.png]]

* When there is **stealing**, `cilk` keeps a descriptor for the block noting how many things have been spawned and how many has finished.
![[Pasted image 20241010143612.png]]
![[Pasted image 20241010143727.png]]
* When a thread finishes, it updates the `done` descriptor and finds new work.
![[Pasted image 20241010143736.png]]
* When the last thread (`thread 2`) finishes, `spawn` equals `done`.
* It can now resume execution and knows it can continue past the sync because all items in the descriptor is finished.
![[Pasted image 20241010143945.png]]
![[Pasted image 20241010143852.png]]
* Once the spawns are done, the continuation is now to do `bar`.
	* Notice that `thread 0` initiated the spawns, but `thread 2` does the `bar` because it finished last.
![[Pasted image 20241010143937.png]]

# `cilk` Summary
* `cilk` uses greedy join scheduling policy
	* All threads always attempt to steal if there is nothing to do
	* Threads only go idle if there is no work to steal in the system
	* Worker thread that initiated spawn may not be thread that executes logic after `cilk_sync`
* Remember:
	* Overhead of bookkeeping steals and managing sync points only occurs when steals occur
	* If large pieces of work are stolen, this should occur infrequently
	* Most of the time, threads are pushing/popping local work from their local dequeue
* Fork-join parallelism: a natural way to express divide-and-conquer algorithms
	* Discussed Cilk Plus, but many other systems also have fork/join primitives (e.g., OpenMP)
* Cilk Plus runtime implements spawn/sync abstraction with a locality-aware work stealing scheduler
	* Always run spawned child (child-first, continuation stealing)
	* Greedy behavior at join (threads do not wait at join, immediately look for other work to steal)
