
# Roadmap
* So far, we’ve assumed **all processors are connected to a memory system** that provides the abstraction of a single shared address space.
* But the implementation of that abstraction can be quite complex.
* Today we cover **techniques for reducing the cost of communication:**
	* between processors
	* between processor(s) and memory

# Shared Address Space Model
* Recall that memory includes a hierarchy of caches on top of DRAM.
![[Pasted image 20241011160252.png]]
* With a **shared address space**, any core can **directly** reference any memory location.
	* For every core and iGPU, the wires connect cores to memory.
![[Pasted image 20241011160431.png]]
* One example of the wires is Intel's ring interconnect:
	* There's a network where **each core has its own L1 and L2 cache**, and the **L3 cache is split** up into four local slices placed next to the cores.
	* When we load, if it's not in our own L1/L2 cache or our L3 slice, it routes the request across the ring bus into the appropriate L3 cache slice.
![[Pasted image 20241011160507.png]]

* Communication abstraction
	* Threads read/write variables in shared address space
	* Threads manipulate synchronization primitives: locks, atomic ops, etc.
	* Logical extension of uniprocessor programming
* Requires **hardware support** to implement efficiently
	* Any processor can load and store from any address
	* Can be costly to scale to large numbers of processors (one of the reasons why high-core count processors are expensive)

# Message Passing
* In the shared address space model, threads communicated by reading and writing to variables in the shared address space.
* Let’s consider a different abstraction that makes explicit communication between processors.

## Message Passing Model (abstraction)
* Threads operate within their own private address spaces
* Threads communicate by sending/receiving messages
	* **send:** specifies recipient, buffer to be transmitted, and optional message identifier (tag)
	* **receive:** specifies sender, buffer to store data, and optional message identifier
	* Sending messages is **the only way** to exchange data between threads.
![[Pasted image 20241011161258.png]]

## Implementation
* Supercomputers don't implement a single shared address space for all processors
	* It only provides mechanisms to communicate between computer nodes.
	* We connect systems together to form a large parallel machine
* The message passing model is for clusters and supercomputers.

# Message Passing Solver
![[Pasted image 20241013134405.png]]

## Network
![[Pasted image 20241013134418.png]]

## Model
* The grid is chunked up into each thread, which has **its own private address space**
* Threads need to send and receive rows between one another to work!
![[Pasted image 20241013134451.png]]

## Data Replication
* Thread 2 needs to allocate not just its own rows, but also **two additional rows** to receive rows from its **neighbors**.
	* Notice that the width of the grid is N + 2. Thread 2 allocates a `rows_per_thread + 2` times `N + 2` matrix.
![[Pasted image 20241013134905.png]]

## Full Implementation
* This is code that's ran by every thread in an SPMD manner (one function, ran by multiple instances in parallel on different input assignments).
* Notice that there **are no global or remote variables**. We communicate by using **messages**:
	* `send(address in local space, # of bytes, tid that receives this, identifier)`
	* `receive(address in local sapce, # of bytes, tid that sends this, identifier)
![[Pasted image 20241013135056.png]]
* Notice the **ghost row** receiving:
	* If we are not the first row, receive from the row above `tid - 1` and put into `localA[0,0]`
	* If we are not the last row, receive from below `tid + 1` and put into `localA[rows_per_thread + 1]`
	* A similar pattern is seen in the **ghost row sending**.
* Notice that the calculation of `diff` is done **only by thread 0**.
	* All other threads send local `my_diff` to `tid0`, then waits to receive `done` from `tid0`
	* Thread 0, once it receives everything, calculates `diff` then sends `done` to all other threads.

## Notes
* Array indexing is relative to **local address space**

* Communication:
	* Performed by sending and receiving messages
	* Bulk transfer — communicating entire rows at a time

* Synchroniziation:
	* Performed by sending and receiving **blocking** messages
![[Pasted image 20241013135920.png]]

## Deadlock with Synchronous send/recv
* **Deadlock** occurs when everyone is waiting for someone else to do something.
* The program, as implemented, will result in **deadlock**.
	* `send()` does not return until the receiver acknowledges (ie. finished its `receive()`).
	* `receive()` does not return until the data is present from the sender.

* More specifically, it occurs right here:
	* When it starts, every thread sends a row and **waits for receiver to acknowledge**
	* But if everyone is sending, no one is able to receive ⟶ deadlock!
![[Pasted image 20241013141833.png]]

* To avoid deadlock, we can do an assignment:
	* every even row **sends then receives**
	* every odd row **receives then sends**
* This makes it **both parallel and fixes deadlock**
![[Pasted image 20241013142656.png]]

## Async send/recv
* `send()` is now async. Returns a **handle** immediately.
	* It can then call `checksend()` on the **handle** later on to see whether it has been received.
* `recv()` also returns immediate. Returns an **intent** immediately.
	* It can then call `checkrecv()` on the **intent** to see if its received yet or not.
![[Pasted image 20241013142816.png]]
* Using the async `send/recv` would've also avoided the deadlock.

# Communication
![[Pasted image 20241013143132.png]]

## Memory Hierarchy
* Think of a parallel system as an extended memory **hierarchy**. We want to minimize communication between these.
![[Pasted image 20241013143159.png]]

* For example, CPU to memory communication requires going through this hierarchy.
![[Pasted image 20241013143254.png]]

## Recall: Bandwidth Limitation
![[Pasted image 20241013143414.png]]
![[Pasted image 20241013143408.png]]

* If you look at any point in the x axis and go up, you can see that the **memory bus is fully utilized**.

* If you increased memory latency, the distance between the math instruction and the start of its corresponding read gets longer.	![[Pasted image 20241013143723.png]]
	* This means the blue bar increases, and the width of the stalled red region increases.
	* **But**, memory utilization does not change — still 100%.
	* Recall:
		* **Latency** is the amount of time between when you issue the load request and you get the answer back.
		* **Bandwidth** is the rate at which you get answers back.

* If memory bus bandwidth was increased, the length of the blue bar shrinks.
	* The blue bar corresponds to the time to transfer data from memory to the processor.
	* By increasing the bandwidth, more data can be transferred in a given amount of time.
	* As a result, the data transfer completes faster, and the blue bar duration decreases.

* In most systems, latency does not matter as much as bandwidth.
	* You can multithread to hide the memory latency.

## Arithmetic Intensity
* If the ratio of math instructions to load instructions was significantly increased, would there still be processor stalls?
	* If we have more math instructions than memory load time, we could have 100% utilization of the processor (orange) but less than 100% utilization of the blue (memory)
	* The **ratio** is the width of the orange bar compared to the width of the blue bar.
![[Pasted image 20241013144800.png]]

* If you want to maximize **arithmetic intensity**, the solution is often **reducing communication**

# Inherent Communication
![[Pasted image 20241013145343.png]]
![[Pasted image 20241013144953.png]]
* The program on the right is bound by communication — much lower arithmetic intensity.
![[Pasted image 20241013145045.png]]
* Instead of communication the entire row, we can **communicate the perimiter** of the chunk.
![[Pasted image 20241013145158.png]]

# Artifactual Communication
![[Pasted image 20241013145348.png]]
![[Pasted image 20241013145424.png]]

## Row-major Traversal
* Recall that in order to update a red dot, you need the black dots neighboring it.
	* Suppose you had cache lines that had 4 grid elements each.
	* Every time you update a dot, you need to load in **three cache lines** for each neighboring row.
![[Pasted image 20241013145433.png]]
* At the end of the row, the cache contains the previous lines and the current lines.
![[Pasted image 20241013145457.png]]
* When we start processing the next row, **we get a cache miss.**
	* This effectively means we load three cache lines every four red dots.
	* The ratio of bandwidth to work is [every four elements] ⟶ [three cache line loads]
![[Pasted image 20241013145505.png]]

## Reducing Artifactual Communication
### Grid Traversal Order
* We can yield better performance by changing the order of computation ⟶ minimize cache loads.
	* We're now doing more work for the same amount of bandwidth.
	* For every six red dots, we load two cache lines (horizontal chunks of four dots).
![[Pasted image 20241013145630.png]]
### Loop Fusion
* This not only increases our arithmetic intensity (less loading per math), but also improves our memory.
	* We no longer need to allocate `tmp1` and `tmp2` on our memory.
![[Pasted image 20241013145655.png]]
### Co-locating Tasks and Sharing Data
![[Pasted image 20241013145701.png]]

# Contention
* A resource can perform operations at a given throughput (number of transactions per unit time)
	* Memory, communication links, servers, CA’s at office hours, etc.
* Contention occurs when **many requests to a resource are made within a small window of time** (the resource is a “hot spot”)
* **Key Idea:** contention for shared resource results in longer overall operation times (and likely higher cost for tasks)
![[Pasted image 20241020143610.png]]
![[Pasted image 20241020143632.png]]

# Summary: Reducing Communication Costs
![[Pasted image 20241020143653.png]]

# Key Tricks
* Always try the simplest parallel solution first, then **measure performance** to see where you stand.

## Performance Analysis
* Determine if your performance is limited by computation, memory bandwidth (or memory latency), or synchronization?
	* E.g. comment out the commute, etc to see what the program is bounded by?
* Try and establish “high watermarks”
	* What’s the best you can do in practice?
	* How close is your implementation to a best-case scenario?
![[Pasted image 20241020144659.png]]

## Roofline Model
* In plot below:
	* X axis correspond to different programs with **different arithmetic intensities**
	* Y axis is the **maximum obtainable instruction throughput** for a program with a given arithmetic intensity
* In the diagonal:
	* We are memory bound ⟶ memory is providing as much data as fast as it can.
	* When we increase the arithmetic intensity (more operations per byte loaded), we can do twice as much work per second in the memory bound case.
* In the flat region:
	* We are compute bound ⟶ you have so many operations per load. The rate at which you can finish those operations is bounded by your compute (memory is not fully utilized).
	* To optimize, we would probably want to improve our code itself to do less compute.
![[Pasted image 20241020143937.png]]
* In the graph, the **green** processor has four times as much compute as the **blue** processor.
	* Suppose that both processors are hooked to the same memory system.
	* The flat region of the blue is at around 16 GFlops/s. The flat of the green is around 64 GFlops/s.
	* The flat region is also **shorter for green** bc its memory bounded for longer (takes more intensity to reach compute bound).

* You can use the roofline model to optimize code.
	* Well written code will follow the roofline plot. Unoptimized code might be below it.
![[Pasted image 20241020144637.png]]
