

**About Me**
* Founder and CEO of Reddit

**Scale of Reddit's Platform**
* Will be 19 years old this summer -> started in 2005
* Were in the first class of YC -> also in that class were Sam Altman, Twitch, etc.
* Reddit was sold in 2006. Steve left in 2009 and came back in 2015.
* They just went public at the end of March.

**Security Problems**
* In the beginning, problems were mostly:
  * Spam posts
  * Algorithm manipulation problems
    * People cheating the upvote/downvote system to get to the front page
  * Both are these are *forever issues* -> still persist today
* Reddit had a leaderboard for top users.
  * The top user was someone who worked at an ISP. He had access to an unlimited number of IP addresses. He made a bunch of fake accounts to boost his karma.
* Spam posts.
  * They remove hundreds of millions of post per month, 95% of which are spam.
  * However, the biggest spam is thirsty marketers. Also includes, political campaigns, etc.
    * They manipulate posts to get more attention on their products.

**Election**
* This is the 4th US election that Reddit has to go through.
* This year, they’ve hit the limits of political campaign’s ability to manipulate on Reddit.
  * In the process of preventing manipulation, spam, etc.. they’ve largely prevented this.
  * Nothing they can do to prevent dumb people from doing dumb.
    * Misinformation is mostly *dumb Americans being dumb.*
* They have seen some manipulation attempts from nation-states.
* Whoever is in power, they are extremely critical of rhetoric against them.
  * They then say that “the system is broken”, etc.
  * The one in power is the one doing the complaining against Reddit

**Moderation**
* Reddit is in contrast to social media.

* Social media is powered by friends, families, followers, and famous people.
  * These are the people who generate *impression* and views on the site.
  * They have some algorithm that cherry pick content and make it popular by putting it in front of people.
    * These optimize for posts consumed and/or time on site -> maximize engagement.
  * If you’re popular on social media, you get credit for that in the real world.
    * To be popular, people try to make posts engaging and interesting.

* Reddit works differently.
  * Every content on Reddit starts at zero points, and it gains visibility by users voting or downvoting on it.
  * Things can’t become popular unless a community on Reddit wants it to be popular.

* Most of the moderation at Reddit is whether a *community* or *behavior* is good or bad.
  * This is a lot simpler than what social media is grappling with.
  * They don’t really moderate individual posts -> they do so at the community level.
    * Users self-moderate the posts. If the post is bad, they won’t upvote it.
      * 99% of people are good and want to have good experiences, so they self-moderate the posts in their community to create nice experiences.
    * However, Reddit still reviews and removes posts that are reported.

**Building Trust and Safety**
* Before 2015, there were not any real controls or moderation.

* Steve came back because Reddit was doing so bad.
  * At the time, it had no safety team.
  * It also had no content policies.
    * The policy was that they don’t remove anything.
      * The original intention was that Reddit would not censor anything.
      * However, at the time the policy was made, there wasn’t anything to remove.
        * Overtime, 
    * Reddit did not really moderate communities, etc.
  * By the time he came, there were more things they need to remove (e.g. racist stuff)

* He had to build the trust and safety to ensure people get good experiences.
  * Wrote a content policy, and created the safety team.

* Very strong and clear policies are brittle. It is good to have some ambiguity to allow you to practically moderate.
  * When a new policy was made, people would just skirt the line.
  * E.g. they have a policy to not allow racist stuff.
    * Then people made a community where all the posts had titles about people committing crimes, and all the pictures were of raccoons.
    * It was meant to be a racist community insinuating that black people were criminals.

* Content policies should be **specifically vague**.
  * Normal people should be able to read it and know what it means.
  * But malicious people should not be able to skirt the line.
  * E.g. laws that are short and specifically vague are much more enduring.

**Advertisers**
* In addition to content policies, they have a more specific ad policies.
  * They do not allow political ads.
  * Ads are not allowed to cause anger, incite negative reactions, etc.
  * Ads are human reviewed.
* Ad companies try to skirt the line of content moderation to try to maximize engagement.

* On platforms where ads make up revenue, advertisers can control the media narrative.
  * Advertisers will say “we don’t like the way you’re moderating this topic, so we’ll pull our advertisements unless you do something different”
  * There are a lot of politics of business related to this.

* By and large, advertising is generally good for culture.
  * It keeps platforms and publishers in the middle, because advertisers get skittish when the platform or people gets too extreme.

**Detection**
* Question: What technologies help you identify problematic community and posts?
* Up until now, moderation on Reddit is almost all powered by humans.
  * *Moderators* (users) on moderate communities on their own.

* Sometimes they use ban lists of negative words. However, these are brittle.
  * E.g. if they ban the word “Karen”, moderators spend a significant amount of time just adjudicating the use of these words.

* To moderate a community:
  * They look at it and see how it feels.
  * They also look at the post removal rate of the community.
    * If a community has a lot or very little removed posts, that may indicate bad.
    * They see how much the moderation team disagrees with the admins.

* Language models are extremely powerful for content moderation.
  * This is mostly because they can consider additional details and sentiment.
  * It scales significantly better, and it can moderate in any language.
  * It also reduces the workload of moderators, creates better relationships.
    * E.g. if a user is banned, moderators are now reviewers, since the LLM was the one who banned them

**Moderation at Scale**
* A few facts:
  * Facebook has all sorts of rules, an external review board, require a real-world identity.
    * But, they still have an unlimited supply of bad behavior and knee-jerk reactions related to political issues.
    * Lesson: Real-world identity does not necessarily prevent bad behavior.

* Reddit’s public content policy is not that detailed.
  * Just a couple hundred words describing major ground rules.

* Right now, Steve’s team is the only one who can ban entire subreddits.
  * However, he would prefer he just be in charge of Executive, and leave the Legislative and Judicial stuff to the communities.
  * In other words, leave the banning and moderation to the community.

* If a community has no moderator, it gets instantly banned. This is Reddit policy.

**Job Advice**
* He originally thought that with the rise of LLMs, CS jobs might be the last to go instead of the first.
  * LLMs are surprisingly extremely good at programming.

* Look at the world through the lens of engineers -> observe a problem, and think about how it can be fixed.

* CS skills and programming are still very practical. 

* Advice: Don’t worry about it. Life is really long, so you can do a lot of things. You can start a company, work at big companies, work at startups, etc.
  * If you’re learning, good.
  * If you’re not learning, make a change.