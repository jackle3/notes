---
Week: Week 3
---
# Royal Hansen

## Introduction to AI and Its Impact

- **Context Windows and AI Capabilities**: Hansen discusses the advancements in AI, notably "long context windows" which allow models to process and analyze large amounts of data (millions of tokens) simultaneously. This capability enables new uses, such as generating personalized audio content from scientific papers.
- **Ongoing Developments**: Emphasizes the rapid pace of AI innovation within environments like Google, suggesting that applications and implications of AI are continuously evolving.

## Historical Use of AI at Google

- **Early AI Integration**: Google has integrated AI into its systems for a long time, using machine learning models for various security tasks:
    - **Access Control Lists**: Used to manage complicated infrastructure permissions.
    - **Phishing Detection in Gmail**: AI models analyze patterns to automatically identify phishing attempts.
    - **Malware Detection on Play Store**: Scans apps to detect and block malicious activities.
    - **Content Classification**: AI helps in moderating content by identifying inappropriate or harmful material, demonstrating the breadth of AI application in maintaining platform security and integrity.

## AI Security Risks

- Phishing/Social Engineering: Language models can be highly effective social engineering tools to manipulate users. The latest trend is "pig slaughtering" where an AI develops a relationship with the target over weeks/months to gain their trust before exploiting them financially.
- Malware: Malware can use language models to dynamically adjust its attack based on the target system configuration to avoid detection by security software.
- Misinformation/Deepfakes: Cheap deepfake technology can create highly convincing audio/video misinformation.
- Bioweapons: DNA printers allow recreating viruses digitally transmitted over the internet by just knowing their base pairs sequence data.

## Trends and Defenses

1. Gen AI is the New UX
    - User interactions happen natively through language/video/images rather than predefined UI screens.
2. Foundation Models Becoming Agents
    - Models will work autonomously as agents taking actions in the real world like booking flights, not just providing information.
    - Need to control what interfaces/services these agents can access and thresholds for human oversight.
3. Data is Becoming Code
    - Massive complexity is encoded in model weights, which can be tuned to change behavior almost like code.
4. Critical Infrastructure
    - The expensive GPU/TPU infrastructure running these models needs robust security practices for code, access control, patching etc.

## Risk: Prompt Injection / Sensitive Data Disclosure

- Like SQL injection, but through natural language rather than constrained input validations.
- Defense: Prompt filtering and curation processes.

## Risk: Unauthorized Data Access / Data Poisoning by Agents

- AI agents may call internet services in an attempt to access unauthorized data.
- Poisoning data sources can manipulate model behavior in subtle ways.
- Defense: Constrain services/URLs agents can call, data sanitization and provenance practices.

## Risk: Insecure Model Development Lifecycle

- Entire software pipeline for developing, training, testing and serving models needs security.
- Risks include backdoored code, manipulated training data, buggy model evaluation etc.
- Defense: Secure development lifecycle, model artifact signing/verification etc.

## Risk: Unsanitized Output / Data Exfiltration by Plugins

- Agents interacting with models may cause unauthorized data leaks or take unintended actions.
- Defense: Output filtering, rate-limiting plugins based on trust levels.

## Other Considerations

- Bug bounty programs for AI models have blurred lines between vulnerabilities and model "hallucinations".
- Using language models to enhance traditional security techniques like fuzzing and file type analysis.
- Need industry-wide framework for securing AI systems akin to OWASP top 10 for web apps.