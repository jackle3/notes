
* Claim: AI Safety has dominated headlines and attention. However, AI Security is a far more valuable and impactful field, because AI Security is:
  * More impactful
  * Problems are harder to solve
  * More fun

* Anthropic Research
  * “Scaling Laws for Neural Language Models”
    * Showed that scaling laws was holding empirically for LLMs
    * The ability for a model to reason continued to improve as they threw more compute at the model
  * ”Language Models are Few-Shot Learners”
    * In GPT3 -> 10x the compute
    * Showed a two-fold result:
      * Loss curves did not diminish -> keep decreasing
      * Models were able to learn from few examples
  * Anthropic was founded by the GPT3 from OpenAI
    * Focused on the frontier of AI safety
    * After GPT3, realized that we did not know how these models worked -> hard to tease out the reasoning in the attention mechanisms.

* Discord/Midjourney
  * Midjourney is a text-to-speech model.
  * Showed that scaling laws also applied to image generation -> increasing compute leads to better performance.
  * Used Discord to let users pick one out of four generated images
    * High confidence signal of what images were good.
  * Safety issue -> Midjourney was getting very good at deepfakes

* Mistral
  * After Discord, joined Andreessen Horowitz and invested in Mistral.
  * The team behind LLaMa left Meta and started Mistral
    * Mistral created LLaMa -> the current leading open-source LLM

## What is AI safety vs. AI security?
* There is a ton of ambiguity between the definitions of the two. The definitions are fluid.
* Both deal with mitigating risks around frontier AI systems.
  * **Security** focuses on protecting AI from external bad actors and misuse.
    * Focused more on its use post-training and after deployment.
    * Focuses on Cybersecurity, Adversarial Attacks, Misuse Prevention, Access Control.
  * **Safety** focuses on ensuring the AI systems themselves are fundamentally safe and beneficial for society.
    * Guiding the model (in pre-training) regarding what the model can and cannot do.
    * Focuses on Alignment, Robustness, Interpretability, Testing/Validation.
  * In terms of inputs:
    * **Security** defends against deceptive inputs and malicious attacks, while **Safety** ensures the AI system is resilient to adversarial inputs.

## Shifts in Problems
* **Adversaries** are now getting bigger -> sponsored by nation-states
  * As such, the problems are becoming more impactful.

* **Entropy** is increasing -> models are stochastic
  * As such, problems are harder to solve.
  * Ex. Mistral CEO confirms “leak” of open source AI model nearing GPT-4 performance.
    * Handed weights of largest model to a group of early access customers on “secure” infrastructure for early feedback.
    * One of the customers held an internal hackathon without telling Mistral.
      * One of the engineers at this company plugged a thumb drive in and downloaded the weights, uploading them to an online message board.
    * Going forward, Mistral only allows testing on models on their own infrastructure
      * Customers have to log into Mistral’s infra to use it.

* **Novel problems**
  * As such, problems are more fun to solve.
  * Ex. CFO of British design firm was scammed for $25 million
    * Hackers had real-time deepfake technology and joined a Zoom call.
    * The CFO thought he was talking to ~six of his colleagues, but they were actually deepfaked.
    * He was tricked into wiring the money.

  * Ex. Golden Gate Claude
    * The state of the art in AI safety and alignment has been RLHF -> we tell the model “good model” and “bad model” when it does something, rewarding it, etc.
      * This presupposes that we can’t look into the guts of the model to change how it thinks **(treats the model as black boxes)**…
        * As such, we align the model by training it to do more or less of things and hope for the best.
    * Anthropic recently published research in interpretability.
      * Found many **clusters for different features** in the “mind” of Claude. Each cluster activates when the model reads relevant text or sees relevant images.
      * There’s a specific combination of neurons that activates when it encounters a mention of the Golden Gate Bridge.
        * When they turn up the strength of the “Golden Gate Bridge” feature, Claude’s responses begin to focus on the Golden Gate Bridge.
        * Its replies to most queries start to mention the Golden Gate Bridge, even if it’s not directly relevant.
      * **Huge breakthrough in AI interpretability**


