---
Week: Week 5
---
# Exam Info

- Last name is **A to L** → Dinkelspiel
- Last name is **M to Z** → Hewlett 200
- It’s a paper exam.
- Use the study sheet to prep for the exam.
    - Indicates all of the material that can be on the exam.
    - The exam will be closed-book.
- The long answer question has no expectation on length, just coverage.
    - You need to answer all of the parts of the question.
- When referencing the reading, they will give more information than just the last name and year → e.g. the title, the subject matter of the reading, etc.
- To prep for the readings, prep with the abstracts of the papers → know what they say and the conclusions that they have.

# What is replication?

- What is **replication**?
- Principle of reproducibility → it should be possible to conduct a scientific study again.
    - Allows researchers to check and extend each others work → accountability
- Principle of replication → it should be possible to conduct a scientific study again (ideally by new researchers) and **find the same results**.
- Forms of replication:
    1. **Direct replication** → read study, then do the exact same study, get same results.
        1. In reality, very difficult to do direct replication in social psych because we can’t possibly have every single factor be the same as the original study
            1. We need the same people with the same mindset/time (basically impossible)
    2. **Conceptual replication** → read study, go and design the study based on the original (details of study may differ but idea should be same), and get similar results.
        1. E.g. test the idea of the above-average effect, but change some stuff → move to a different population, move to different domains of evaluation, change metrics, etc.

## Replication failures

- **Direct replication failures**: you copied every single detail of a study, but you were not able to replicate the results.
    - Conclusions: could be that the original study was bad, or could mean something changed in the world.
- **Conceptual replication failure:** you replicate the idea of the study, but some things change.
    - Conclusions: could indicate a boundary condition → the thing you changed from the old study to the new study was significant.
        - E.g. for the above-average effect study, if we changed the domain of evaluation from education to something like juggling, we ended up getting the below-average effect.
- Or, replication failure could also mean the **original study was done poorly.**

# Replication problems

- What are the replication **problems** in social psych?
    - Most clearly in recent times, replication rates for studies in several fields have been low.
- Recent discoveries of middling or low replications rates in:
    - Social psychology
    - Behavioral economics
    - Medicine
    - Sociology

## Open Science Collaboration (2015)

- Replicated 100 studies from three major psychology journals.
- They found that original and replication effect sizes were correlated ($r=0.51$﻿)
- But only 36-47% of the original studies were successfuly replicated → got same statically significant result from original study in new re-run study.
    - More recent estimates are now ~50%.
    - There were many results that even had opposite effects as the original.

**Reasons for these results:**

- Direct replications are arguably impossible → different subjects, etc.
    - Many replication attempts were very different, more conceptual than direct.
- Some replication attempts were poorly done.
- The OSC only replicated one test from one study → the paper could still be generally true, just not that one study.
- The specific paper could be wrong, but the line of research could still be true.
- But also: **unreliable methodological practices** that have become prevalent in research.
    - The competition to publish was so high that people were cutting corners to be quicker.

## Methodological problems

- Also sometimes called **questional research practices.**
- Most of these revolve around problems with **statistical significance testing**.
    - Statistical significance testing is key tool for inferring if experiments find an effect.
- The “p value” tells you the probability that a difference in means is statically significant.
    - i.e. tells you that result is unlikely to be by chance.
    - lower p-value means result is more reliable → more statitiscally significant.
- **Field-wide norm:** if $p < 0.05$﻿, the effect is considered statistically significant → very unlikely to be a false-positive.
    - False postive = effect is observed to be statistically significant but is likely not a true effect.
- **Problem:** the p-value was being misused, whether intentionally or unintentionally, in the pursuit of publishing studies.
- **Questional Research Practices** → practices that skew the p-value.
    - Study **multiple dependent variables**, then only publish the statistically significant one.
        - E.g. measure three variables, one is statistically significant → study looks like that independent variable is significant, but it actually only affects 1 of 3.
    - **Arbitrary data exclusions** → remove and clean data that makes the data more significant.
        - E.g. after getting bad results, go back to the data to exclude outliers, then rerun.
    - **Optional stopping** → run the study and analyze it as you go along. Once you reach a p-value less than 0.05, then just stop the study there (don’t gather more samples)
        - Severe bias in how they are analyzing their results.
    - **Arbitrary tweaking and re-running statistical models.**
    - **Small samples** → making broad conclusions from small samples.

# Causes

- What are the **causes** of those problems?
    - Why did people engage in these questionable research practices?
- **Individual-level factors:**
    - Insufficient appreciation of biasing effects → not realizing how much these practices wouuld affect the reliability of the findings.
    - Feeling of discovery → after running 20 failed models and finding one that works, you feel like you must publish it as a successful discover.
    - Insufficient resources → not enough subjects, money, etc.
    - Motivated reasoning → researchers want to succeed, so they only report the successful studies.
    - Dishonest research practices → inventing data for the study, etc.
- **System-level factors:**
    - Disinterest in null effects → literature is not interested in studies that disprove a hypothesis.
    - Insufficient monitoring → did not have practices for evaluating datasets, people weren’t required to post data, etc.
    - Extreme competition → high inequality in professional rewards, so people are more inclined to cut corners to get successful results.

# Solutions

- What **solutions** have been proposed and implemented?

## Pre-registration

- A practice where you publicly post a research plan before conducting a study.
- This can include:
    - Sample size, data, exclusion rules, hypotheses, measures, statistical analyses.
- This practice restricts “researcher degrees of freedom”
    - It’s like a **pre-commitment** **to the study details**, while being unaware of the data and results.

## Registered Reports

- Research project is reviewed for publication **before the data is collected**.
- You pre-submit a paper to the journal, and the reviewer and editors of the journal evaluate the study.
    - Allows you to fix your study and paper before it is too late to fix it.
- Allows us to overcome the “positive results bias”
    - Evaluate the paper based on the research design itself, not on whether it produces good/flashy results.

# The Problem of Surprisingness

- Counter-intuitive findings are particularly appealing and interesting.
    - But it’s also less likely to replicate.
- In highly competetive, under-regulated markets for human belief, the surprisingness of the study can out-compete the validity of it.
    - This is an aspect of social science.

# Indicators of a reliable paper

- The indicators are:
    - Paper is preregistered.
    - Materials are posted publicly.
    - Sample size is large or based on power analysis.
    - Aligns with other research.
    - Results are not fantastical.
- With these features, we can achieve high replicability.
    - Protzko found that papers that performed these has 85% replication rate (very high)

# Culture Shift

- More transparency and care
- Openness to new methods and approaches
- Greater humility, reflexiveness, and self-criticism in science

# Implications

- What are the **implications**?
- Many specific published findings were debunked due to these replication studies.
    - Some research programs and theories were fully debunked.
    - Some conflict, but also lots of reform.
- With regards to the class:
    - We avoid research that we expect is unreliable.
    - We draw on meta-analyses and research programs.
    - The research we teach does not rely on a single study.
        - E.g. cognitive dissonance, attributions, above-average effect, stereotyping
    - Some studies are better understood to be demonstrations than rigorous experiments
        - E.g., When Prophecy Fails (cognitive dissonance with doomsday cult)
    - Also, results can be challenged!