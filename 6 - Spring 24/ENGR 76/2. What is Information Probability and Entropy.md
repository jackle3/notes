---
Notes: Surprise, entropy
Week: Week 1
---
# Random Variable

- **Variable:** a quantity that can take on different values.
    - E.g. $X$﻿ is the location of a certain particle, $X \in [0, 5]$﻿
- **Random variable:** a quantity that can take on different values with some probability associated with each value.
    - E.g. $X \in \{1, 2, \dots, n\}$﻿ where $P(X = 1) = p_1, P(X = 2) = p_2, \dots$﻿

## Axioms

- Probabilities are less than one and non-negative: $0 \leq p_i \leq 1$﻿
- The sum of all outcomes is one: $\sum_{i=1}^n p_i = 1$﻿

## Examples

- $X$﻿: outcome of a fair coin flip
    - $X \in \{H, T\} $﻿ where $P(X = H) = P(X = T) = \frac{1}{2}$﻿
    - This is a uniform distribution.
- $X$﻿: outcome of a fair dice roll
    - $X \in [1, 6]$﻿ where $P(X = 1) = \dots P(X = 6) = \frac{1}{5}$﻿
    - This is a uniform distribution.
- $X$﻿: the first letter of a randomly chosen word in English.
    - $X \in \{A, B, C, \dots, Z\}$﻿
    - This is not a uniform distribution → some letters are more common than others.

## Expectation

- Also known as the mean or average of a random variable $X$﻿
- Suppose we have $X \in \{1, 2, \dots, n\}$﻿ with probabilities $p_1, p_2, \dots, p_n$﻿

$\mathbb{E}[X] = \sum_{i=1}^n i \cdot p_i$

- For instance, the expected value of a fair dice roll is

$\mathbb{E}[X] = \sum_{i=1}^6 i \cdot p_i = \frac{1}{6}(1 + 2 +3 +4 +5 + 6) = 3.5$

# Multiple random variables

- Suppose we do two coin flips, with a random variable for each.
    - $X_1$﻿: outcome of the first flip → $P(X_1 = H) = P(X_1 = T) = \frac{1}{2}$﻿
    - $X_2$﻿: outcome of the second flip → $P(X_2 = H) = P(X_2 = T) = \frac{1}{2}$﻿
- The outcome of this experiment is:
    - $X_1, X_2 \in \{HH,HT,TH,TT\}$﻿

## Independence

- Two random variables $X_1$﻿ and $X_2$﻿ are _independent_ if and only if the outcome of one does not affect that other.
    - In other words, $\forall (i,j). \quad P(X_1 = i, X_2 = j) = P(X_1 = i) \cdot P(X_2 = j)$﻿

## Joint probability

- This is the probability of two or more random variables getting some value.
    - E.g $P(X_1 = H, X_2 = H)$﻿
- Since the two flips don’t affect each other, $X_1$﻿ and $X_2$﻿ are independent.
    - The probability then just becomes $P(X_1 = H) \cdot P(X_2 = H) = \frac{1}{4}$﻿
- This is the same with the other scenarios.
    - $P(X_1 = H, X_2 = T) = P(X_1 = H) \cdot P(X_2 = T) = \frac{1}{4}$﻿

# Information Sources

- Examples of source: Books, text messages, emails, speech, photos, etc.
- Suppose for this, we focus on a text message that is limited in length.
    - It’s very difficult to quantify the information _value_ of a message.
    - **Length** - we think that if a message is longer, it likely provides more information.

## Surprise

- **Claude Shannon** wrote “A mathematical theory of communication” in 1948.
- He said: the information value of a source is related to with how _surprising_ it is.
    - We would measure surprise by how probable the message is.
        - The more probable, the less surprise that we have.
        - The less probable, the more surprise.
- The **surprise** $s(p)$﻿ should be a function of the probability $p$﻿ of that message.
    - Condition 1: $s(p)$﻿ should be decreasing with increasing $p$﻿
    - Condition 2: $s(p)$﻿ should be smooth and continuous in $p$﻿
- Let $X$﻿ and $Y$﻿ be two independent messages.
    - $P(X = i, Y = j) = P(X=i) \cdot P(Y = j)$﻿ = $p_i \cdot q_j$﻿
    - The surprise for this joint event is $s(p_i, q_j)$﻿.
    - Since the events are independent, the surprise for the joint events is:
        - Condition 3: $s(p_i, q_j) = s(p_i) + s(q_j) \quad \forall 0 \leq p_i, q_j \leq 1$﻿
- The only function that satisfies all three conditions is:
    
    $s(p) = \log_2 \Bigg(\frac{1}{p}\Bigg) = -\log_2(p)$
    
    - The base of the $\log$﻿ is arbitrary, though in this class we will use base 2.
        - The use of base changes the units of the resulting surprise factor.
    
    ![Untitled 49.png](attachments/Untitled%2049.png)
    
    - Observations:
        - Condition 1: $s(p)$﻿ is zero when $p = 1$﻿, and $s(p)$﻿ increases with decreasing $p$
        - Condition 2: $s(p)$﻿ is clearly smooth and continuous on $[0, 1]$﻿
        - Condition 3:
			$$\begin{align*} s(p, q) &= \log_2\Bigg(\frac{1}{p \cdot q}\Bigg) \\ &= \log_2\Bigg(\frac{1}{p} \cdot \frac{1}{q}\Bigg) \\ &= \log_2\Bigg(\frac{1}{p}\Bigg) + \log_2\Bigg(\frac{1}{q}\Bigg) \\ &= s(p) + s(q) \end{align*}$$
            
        - The surprise function is always positive, so its easier to interpret.

## Entropy (Information Value)

- We will model information sources as a random variables.
    - Suppose we have $X \in \{1, 2, \dots, n\}$﻿ with probabilities $p_1, p_2, \dots, p_n$﻿
- The information value (or entropy) of $X$﻿ is its weighted average surprise.

$$\begin{align*} H(X) &= \sum_{i=1}^n p_i \cdot s(p_i) \\ &= \sum_{i=1}^n p_i \cdot log_2 \Bigg(\frac{1}{p}\Bigg) \end{align*}$$

## Properties

- Entropy is always positive → $H(X) \geq 0$﻿
- For a uniform random variable, the entropy is simply the $\log$﻿ of the number of different values the random variable can take.
    
    $H(X) = \sum_{i=1}^n \frac{1}{n} \cdot \log_2 \Bigg(\frac{1}{1/n} \Bigg) = \frac{1}{n} \sum_{i=1}^n \ \log_2 (n) = \log_2(n)$
    

## Example

- Suppose $n = 2$﻿.
    - We have $X \in \{1, 2\}$﻿ and $P(X = 1) = p_1$﻿ and $P(X = 2) = p_2$﻿.
    - We know that $p_1 + p_2 = 1$﻿, which means that $p_2 = 1 - p_1$﻿.
- The entropy is thus:
$$\begin{align*} H(X) &= p_1 \log \frac{1}{p_1} + p_2 \log \frac{1}{p_2} \\ &= p_1 \log \frac{1}{p_1} + (1 - p_1) \log \frac{1}{(1 - p_1)} \end{align*}$$
    
    - This function is also known as the binary entropy function.
    
    ![Untitled 1 16.png](attachments/Untitled%201%2016.png)
    
- This graph tells us that the entropy, or the value of information, is largest when the two outcomes of $X$﻿ are equally likely.