---
Notes: Intro, project overviews
Week: Week 1
---
# Topics

![Untitled 41.png](../../attachments/Untitled%2041.png)

- This course focuses on the software principles and techniques → functionality implemented with hardware
    - Error Correction Codes
    - Sampling and Quantization Techniques
    - Compression Algorithms
    - Frequency Domain Techniques

# Analog vs. Digital

- Analogy → take the analog signal (e.g. sound) and find a way to store it in a physical medium
    - E.g. store the analogic signal of sound into a magnetic tape, such that the amplitude of the signal is the same as the amplitude of the magnetic waves.
- Digital → convert any information source (e.g. analog, text, etc) into a sequence of bits.

## Bits

- The bit is **universal information currency**
    - A standardized binary representation for all sources → enables modular designs.
- Electronic circuits have become more and more digital, without loss of optimality.
    - **Source-Channel Separation Theorem (Shannon 1948):**
        - If a source can be transmitted over a channel **(via any interface)** at a certain resolution…
        - Then it can be transmitted using a **binary interface** between the source and the channel at the same resolution.

## Communication

- Analog Communication:
    
    ![Untitled 1 8.png](../../attachments/Untitled%201%208.png)
    
    - Message is one of a continuum of possibilities.
    - We can never fully remove the effect of noise.
        - It’s hard to tell whether variations are attributed to noise or attributed to the actual message.
- Digital Communication:
    
    ![Untitled 2 9.png](../../attachments/Untitled%202%209.png)
    
    - Message is one of a finite set of possible choices (bits).
    - Receiver can remove the effects of noise induced by the channel.
        - Suppose zeros had 0v and ones had 5v → if we’re varying around 0v, we can safely attribute that to noise and remove its effects.

## Information Science and Engineering

- There are two mains goals
    1. Efficient representation (compression) of analog signals in bits
    2. Preserving information (reliable communication) amidst noise

![Untitled 3 9.png](../../attachments/Untitled%203%209.png)

# Compression

- Image compression → representing images efficiently for storage and sharing
- For example, an image from a 12 MP camera has:
    - $12 \times 10^6$﻿ pixels
    - $3$﻿ color channels per pixel
    - $1$﻿ byte per color channel, per pixel
    - $12 \times 10^6 \cdot 3 \cdot 1 = 36$﻿ MB for the image
- This is a lot! To fix this, there are compression algorithms and formats like JPEG/PNG/HEIF.

![Untitled 4 8.png](../../attachments/Untitled%204%208.png)

## Project Overview

1. Compress the _same_ information into less space (lossless compression)
    1. What is information? (probability and entropy)
    2. Compression and fundamental limits (Huffman coding)
2. Remove information the human eye can’t see (lossy compression)
    1. Signals, frequency representation, bandwidth (discrete cosine transform)
    2. Quantization, sampling and reconstruction (from analog to digital)

# Communication

- Concerns sending information reliably over noisy channels.
    - Sends between an encoder and a decoder.

![Untitled 5 8.png](../../attachments/Untitled%205%208.png)

- The speaker is the one sending information to the microphone. However, there can also be noise from people and from the room.
    - Need to figure out how to distinguish between signal and noise/interference.

## Project Overview

1. What is communication? (channels and noise)
2. Representing bits for physical/analog transmission (modulation)
3. Bandwidth, spectrum shaping and sharing (frequency-domain filtering)
4. Fundamental limits (channel capacity)
5. Separation of compression and communication (separation principle)
6. Adding redundancy to correct errors (error-correcting codes)