# Memory Stalls
* We cannot access all memory in one cycle, especially if you have large amounts of memory.
	* Need to optimize the memory system and re-unify data/instruction memory.
![[Pasted image 20250211143326.png]]
![[Pasted image 20250211143356.png]]
![[Pasted image 20250211143432.png]]

## The Memory Problem
* We want to build a big, fast, and cheap memory
	* But big memories are slow and fast memories are expensive

## Locality
* Principle of locality
	* Programs work on a relatively small portion of data at any time
	* Can predict data accessed in near future by looking at recent accesses
* Temporal locality
	* If an item has been referenced recently, it will probably be accessed again soon
* Spatial locality
	* If an item has been accessed, nearby items will tend to be accessed soon
![[Pasted image 20250211143534.png]]

## Exploiting Locality: Memory Hierarchy
![[Pasted image 20250211143556.png]]
* We can build a hierarchy of memory that exploits locality — each level has faster access time.
![[Pasted image 20250211143602.png]]

# Caches
* **A cache holds recently referenced data**
	* Functions as a buffer for larger, slower storage components
* **Exploits principle of locality**
	* Provide as much inexpensive storage space as possible
	* Offer access speed equivalent to the fastest memory
		* For data in the cache
		* Key is to have the right data cached
* **Computer systems often use multiple caches**
* **Cache ideas are not limited to hardware designers**
	* Example: Web caches widely used on the Internet

## Mapping and Search
* **Mapping**: Where can I store a particular piece of data?
	* Direct mapped (single location)
		* Each memory address can only go to ONE specific location in the cache
		* O(1) lookup time but potential for conflict misses
	* Fully associative (anywhere)
		* A memory address can be stored in ANY location in the cache
		* O(n) lookup time but no conflict misses
	* N-way Set associative (anywhere in a set)
		* Cache is divided into sets, each set has N locations (ways)
		* Memory address maps to a specific set but can go in any of the N ways within that set
		* Only search through set size, not the entire cache
![[Pasted image 20250213141215.png]]

## Replacement
* **Replacement Policy**: What do I throw out to make room?
	* Random - randomly select a cache line to evict
	* LRU (Least Recently Used) - evict the line that hasn't been accessed for longest
	* FIFO (First In First Out) - evict the oldest line

## Block Size
* **Block/Line Size**: How much data do I move at a time?
	* Larger blocks exploit spatial locality better
	* But larger blocks also mean fewer total blocks in cache, less temporal locality

## Write Policy
* **Writes**: How do we handle writes?
	* Bypass cache altogether
	* Write-through: Write to both cache and memory immediately
	* Write-back: Write only to cache, mark block as dirty, write to memory when evicted

## Cache Terminology
![[Pasted image 20250211144604.png]]

# Cache Misses
![[Pasted image 20250211145041.png]]
## Miss Time
![[Pasted image 20250211145016.png]]

## Average Memory Access Time
![[Pasted image 20250211145030.png]]

## How Processors Handle a Miss
![[Pasted image 20250213135108.png]]
* For our processing, **an instruction or data miss stalls the pipeline**
	1. Stall the pipeline (bc you don't have the data it needs)
	2. Send the address that missed to memory
	3. Instruct main memory to perform a read, then wait for it to complete
	4. When access completes, return data to processor
	5. Resume the instruction and pipeline

# Direct Mapped Cache
* **Mapping:** Each memory address can only go to ONE specific location in the cache.
	* Block address is hashed to a specific cache index.
![[Pasted image 20250211144205.png]]

## Tag and Valid Bits
* Questions:
	* But if we read a cache line, how do we know which memory address it is for?
	* How do I know if a line has valid data?
* Answer: **tags and valid bits**
![[Pasted image 20250211144240.png]]
![[Pasted image 20250211144304.png]]
![[Kapture 2025-02-11 at 14.44.23.gif]]

## Block Size: Reducing Misses
* To reduce misses, we can use a cache with a **larger block size**
	* **Motivation:** exploit spatial locality to amortize overheads of misses
	* **Impact on tag/index size:** more offset bits, less index bits
	* **Impact on cache overhead:** Larger blocks mean fewer total blocks, so we need fewer tags and valid bits — less entries in the cache
![[Pasted image 20250213135620.png]]
![[Pasted image 20250213135755.png]]
![[Pasted image 20250213135805.png]]
* The miss rate increases when the block size gets too big because larger block sizes means fewer number of blocks in the cache.
	* Higher chance of a conflict miss because there are more ways to map the same block to the cache

## Problem with Direct Mapped Cache
* Suppose two blocks are used concurrently and map to the same index in the cache
	* Only one block can fit in the cache, regardless of cache size
	* There is no flexibility in placing the second block elsewhere
* **Thrashing**
	* If accesses alternate, one block will replace the other before reuse
	* Every reference will cause a miss -- no benefit from caching
![[Pasted image 20250213140843.png]]

# Fully Associative Cache
* **Mapping:** there is no cache index to hash, every block can go to any location in the cache
	* Use any available cache entry to store memory elements
	* No conflict misses, only capacity misses
	* Must compare cache tags of **all** entries to find a match (expensive)
![[Pasted image 20250213140932.png]]

# N-Way Set Associative Cache
* **Mapping:** Compromise between direct mapped and fully associative
	* Cache is divided into sets, each set has N locations (ways)
	* Each memory block can go to one of the N ways in a set
	* A cache contains some number of sets
* How to think of a N-way associate cache with X-sets?
	1. N direct mapped caches each with X entries
		* Caches searched in parallel (all N ways in set X)
		* Need to coordinate on data output and signaling hit/miss
	2. X fully associative caches each with N entries
		* One cache (set) is searched in each case

* This is an example of view 1.
![[Pasted image 20250213142535.png]]

## Tag and Index
![[Pasted image 20250213142402.png]]
![[Pasted image 20250213142418.png]]

## 4-Way Set Associative Cache
* A 4-way set associative cache has 4 "ways" per set
	* 256 sets total in this example, each set contains 4 possible locations for data
![[Pasted image 20250213141607.png]]
* Key components and operation:
	* Address is split into **index (green, selects set)** and **tag (red, identifies data)**
	* Index points to same row (set) across all 4 ways simultaneously
	* Parallel comparison:
		* All 4 ways in selected set checked at the same time
		* Each way has dedicated comparator hardware
		* The results feed into an encoder that determines which way (if any) contains the matching data
* Output signals:
	* "Hit" indicates if data was found in any way
	* Multiplexer selects data from matching way

![[Pasted image 20250213142522.png]]

## Pros
* Increased associativity decreases miss rate
	* As the number of ways increases, the miss rate decreases because there is more flexibility in where each cache line can be placed, reducing conflict misses that occur when multiple addresses map to the same cache location.
	* Eliminates conflict misses, but with diminishing returns
![[Pasted image 20250213142806.png]]

## Cons
* Area overhead
	* More storage needed for tags (compared to same sized direct mapped)
	* Also need N comparators for each way
* Latency
	* `Critical path = way access + comparator + logic to combine answers`
		* Logic to `OR` hit signals and multiplex the data outputs
	* Cannot forward the data to the processor immediately
		* Must wait for selection and multiplexing
		* Direct mapped cache can forward data immediately because it can assume a hit and recover later if it was a miss
* Complexity
	* Dealing with replacement is hard

# Summary of Mapping
* The blocks are 16 bytes so the block offset has to be 4 bytes for all.
* The index shrinks in size as the associativity increases.
![[Pasted image 20250213143149.png]]

# Replacement Methods
* **Direct Mapped Cache**
	* No replacement needed
	* Just replace the line at the index that you need
* **N-way set associative cache**
	* Need to choose which way to replace
		1. Choose a line at random
		2. Choose least recently used line (how to encode?)

# Write Methods
* When we want to write, where do we put the data? In cache, memory, or both?
* Caches have different policies for this
	* Most systems store the data in the cache — faster writes, flush to memory later
	* Some store in memory as well
* **Interesting Observation:** if there is a cache hit, the processor does not need to wait until the write completes to continue

## Write-through
* Write to both cache and memory immediately
* Replacing a cache entry is simple: just overwrite the new block
* Memory write causes significant delay if pipeline must stall

## Write-back
* Only cache entry is written to, so main memory and cache is **inconsistent**
* Add "dirty" bit to cache line to indicate whether line must be committed to memory
* Replacing cache entry is more complex:
	* If line is dirty, write to memory first
	* If line is clean, just replace the block

## Tradeoff
![[Pasted image 20250213143653.png]]

## Write Buffers
* We can use a write buffer between **cache and memory** to avoid stalls for write-thru
	* Processor writes into the cache and the write buffer
	* Memory controller slowly drains the write buffer into memory
* Write buffer is a FIFO buffer
	* Holds a small number of writes, allows you to **absorb small bursts** as long as the long-term rate of writing to the buffer does not exceed the rate of writing to memory
![[Pasted image 20250213143756.png]]

## Write Hits
![[Pasted image 20250213144618.png]]
* You should not access tag and write data in parallel because if its a miss, you could cause a coherency issue and corrupt the cache.

## Write Miss
![[Pasted image 20250213144637.png]]
![[Pasted image 20250213144658.png]]

## Typical Choices
![[Pasted image 20250213144919.png]]
