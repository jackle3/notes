
# Advanced Pipelining: Modern Processors
![[Pasted image 20250206142753.png]]

* We are allowed to do out-of-order execution because of **instruction-level parallelism**
![[Pasted image 20250206142923.png]]

## Deeper Pipelining
* We can increase the number of pipeline stages
	* This leads to fewer levels of logic per pipeline stage
	* Also leads to higher clock frequency (because each stage is faster, we can run the whole processor at a higher speed)
![[Pasted image 20250206144431.png]]
* **Issues**: branch delay, load delay, forwarding/stalls complexity

## Superscalar Processors
* Superscalar processors are processors that can execute multiple instructions in parallel
* They do this by having multiple functional and execution units
	* The ideal CPI is now 0.5 because every cycle, we finish execution for 2 instructions.
![[Pasted image 20250206144510.png]]

# Dynamic Scheduling
* Execute instructions out of order
	* Fetch multiple instructions per cycle using branch prediction
	* Figure out which instructions can be executed in parallel
* **Issue**: the instructions are using the same registers. If the `sub` executes before the `or`, then the `or` will use the wrong value.
	* This is a write-after-write hazard (`add` writes to `t0` first and `sub` writes to `t0` second)
![[Pasted image 20250206144808.png]]

## Register Renaming
* To fix the issue from before, we can **map** the registers to get rid of false dependencies.
	* There are more physical registers than the architectural registers (i.e. more than the 32 registers that the ISA gives)
![[Pasted image 20250206144918.png]]

## Modern MIPS Processors
![[Pasted image 20250211141438.png]]
* This processor is `16R4W`, meaning it has 16 read ports and 4 write ports
* Read ports allow the processor to read from registers
	* Each read port can read from one register per cycle
	* Having 16 read ports means we can read from up to 16 different registers in parallel each cycle
* Write ports allow the processor to write to registers
	* Each write port can write to one register per cycle
	* Having 4 write ports means we can write to up to 4 different registers in parallel each cycle
* The number of ports directly impacts instruction-level parallelism:
	* More read ports = more instructions can read their operands simultaneously
	* More write ports = more instructions can write their results simultaneously

![[Pasted image 20250211141445.png]]

## Caveats of Out-of-Order Processors
![[Pasted image 20250211142044.png]]
* When instructions are fetched and decoded, they are stored in the **reservation station** before they are dispatched to out-of-order execution.
* After out-of-order execution, they are placed into a **re-order buffer** to ensure in-order completion and retirement
* What can go wrong if we reorder instructions?
	* **Exceptions**: When executing out-of-order, an instruction may raise an exception (e.g. divide by zero, page fault). We need to ensure that exceptions are handled in the original program order, not the execution order.
		* The **re-order buffer helps maintain precise exceptions** by only committing instructions in-order.
	* **Mispredictions**: Branch mispredictions require rolling back speculative execution.
		* The **re-order buffer allows us to identify and squash all speculative instructions** after the mispredicted branch while preserving earlier instructions.

# Dynamic Branch Prediction
* We predict the direction of branches based on past behavior
	* Keep a table of branch behavior and look up to get prediction
* This table is called the *branch prediction buffer* (or *branch history table*)
	* Maps from the lower bits of the PC address to a 1 bit value
		* Value says whether the branch was taken or not the last time
	* Evaluate actual branch condition and correct if prediction was wrong
		* Recover by flushing pipeline and restarting fetch, and then reset prediction
	* We also often use branch prediction tables with 2 bit values — more accurate
![[Pasted image 20250211135220.png]]

## 2-Bit Branch Predictor State Machine
![[Pasted image 20250212101027.png]]
* The 2-bit branch predictor uses a state machine with 4 states to make more stable predictions:
	* States 0,1 predict the branch will be taken (T)
		* State 0: Strongly predict taken
		* State 1: Weakly predict taken
	* States 2,3 predict the branch will not be taken (N)
		* State 2: Weakly predict not taken
		* State 3: Strongly predict not taken
* Key advantages over 1-bit prediction:
	* More resistant to changing prediction — requires two wrong predictions to switch
	* Prevents thrashing between taken/not taken predictions
* The state transitions on each branch outcome:
	* When branch is taken (T): Move one state toward strongly taken (left)
	* When branch is not taken (N): Move one state toward strongly not taken (right)
	* Requires multiple opposite outcomes to change branch prediction (from taken to not taken)

## Branch Prediction Example
![[Pasted image 20250211140230.png]]
* Assume the 1-bit is initialized to `Not Taken` and the 2-bit is initialized to `Weakly Taken`

| Index | Actual      | 1-bit Branch Predictor                                 | 2-bit Branch Predictor                                             |
| ----- | ----------- | ------------------------------------------------------ | ------------------------------------------------------------------ |
| 0     | `Taken`     | Predicts `Not Taken` ⟶ Mispredict<br>Change to `Taken` | Predicts `Weakly Taken` ⟶ Correct<br>Change to `Strongly Taken`    |
| 1     | `Taken`     | Predicts `Taken` ⟶ Correct                             | Predicts `Strongly Taken` ⟶ Correct                                |
| 2     | `Taken`     | Predicts `Taken` ⟶ Correct                             | Predicts `Strongly Taken` ⟶ Correct                                |
| 3     | `Taken`     | Predicts `Taken` ⟶ Correct                             | Predicts `Strongly Taken` ⟶ Correct                                |
| 4     | `Not Taken` | Predicts `Taken` ⟶ Mispredict<br>Change to `Not Taken` | Predicts `Strongly Taken` ⟶ Mispredict<br>Change to `Weakly Taken` |

# Limits of Advanced Pipelining
![[Pasted image 20250211141623.png]]
* Branch missprediction is a significant contributor to wasted work.
![[Pasted image 20250211141637.png]]
* There are also tradeoffs in terms of perofrmance and power.
![[Pasted image 20250211141811.png]]
* Observe how the the CPI is decreased a lot due to pipeline stalls and memory hierarchy stalls.
![[Pasted image 20250211141814.png]]

# Memory Stalls
* We cannot access all memory in one cycle, especially if you have large amounts of memory.
	* Need to optimize the memory system and re-unify data/instruction memory.
![[Pasted image 20250211143326.png]]
![[Pasted image 20250211143356.png]]
![[Pasted image 20250211143432.png]]

## The Memory Problem
* We want to build a big, fast, and cheap memory
	* But big memories are slow and fast memories are expensive

## Locality
* Principle of locality
	* Programs work on a relatively small portion of data at any time
	* Can predict data accessed in near future by looking at recent accesses
* Temporal locality
	* If an item has been referenced recently, it will probably be accessed again soon
* Spatial locality
	* If an item has been accessed, nearby items will tend to be accessed soon
![[Pasted image 20250211143534.png]]

## Exploiting Locality: Memory Hierarchy
![[Pasted image 20250211143556.png]]
![[Pasted image 20250211143602.png]]

# Caches
* **A cache holds recently referenced data**
	* Functions as a buffer for larger, slower storage components
* **Exploits principle of locality**
	* Provide as much inexpensive storage space as possible
	* Offer access speed equivalent to the fastest memory
		* For data in the cache
		* Key is to have the right data cached
* **Computer systems often use multiple caches**
* **Cache ideas are not limited to hardware designers**
	* Example: Web caches widely used on the Internet

## Cache Questions
* **Mapping**: Where can I store a particular piece of data?
	* Direct mapped (single location)
		* Each memory address can only go to ONE specific location in the cache
	* Fully associative (anywhere)
		* A memory address can be stored in ANY location in the cache
	* N-way Set associative (anywhere in a set)
		* Cache is divided into sets, each set has N locations (ways)
		* Memory address maps to a specific set but can go in any of the N ways within that set
* **Replacement Policy**: What do I throw out to make room?
	* Random - randomly select a cache line to evict
	* LRU (Least Recently Used) - evict the line that hasn't been accessed for longest
	* FIFO (First In First Out) - evict the oldest line
* **Block/Line Size**: How much data do I move at a time?
	* Larger blocks exploit spatial locality better
	* But larger blocks also mean fewer total blocks in cache
* **Writes**: How do we handle writes?
	* Bypass cache altogether
	* Write-through: Write to both cache and memory immediately
	* Write-back: Write only to cache, mark block as dirty, write to memory when evicted

## Cache Terminology
![[Pasted image 20250211144604.png]]

## Direct Mapped Cache
![[Pasted image 20250211144205.png]]
* But if we read a cache line, how do we know which memory address it is for?
* How do I know if a line has valid data? Answer: **tags and valid bits**
![[Pasted image 20250211144240.png]]
![[Pasted image 20250211144304.png]]
![[Kapture 2025-02-11 at 14.44.23.gif]]

## Miss Time
![[Pasted image 20250211145016.png]]

## Average Memory Access Time
![[Pasted image 20250211145030.png]]

## The 3Cs of Cache Misses
![[Pasted image 20250211145041.png]]

## How Processors Handle a Miss
![[Pasted image 20250211145053.png]]
