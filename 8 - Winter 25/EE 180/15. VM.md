
# Translation Look-aside Buffer
* Virtual memory is great but we just doubled the memory accesses
	* A load requires an access to the page table first
	* Then an access to the actual data
* How can we **do translation fast** without an additional memory access?

## Properties
* TLB is a hardware cache that stores PTEs (Page Table Entries)
	* It caches the virtual page number to physical frame number mappings from the page table
	* Also caches the associated metadata (like permission bits)

* Key idea: locality in accesses ⟶ locality in translations
* TLB design: similar issues to all caches
	* Basic parameters: capacity, associativity replacement policy
	* Basic optimizations: instruction/data TLBs, multi-level TLBs, …
	* Misses may be handled by HW or SW
		* x86: hardware services misses (why?)
			* Faster handling of TLB misses since no context switch to OS needed
			* But less flexible since page table structure is fixed in hardware
		* MIPS: software services misses through exception (why?)
			* More flexible since page table structure can be changed
			* But slower since context switch to OS is needed

## TLB Miss Handling
* Access page table entry (PTE) in memory
	* Could be handled in hardware
		* Can get complex for more complicated page table structures
	* Or in software
		* Raise a special exception, with optimized handler
		* This is what MIPS does using a **special vectored interrupt**
* If page is in memory
	* Copy PTE into TLB and retry
* If page is not in memory (page fault)
	* OS handles fetching the page and updating the page table
	* Then restart the faulting instruction

## TLB Organization
* When a virtual page number is requested, it will:
	* Check TLB for the PTE
	* If TLB miss:
		* Access page table in memory (via HW or SW depending on architecture)
		* If page is in memory:
			* Install PTE in TLB
			* Form physical address using frame number + offset
		* If page fault (page not in memory):
			* OS must handle bringing page from disk to memory
			* Update page table
			* Then retry the instruction from start (which will bring it into the TLB)
![[Pasted image 20250220144605.png]]

## TLB Entries
* Each TLB entry stores a page table entry (PTE)
* Each entry contains what page table contains as data
	* Physical page numbers
	* Permission bits (RXW)
	* Other PTE information (dirty bit, LRU info, etc)
* In addition, there is the TLB entry metadata
	* Tag: portion of virtual page # not used to index the TLB
		* Depends on the TLB associativity
	* Valid bit
	* LRU bits (if TLB is associative and LRU replacement is used)

# Caveat: Context Switching
* The TLB is shared by all processes. What happens when we switch between processes?
* The OS must **flush the entries in the TLB**
	* Large number of TLB misses after every switch
* Alternatively, use a **process ID** in each TLB entry
	* Allows entries from multiple processes to co-exist
	* Gradual replacement

## Example TLB Design
* Notice that this design allows you to disable caching of a certain memory address
	* Especially useful for MMIO ⟶ IO addresses are not real, so we don't want to cache them.
![[Pasted image 20250220145103.png]]

# TLB and Memory Hierarchies
## Basic Process (Physically Indexed, Physically Tagged)
* Process:
	1. CPU generates virtual address (VA)
	2. TLB lookup using virtual page number (VPN) to get physical page number (PPN)
	3. Physical address is formed by combining PPN + page offset
	4. Physical address is used to access cache/memory
		* Index bits select the cache set
		* Tag bits are compared with cache tags
		* Offset bits select bytes within cache line

* Hit Time = TLB access + Cache access (sequential)
	* Must wait for TLB translation before cache lookup can begin
	* Critical path: TLB lookup → Cache access → Data return
![[Pasted image 20250225134900.png]]

## Virtually Indexed, Physically Tagged Caches
* Process:
	1. CPU generates virtual address
	2. In parallel:
		* Page offset bits from virtual address used to index into cache
			* These bits are not translated ⟶ fit the cache index into the page offset bits
		* TLB lookup translates VPN to PPN (i.e. VPN to cache tag)
	3. Physical tag from TLB compared with cache tags
	4. On match, return data from indexed cache line

* Hit Time = max(TLB access, Cache index access) + Tag compare
	* Overlapped TLB and cache access reduces latency
	* Critical path is typically TLB access time

* Constraint: `Cache Size` ≤ `Page Size * Associativity`
	* Observe that $\text{cache index} + \text{byte offset} \leq \text{page offset}$
	* This means $2^{\text{cache index}} \cdot 2^{\text{byte offset}} \leq 2^{\text{page offset}}$
		* The number of rows (sets) in the cache is equal to $2^{\text{cache index}}$
		* The number of bytes in each row is equal to $2^{\text{byte offset}}$
		* The page size is $2^{\text{page offset}}$
	* Associativity further increases the possible cache size:
		* In a direct-mapped cache (associativity = 1), each set has 1 cache line
		* With N-way associativity, each set can store N cache lines
		* This effectively multiplies our storage capacity by N
		* For example, with 4-way associativity:
			* Each set can store 4 cache lines instead of 1
			* Total cache size becomes 4 times larger
			* But we still use the same index bits from the virtual address
	* Therefore, when we account for associativity:
		* Left side: $\text{associativity} \cdot 2^{\text{cache index}} \cdot 2^{\text{byte offset}}$ (total cache size)
		* Right side: $\text{associativity} \cdot 2^{\text{page offset}}$ (page size × associativity)
	* This gives us: $\text{cache size} \leq \text{page size} \cdot \text{associativity}$
![[Pasted image 20250225135001.png]]

## Virtual Memory and Caches
* For the last two, we usually think of DRAM as a superset of the cache data
![[Pasted image 20250225142542.png]]

# Caveat: Limited Coverage
![[Pasted image 20250225142621.png]]

# Page Size Tradeoff
![[Pasted image 20250225142640.png]]
![[Pasted image 20250225142740.png]]

# Page Table Size
![[Pasted image 20250225142905.png]]
![[Pasted image 20250225143012.png]]
![[Pasted image 20250225143038.png]]

# Putting it All Together: Intel P6 Processor
![[Pasted image 20250225143719.png]]

## Abbreviations
![[Pasted image 20250225143808.png]]

## Overview
![[Pasted image 20250225143825.png]]

## TLB Translation
![[Pasted image 20250225144255.png]]

## Page Table Translation
![[Pasted image 20250225144305.png]]

### 2-Level Structure
![[Pasted image 20250225144027.png]]
![[Pasted image 20250225144043.png]]

### Translation
![[Pasted image 20250225144127.png]]

### Page Entries
![[Pasted image 20250225144155.png]]
![[Pasted image 20250225144200.png]]

## Translation Example
### 1/1: Page Table and Page Present
![[Pasted image 20250225144437.png]]

### 1/0: Page Table Present but Page Missing
![[Pasted image 20250225144451.png]]
![[Pasted image 20250225144503.png]]

### 0/1: Page Table Missing but Page Present
![[Pasted image 20250225144512.png]]

### 0/0: Page Table and Page Missing
![[Pasted image 20250225144612.png]]
![[Pasted image 20250225144620.png]]

## L1 Cache Access
![[Pasted image 20250225144635.png]]
![[Pasted image 20250225144650.png]]

### VIPT
![[Pasted image 20250225144702.png]]

# Hardware Support for the OS
![[Pasted image 20250225144717.png]]

# Virtual Memory Summary
![[Pasted image 20250225144728.png]]
