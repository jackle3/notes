# Compilation Hierarchy
![Pasted image 20250123134907](attachments/Pasted%20image%2020250123134907.png)

# 1. Compiler
* The compiler convert high-level languages to machine code!
![Pasted image 20250123134924](attachments/Pasted%20image%2020250123134924.png)
* **Front-end (analysis)**
	* Scanner: Breaks source code into tokens like keywords, identifiers, and operators.
	* Parser: Analyzes tokens to create a parse tree representing program structure. Creates **abstract syntax tree** (AST).
	* Semantic Analyzer: Checks for semantic errors and type checks, then builds **symbol tables**.
	* Intermediate Code Generation: Uses AST and symbol tables to create an intermediate representation (IR) of the program.
* **Back-end (optimization and synthesis)**
	* IR Optimization: Performs optimizations on the intermediate code to improve efficiency.
	* Assembly/Object Generation: Converts optimized IR into target machine code.
	* Assembly/Object Output: Produces final executable code and object files.

## Abstract Syntax Tree
* Given code, **parser** creates an AST that represents the program structure.
* Each node in the tree represents a construct in the source code.
* Leaves are tokens, internal nodes are operators.
![Pasted image 20250123135349](attachments/Pasted%20image%2020250123135349.png)

## Type Checking
* Use the AST, the semantic analyzer checks for type errors.
![Pasted image 20250123135427](attachments/Pasted%20image%2020250123135427.png)

## Intermediate Representation
![Pasted image 20250123135446](attachments/Pasted%20image%2020250123135446.png)
* **Three-address code (TAC)**
	* An intermediate representation used by compilers like GCC
	* Each instruction has at most 3 operands (2 sources, 1 destination)
	* Simple format: `x = y op z` where op is an operator
	* Makes code optimization and analysis easier by breaking complex expressions into simple steps
	* Example:
		* Original: `a = b + c * d`
		* TAC:
```
t1 = c * d		# Multiply c and d, store in temp
a = b + t1		# Add b to result, store in a
```

* **Single Static Assignment (SSA)**
	* A refinement of TAC where each variable is assigned exactly once
	* Makes data flow analysis simpler by giving each value a unique name
	* Example:
		* Original: `a = b + c * d`
		* SSA:
```
t1 = c * d		# First computation
a1 = b + t1	 # Single assignment to a1
```


## Optimizing Compilers
* **Improve performance** by providing efficient mapping of program to machine
	* Instruction selection and ordering
	* Eliminating minor inefficiencies
	* Register allocation
* **Does not (usually) improve asymptotic efficiency**
	* Up to programmer to select best overall algorithm
	* Big-O savings are (often) more important than constant factors
		* But constant factors also matter

## Limitations of Optimizing Compilers
* **Fundamental constraints**
	* Improved code must have same output
	* Improved code must have same side effects
	* Improved code must be as correct as original code
* **Most analysis is performed only within procedures**
	* Whole-program analysis is too expensive in most cases
* **Most analysis is based only on static information**
	* Compiler has difficulty anticipating run-time inputs
	* Profile driven dynamic compilation becoming more prevalent
	* Exception: Dynamic compilation (e.g. Java JIT)
		* Compile code at runtime based on actual execution patterns
* **When in doubt, the compiler must be conservative**
	* Better to skip optimization than risk incorrect behavior
## Types of Optimizations
### Local Optimizations (Within Basic Blocks)
* **Constant folding and propagation**
	* Evaluates constant expressions at compile time
	* Example: `x = 2 + 3` → `x = 5`
* **Common subexpression elimination**
	* Stores repeated computations in temporary variables
	* Avoids redundant calculations
* **Dead code elimination**
	* Removes code with no effect on output
	* Examples: unused variables, unreachable code
* **Register allocation**
	* Maps frequently used variables to registers for fastest access
	* Key decisions:
		* Allocation: Which variables should be in registers?
		* Assignment: Which register gets which value?
	* Avoid register spilling (moving values back to memory) when possible

### Global Optimizations (Across Basic Blocks)
* **Loop optimizations**
	* Loop invariant code motion: Moves unchanging code outside loops
	* Loop unrolling: Duplicates loop body to reduce overhead
	* Loop fusion: Combines multiple loops into a single loop to reduce overhead and improve cache locality
![Pasted image 20250123140943](attachments/Pasted%20image%2020250123140943.png)
* **Function inlining**
	* Replaces function calls with actual function code
	* Reduces call overhead at cost of code size
* **Strength reduction**
	* Replaces expensive operations with cheaper equivalents
	* Example: `i * 4` → `temp += 4` in loops
* **Induction variable optimization**
	* Simplifies and combines loop counter variables
	* Works with strength reduction
	* Example: `j = i * 4` → `j += 4` per iteration

## GCC Optimizations
* GCC has different optimization levels that can be enabled with flags:
	* `-O1`: Basic optimizations
		* Local optimizations within basic blocks
		* Register allocation and instruction scheduling
	* `-O2`: Moderate optimizations
		* All `-O1` optimizations plus:
		* Global optimizations across basic blocks
		* Loop optimizations
		* No space-speed tradeoffs
	* `-O3`: Aggressive optimizations
		* All `-O2` optimizations plus:
		* Function inlining
		* Loop unrolling
		* Vectorization
		* May increase code size significantly
	* `-Os`: Optimize for size
		* Similar to `-O2` but avoids optimizations that increase code size
		* Useful for embedded systems with limited memory

![Pasted image 20250123141550](attachments/Pasted%20image%2020250123141550.png)

# 2. Assembler
* Expands macros and pseudoinstructions, converts constants and labels to machine code
* Primary purpose is to generate machine code from assembly language (producing an object file)
	* Machine language instructions, application data, and information for memory organization
## Pseudoinstructions
![Pasted image 20250123143841](attachments/Pasted%20image%2020250123143841.png)
## Object File
![Pasted image 20250123144116](attachments/Pasted%20image%2020250123144116.png)

# 3. Linker
* Combines multiple object modules into a single executable program
* Main responsibilities:
	* Identifies where code and data sections will be placed in memory
	* Resolves all code and data cross-references between modules
	* Produces executable file if all references are successfully resolved
## Linking Process
1. **Memory Placement**
	* Places code and data modules at specific memory addresses
	* Allocates space for each section (text, data, bss, etc.)
2. **Address Resolution**
	* Determines final addresses for all data and instruction labels
	* Updates symbol tables with resolved addresses
3. **Reference Patching**
	* Patches both internal references (within same module)
	* Patches external references (between different modules)
## Benefits
* Separation between compiler and linker enables:
	* Modular code development
	* Efficient use of standard libraries
	* Code reusability across projects
	* Independent compilation of source files
# 4. Loader
* Program loader is used at runtime to load executable into memory
* Main responsibilities:
	* Reads executable file header to determine size of text/data segments
	* Creates virtual address space large enough for program
	* Copies program sections from executable file on disk into memory
		* Code segment
		* Data segment
		* Stack segment
	* Sets up program arguments on stack
	* Initializes machine registers and set stack pointer
	* Jumps to program startup routine to begin execution
	* Handles program termination when execution completes
		* Cleans up resources
		* Returns control to operating system

---
----
# Logic Design
* A digital computer system is built from three fundamental components:
	1. **State elements**: Memory components that store information
	2. **Combinational logic**: Circuits that perform computations
	3. **Clock**: A signal that coordinates when things happen
![Pasted image 20250123142048](attachments/Pasted%20image%2020250123142048.png)

# Combinational Logic
* Think of combinational logic as a pure function in programming:
	* Takes inputs and produces outputs based on those inputs
	* Has no memory or internal state
	* Always produces the same output for the same input
	* The output is computed continuously as inputs change
## Boolean Algebra
* All digital logic is based on binary (0 and 1) and three basic operations:
	* AND (written as `·` or `*`): Like logical `&&` in programming
	* OR (written as `+`): Like logical `||` in programming
	* NOT (written as `¯` or `'`): Like logical `!` in programming
* These map directly to electrical signals:
	* 1 (TRUE) = high voltage
	* 0 (FALSE) = low voltage
![Pasted image 20250123142210](attachments/Pasted%20image%2020250123142210.png)

## Logic Gates: Building Blocks
* Logic gates are the physical implementation of boolean operations
* Each gate takes one or more binary inputs and produces a binary output
![Pasted image 20250123142217](attachments/Pasted%20image%2020250123142217.png)

### Basic Logic Functions: Combining Gates
* We combine gates to create more complex functions
* Example: Building a circuit that adds two bits
	* Requires multiple gates working together
	* The arrangement of gates determines the function
![Pasted image 20250123142339](attachments/Pasted%20image%2020250123142339.png)

### Multiplexor (MUX): A Digital Switch
* A multiplexor is like an if-else statement in hardware:
	* Takes multiple input values (like 2, 4, or 8 inputs)
	* Takes a "select" signal (like a binary number)
	* Outputs one of the inputs based on the select value
* Example: 2-input multiplexor
	* If select = `0`, output = first input
	* If select = `1`, output = second input
	* Very common in CPU design for choosing between different values
![Pasted image 20250123142411](attachments/Pasted%20image%2020250123142411.png)

## Specifying Logic Functions: From Requirements to Circuits
* Two main ways to specify what we want our circuit to do:
	1. **Logic Diagram**: Drawing the actual gates and connections![Pasted image 20250123142426](attachments/Pasted%20image%2020250123142426.png)
	2. **Truth Table**: Listing all possible inputs and desired outputs![Pasted image 20250123142448](attachments/Pasted%20image%2020250123142448.png)
## Optimizing Logic: Making Circuits More Efficient
* Most logic functions can be implemented in many ways. We find the most optimal implementation to:
	* Reduce power consumption (fewer gates use less electricity)
	* Reduce circuit size (smaller chips are cheaper to manufacture)
	* Reduce delay (fewer gates means faster computation)
![Pasted image 20250123144814](attachments/Pasted%20image%2020250123144814.png)

### Optimization Techniques
1. **Boolean Identity Functions**: Like simplifying algebra expressions
	* Basic rules that always work:
	* `A + 0 = A` (OR with 0 doesn't change the value)
	* `A * 1 = A` (AND with 1 doesn't change the value)
	* `A + 1 = 1` (OR with 1 is always 1)
	* `A * 0 = 0` (AND with 0 is always 0)

2. **De Morgan's Laws**: Important rules for simplifying NOT operations
	* `NOT(A AND B) = (NOT A) OR (NOT B)`
	* `NOT(A OR B) = (NOT A) AND (NOT B)`
	* Think of it like distributing the NOT and flipping AND/OR
![Pasted image 20250123142726](attachments/Pasted%20image%2020250123142726.png)

3. **Don't Cares**: Ignoring impossible or irrelevant cases
	* Example: If a 2-bit counter can only count 0-2, we don't care what happens for input 3
![Pasted image 20250123142737](attachments/Pasted%20image%2020250123142737.png)

4. **Karnaugh Maps**: Visual tool for finding simpler expressions
	* Groups similar terms together
	* Larger groups mean simpler expressions
	* Similar to finding patterns in truth tables
![Pasted image 20250123142802](attachments/Pasted%20image%2020250123142802.png)

## ALU: The Calculator of the Computer
* ALU (Arithmetic & Logic Unit) combines multiple operations in one circuit
	* Can perform arithmetic (add, subtract)
	* Can perform logic operations (AND, OR, etc.)
	* Selection between operations controlled by control signals
![Pasted image 20250123142955](attachments/Pasted%20image%2020250123142955.png)

# State Elements: The Memory of the Circuit
* While combinational logic computes things, state elements remember things
* Think of them as variables in hardware
![Pasted image 20250123151316](attachments/Pasted%20image%2020250123151316.png)

## Flip-Flops: The Basic Memory Unit
* A flip-flop is like a single variable in hardware
* Key features:
	* Stores one bit (0 or 1)
	* Updates only when told to (on clock edge)
	* Maintains its value until next update
![Pasted image 20250123151501](attachments/Pasted%20image%2020250123151501.png)

### Understanding Flip-Flop Behavior
* A flip-flop operates as follows:
	* The input (D) is a binary value (0 or 1) that can change at any time
	* The clock (Clk) is a periodic signal that triggers on its rising/falling edge
	* The output (Q) holds the value that was present at D when the last clock edge occurred
	* The output remains stable regardless of input changes until the next clock edge
* Key components:
	* D (Data): The input value we want to store
	* Clk (Clock): The signal that triggers updates
	* Q: The stored output value

### Flip-Flops with Write Control
* Adds an "enable" signal
* Only updates when both:
	* Clock edge occurs AND write control (enable) is active
* Useful when we want to selectively update stored values
![Pasted image 20250123151456](attachments/Pasted%20image%2020250123151456.png)

## Clocking: The Heartbeat of Digital Systems
* The clock coordinates when things happen in digital circuits
* Between clock ticks:
	1. Flip-flops output stable values
	2. These values flow through combinational logic
	3. Results arrive at inputs of next flip-flops
	4. Next clock tick captures these results
![Pasted image 20250123151549](attachments/Pasted%20image%2020250123151549.png)

## Critical Timing: Making Sure Everything Works
* Like any system, timing is crucial
* Two key timing requirements:
	* **Setup** time ($t_s$): Data must be stable BEFORE clock edge
		* Clock period: $t_{ck} > t_{pd} + t_s + t_{skew}$
		* Where:
			* $t_{ck}$ is the time between clock edges (clock period)
			* $t_{pd}$ is propagation delay
			* $t_{skew}$ is clock skew
		* Can usually be resolved by increasing the clock period because this means the longest chunk of combinational logic is taking too long to get to the flip-flop.
	* **Hold** time ($t_h$): Data must remain stable AFTER clock edge
		* Must satisfy: $t_h < t_{cd} - t_{skew}$
		* Where:
			* $t_{cd}$ is contamination delay
		* You're screwed here because it means the inputted value is changing before the flip-flop can save it.
* Think of it like catching a ball:
	* Setup time: Getting your hands ready in position
	* Hold time: Keeping your hands steady as you catch
![Pasted image 20250123151715](attachments/Pasted%20image%2020250123151715.png)

## Registers: Storing Multiple Bits
* A register is like an array of flip-flops
* Used to store multi-bit values (like a byte or word)
* Register files are like small arrays in hardware:
	* Each element is a multi-bit register
	* Can read/write by specifying register number
	* Similar to accessing array elements by index

# Memory Structures: Large-Scale Storage
* When we need to store lots of data, basic flip-flops become impractical
* Two main types of computer memory:
	1. **SRAM (Static RAM)**:
		 * Like a high speed storage solution
		 * Fast but expensive
		 * Used for small, critical storage (processors, CPU caches)
	1. **DRAM (Dynamic RAM)**:
		 * Like bulk storage
		 * Slower but much cheaper
		 * Used for main memory (RAM in your computer)

## SRAM: Fast but Expensive Memory
* Static Random Access Memory (SRAM) has:
	* Very fast performance
	* Expensive to build
	* Takes up a lot of space
	* Used where speed is critical (CPU caches)

### How SRAM Works
* Each SRAM cell (storing 1 bit) uses 6 transistors:
	* Two cross-coupled inverters form a latch circuit
		* The inverters maintain a stable state (0 or 1)
		* This forms the core storage element
	* Two access transistors (T1 and T2) connect to bit lines (b and b')
		* When word line is grounded: transistors are off, latch retains state
		* When word line is activated:
			* For reading: T1 and T2 connect latch to bit lines
				* State 1: b is high, b' is low
				* State 0: b is low, b' is high
			* For writing: Sense/Write circuit drives b and b'
				* Forces cell into desired state
				* State is maintained when word line deactivates
![Pasted image 20250123152013](attachments/Pasted%20image%2020250123152013.png)
* Key advantages:
	* Very fast access times
	* Keeps data as long as power is on
	* No need to refresh
* Key disadvantages:
	* Takes up lots of chip area (6 transistors per bit)
	* Consumes more power
	* Very expensive per bit

## DRAM: The Bulk Storage Solution
* Dynamic Random Access Memory (DRAM) is like a cargo truck:
	* Can store lots of data
	* Much cheaper per bit
	* Slower than SRAM
	* Used for main system memory (your computer's RAM)

### How DRAM Works
* Each DRAM cell uses just 1 transistor and 1 capacitor:
	* Capacitor stores the data as electrical charge
		* Think of it like a tiny battery
		* Charged = 1, Discharged = 0
	* Transistor acts like a gate to access the capacitor
* Key challenge: The capacitor leaks charge over time
	* Like a bucket with a tiny hole
	* Must be periodically refreshed (recharged)
	* This is why it's called "dynamic"
![Pasted image 20250123152104](attachments/Pasted%20image%2020250123152104.png)
* Key advantages:
	* Very dense (1 transistor + 1 capacitor per bit)
	* Much cheaper per bit
	* Can store gigabytes of data
* Key disadvantages:
	* Needs constant refreshing
	* Slower than SRAM
	* Higher power usage due to refreshing

## Memory Organization: How It All Fits Together
* Memory is organized like a giant spreadsheet:
	* Rows (word lines): Select which cells to access
	* Columns (bit lines): Carry data in/out
	* Address: Specifies which row and column
	* Data lines: Carry the actual data being read/written
![Pasted image 20250123152132](attachments/Pasted%20image%2020250123152132.png)

## Memory Timing: How Data Moves In and Out
There are two main ways memory can operate:
### 1. Combinatorial/Asynchronous Memory
* Like a vending machine that responds immediately:
	* Put in the address (like inserting money)
	* Get data back after a short delay
	* No clock needed
	* Output changes whenever inputs change
* Simpler but less predictable timing
![Pasted image 20250123155716](attachments/Pasted%20image%2020250123155716.png)
### 2. Synchronous Memory
* Like a train that only moves on schedule:
	* Everything happens on clock edges
	* More predictable timing
	* Easier to work with other components
	* Address changes happen in sequence (2->3->4->5)
![Pasted image 20250123155721](attachments/Pasted%20image%2020250123155721.png)

## Reading and Writing Memory
* Reading from memory:
	* Like looking up a value in an array
	* Provide address, get data back
	* May need to wait for data to be ready
* Writing to memory:
	* Like storing a value in an array
	* Provide address and data
	* Assert write signal
	* Data is stored at that location

### Two Types of Interfaces
1. **Combinatorial (Asynchronous)**:
	 * Like a direct phone call - immediate response
	 * Read: Data appears after address stabilizes
	 * Write: Data stored when write signal is high
	 * Timing depends on circuit delays
2. **Clocked (Synchronous)**:
	 * Like a scheduled meeting - everything happens at specific times
	 * All operations synchronized to clock edges
	 * More predictable but less flexible
	 * Most modern memory systems use this approach
