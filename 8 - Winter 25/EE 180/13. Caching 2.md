# Review
### 4-Way Set Associative Cache
* We have $X$ sets with $N$ ways.
	* The set-associative cache can be seen as $N$ direct-mapped caches with $X$ entries
	* All $N$ ways can be checked in parallel, and the output is combined together
![Pasted image 20250218133311](../../attachments/Pasted%20image%2020250218133311.png)

### Cache Write Policies
![Pasted image 20250218133433](../../attachments/Pasted%20image%2020250218133433.png)

### Write Miss Options
* When we miss on a write, the main questions are:
	* Allocate: Do we want to bring that line into the cache?
	* Fetch: Do we want to fetch the line from memory, knowing that we will write into the line and thus have to write it back into memory anyways?
![Pasted image 20250218133611](../../attachments/Pasted%20image%2020250218133611.png)

* This is the set of actions we do when we hit a write-miss:
	* Write-invalidate is only for direct-mapped caches because
		* With no-write-allocate, we're not bringing the line into cache on writes anyway
		* In a direct-mapped cache, there's only one possible location for each line
		* So on a write miss, we can simply invalidate that one location if needed
	![Pasted image 20250218134153](../../attachments/Pasted%20image%2020250218134153.png)
### Typical Choices
* Write-back caches:
	* *Write-allocate with fetch-on-miss:* since we're keeping modified data in cache to minimize memory writes, we want to allocate space and fetch the full line to enable future reads/writes to the same line.
* Write-through caches
	* *Write-allocate with fetch-on-miss:* when writes go directly to memory but we expect subsequent reads/writes to the same line, fetching the whole line maximizes cache utilization.
	* *Write-allocate with no-fetch-on-miss:* since we're writing through to memory anyway, we can save bandwidth by not fetching data we'll immediately overwrite.
	* *No-write-allocate with write-around:* when we don't expect locality in write patterns, skipping cache allocation entirely (going around) reduces cache pollution and memory traffic.

# Caches and Processors
* So far, we have had unified instruction and data caches
* Most chips have **separate caches for instructions and data**
	* Often noted as `$I` and `$D` or `I-cache` and `D-cache`
	* Advantages:
		* Extra access port ⟶ more bandwidth
		* Low hit time
		* Ability to customize each cache for specific access patterns
	* Disadvantages:
		* Inefficient capacity utilization due to fixed partitioning
		* Potentially higher miss rates when one cache is full while the other is underutilized

* **Unified Cache**: a single unified DRAM with two ports for accessing instructions and data
	* More flexible and efficient use of total memory capacity ⟶ dynamically allocate space between instructions and data based on program, rather than statically partitioning as in separate caches.

## Multilevel Caches
* **Primary (L1) caches** are attached to CPU
	* Small, but fast ⟶ focuses on hit time rather than miss rate
	* Usually has separate `I-cache` and `D-cache`
* **L2 cache** services misses from primary cache
	* Larger, slower, but still faster than main memory
	* Focuses on low miss rate rather than low hit time ⟶ already incurred delay from L1 miss, want to avoid even larger penalty of missing
	* Usually implemented as a `Unified cache`
* **Main memory** services L2 cache misses
	* Many chips also include an L3 cache ⟶ shared between cores of a multi-core chip
	* Usually implemented as a `Unified cache` to minimize miss rate
![Pasted image 20250218135451](../../attachments/Pasted%20image%2020250218135451.png)

## Cache Datapath
* Cache outputs hit if valid bit (V) is set and tag matches address tag
![Pasted image 20250218135709](../../attachments/Pasted%20image%2020250218135709.png)

## Control Signals
![Pasted image 20250218135647](../../attachments/Pasted%20image%2020250218135647.png)
![Pasted image 20250218135653](../../attachments/Pasted%20image%2020250218135653.png)

## Cache Controller FSM
![Pasted image 20250218135746](../../attachments/Pasted%20image%2020250218135746.png)

# Multi-core Caches and Memory
* **Cache coherence**
	* Definition: Ensures all cores see consistent data values for each memory location
	* Goal: Any read operation returns the most recently written value, regardless of which core wrote it
	* Maintains consistency for a *single memory location* across all processors
* **Memory Consistency**
	* Definition: Defines the ordering rules for when writes become visible to other cores
	* Goal: Specifies how reads and writes to different memory locations should appear to be ordered across all cores
	* Maintains consistency for *all memory locations* across all processors
		* Defines when writes to `X` propagate to other processors, relative to reads and writes to **other** memory locations

# Cache Coherence Protocols
* Naive protocol: turn off caching
* Smarter protocol: allow one writer at a time
	* All caches can have copies of data while read-shared
	* On a write, invalidate all copies in other caches and allow a single writer

* For any memory address `x`, at any given time period (epoch):
	* **Single-Writer, Multiple-Read (SWMR) Invariant**
		* We either have a single writer or multiple readers.
			* **Read-write epoch:** ONE processor with write access (can also read)
			* **Read-Only epoch:** multiple processors with read-only access
		* SWMR prevents concurrent writes to same address
	* **Data-Value Invariant (write serialization)**
		* The value of the memory address at the start of an epoch is the same as the value of the memory location at the end of its last read-write epoch
		* When transitioning between epochs (time periods):
			* The value at the start of a read-only epoch must match the value from the end of the previous write epoch
		* Data-Value ensures all reads see the most recent write
![Pasted image 20250218140942](../../attachments/Pasted%20image%2020250218140942.png)

## Write-Through, No-Write-Allocate Coherence
* The processor that `PrWr` stays in its current state (`Valid` or `Invalid`)
* If a controller sees a `BusWr`, it moves from `Valid` to `Invalid` (need to refresh cache line from mem)
![Pasted image 20250220133823](../../attachments/Pasted%20image%2020250220133823.png)

## MSI Coherence Protocol
* MSI (Modified, Shared, Invalid) protocol for write-back caches
![Pasted image 20250218141229](../../attachments/Pasted%20image%2020250218141229.png)
* States:
	* Modified (M): Cache has exclusive access and data is dirty
	* Shared (S): Multiple caches can have read-only copies
	* Invalid (I): Cache line contains invalid data
* State transitions:
	* I ⟶ S: `PrRd` triggers bus read (`BusRd`) if line invalid. Cache gets shared copy
	* I ⟶ M: `PrWr` triggers "read-for-ownership" (`BusRdX`) for exclusive access
	* S ⟶ M: If line is Shared but processor writes (`PrWr`), request exclusive access (`BusRdX`)
	* M ⟶ S: On remote read, if cache is Modified, owner supplies data via `BusWB` and transitions to Shared
	* M ⟶ I: On remote read-exclusive, Modified cache supplies data via `BusWB` and invalidates
	* S ⟶ I: On remote read-exclusive (`BusRdX`), invalidate shared copy

### Example: Implementing LL/SC
* We can use cache coherence protocols to implement `LL/SC`
* Load-Linked (LL):
	* Gets read access (S state)
	* Records the cache line for monitoring
* Store-Conditional (SC):
	* Only succeeds if cache line stayed in S state
	* Fails if line was invalidated by another processor

# Memory Consistency
* Memory consistency defines when writes become visible to other processors
* "Visible" means: when a read operation returns the newly written value
* Critical for maintaining program correctness in multi-threaded applications

## Sequential Consistency
* Definition: All processors observe the same global ordering of memory operations
* Key Properties:
	* Operations appear to execute in program order
	* Global ordering is consistent across all processors
	* No processor sees writes in a different order than others
* The ISA includes rules about how loads and stores should be ordered
## Example: ISA Memory Ordering Rules
1. Write Completion
	* A write is only considered complete when all processors can see the new value
	* Ensures global visibility before proceeding
2. Operation Ordering
	* Processors must maintain program order for writes
	* Cannot reorder writes with other memory accesses
### Implementation Consequences
* Write Ordering
	* If P writes `X` then `Y`, any processor seeing new `Y` must also see new `X`
* Read Flexibility
	* Processors can reorder reads for performance
	* But writes must strictly maintain program order

# Cache Friendly Software
* Cache coherence and memory consistency make it so that caches are transparent to the software
	* While this is good, it can lead to performance inefficiencies
![Pasted image 20250218143154](../../attachments/Pasted%20image%2020250218143154.png)
* Left example: loop in row-major order, going through each row one by one
	* This matches how the data is stored in memory
	* For each cache block:
		* First access is a miss
		* Next 3 accesses are hits (spatial locality)
	* The miss rate is `1 miss per 4 accesses = 25%`
* Right example: loop in column-major order, going through each column one by one
	* This does not match how the data is stored in memory ⟶ each item in column is in a different cache block
	* Every access is a miss, so miss rate is `100%`

## Matrix Multiplication Example
![Pasted image 20250218144807](../../attachments/Pasted%20image%2020250218144807.png)
* We need to order these loops correctly in order to have cache friendly performance.
![Pasted image 20250218144832](../../attachments/Pasted%20image%2020250218144832.png)
* Notice that the most efficient way is to do matrix multiplication in blocks with a fixed block size.
![Pasted image 20241107153354](../../attachments/Pasted%20image%2020241107153354.png)
