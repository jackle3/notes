# Memory
* Once we read the address, it gets decoded into multiple word lines (shown in red horizontal lines)
* The word lines intersect with bit lines (shown in green vertical lines) at memory cells
* When a word line is activated:
	* All memory cells along that row connect to their respective bit lines
	* The multiplexer (Mux) at the bottom selects which bit line's data to output
	* The write enable (WE) signal controls whether we are reading or writing to the selected memory cells
* This forms a memory bank structure where:
	* Each row represents one memory word
	* Each column represents one bit position
	* The decoder and multiplexer allow us to access any word in memory
![[Pasted image 20250128144909.png]]
![[Pasted image 20250128144953.png]]
![[Pasted image 20250128145045.png]]
* To make sure the write enable goes up after the address and input data is stable:
	* **Option 1:** We `and` the `WE` signal with the inverse of the `CLK` signal
		* This means the write operation happens in the second half of the clock cycle:
			* First half (CLK high): Address and data signals propagate and stabilize
			* Second half (CLK low): WE signal enabled, allowing write to occur
		* By waiting half a clock cycle, we ensure the address and data are valid before writing
		* The next rising edge of the clock marks the start of the next instruction
	* **Option 2:** We add a delay element to the WE signal path
		* This introduces a small time delay after the address and data signals
		* The delay ensures WE is activated only after inputs are stable
		* Similar to Option 1 but uses dedicated delay hardware instead of clock gating

# Single Cycle Processor
* In a single cycle processor, each instruction must complete within one clock cycle.
* The `load` instruction is typically the slowest, taking 800 picoseconds to:
	* Read from memory, process the data, write back to registers
![[Pasted image 20250128145059.png]]
* To ensure correct operation:
	* The clock period must be at least as long as the slowest instruction
	* We set it to 800 picoseconds to match the `load` timing
	* This gives us a clock frequency of 1.25 GHz (1/800ps)
* This means all instructions, even simple ones, must wait 800ps to complete — limiting overall processor performance
![[Pasted image 20250128145201.png]]

## Variable Clock Single Cycle Processor
* You can implement a processor with a variable-length clock
* Depending on the program instruction mix, we can get a better CPU clock cycle.
* Issue: this is very hard to design at high speeds.
![[Pasted image 20250128145207.png]]

# Pipelining
![[Pasted image 20250130134709.png]]
* Filling period: edge on left, not all pipelines utilized
* Draining period: edge on right, not all pipelines utilized

## Processor Execution
* Executing an instruction has 5 stages, each of which take one clock cycle.
![[Pasted image 20250130134822.png]]
* To pipeline, we can overlap instructions in different stages.
	* This ensures all our hardware is used all the time.
	* Clock cycle is faster:
		* Originally, we had one cycle for the entire `lw` instruction
		* Now we break it into 5 cycles for each stage.
![[Pasted image 20250130134851.png]]

### CPI
* The CPI is 1 ⟶ we are fetching and finishing one instruction per cycle
* The latency is 5 cycles ⟶ each instruction still takes 5 cycles to complete
![[Pasted image 20250130142130.png]]


## Pipeline Datapath
* These pipeline registers (e.g. `IF/ID`, `ID/EX`) are not accessible by software.
* Pipelining is done by the hardware — the software is the same.
![[Kapture 2025-01-30 at 14.00.03.gif]]

## Pipelined Control
![[Pasted image 20250130140839.png]]
![[Pasted image 20250130140844.png]]
* There are multiple versions of the control signals propagated through the stages of the pipeline.
	* E.g. `ExtOp_rf` is the `ExtOp` control signal for the instruction currently in the `RF/ID` stage.
	* It will change names to `ExtOp_ex` when this instruction reaches the `EX` stage.
![[Pasted image 20250130140849.png]]

## Pipelined Processor Diagram
![[Pasted image 20250130141218.png]]

## MIPS ISA
![[Pasted image 20250130141234.png]]

## Pipeline Performance
![[Pasted image 20250130141332.png]]
* Both have the same CPI of 1.
	* In single-cycle, the clock cycle time is the time for the slowest instruction because we wait for the slowest instruction to finish before starting the next one.
	* In pipelined, the clock cycle time is the time for the slowest stage, since we can overlap instructions in different stages.
![[Pasted image 20250130141354.png]]

# Pipelining Issues
* If pipelining improves clock cycle time while maintaining CPI=1, why not add more stages?
* Three main issues prevent unlimited pipelining:
	1. Some operations must complete within a single cycle
	2. In practice, CPI is not actually 1
		* Pipeline hazards and stalls increase effective CPI
	3. Each pipeline stage requires additional registers
		* More stages = more area overhead
		* Increased power consumption from:
![[Pasted image 20250130142247.png]]

# Pipeline Hazards
* Situations that prevent completing an instruction every cycle (leads to CPI > 1)
* **Structure Hazards** ⟶ a required resource is busy
* **Data Hazards** ⟶ Must wait for previous instruction to produce/consume data
* **Control Hazards** ⟶ Next PC depends on previous instruction

## Structural Hazards
* Resource conflict when **two instructions need same hardware** in same cycle
* Example: Pipeline with single unified memory
	* If instruction memory and data memory share same physical memory
	* Load/store instructions need memory access for data
	* Instruction fetch also needs memory access
	* One must stall while other accesses memory
	* Creates a "bubble" in pipeline
* Other examples:
	* Functional units not fully pipelined (e.g. multiply/divide)
		* Unit busy with previous operation when next instruction needs it
		* Must stall until unit available
	* Register file when simultaneous read/write needed
		* Though modern designs typically handle this with special hardware
![[Pasted image 20250130142428.png]]
![[Pasted image 20250130142447.png]]
* To solve this, we used delayed write-back:
	* Force ALU instruction to delay write by one cycle so that pipeline works
![[Pasted image 20250130142526.png]]

## Data Hazards
![[Pasted image 20250130142831.png]]
![[Pasted image 20250130143031.png]]
* In the 5-stage pipeline, we do not need to worry about:
	* WAW hazard because the write-backs for subsequent instructions will be staggered in program order.
	* WAR hazard because the read is before the write in the pipeline, meaning that the reads and writes will be staggered in the correct program order.
* However, we do need to worry about RAW hazards.

### RAW Hazards
![[Pasted image 20250130144058.png]]
![[Pasted image 20250130144115.png]]
![[Pasted image 20250130144215.png]]

### Stalls
![[Pasted image 20250130144408.png]]
![[Pasted image 20250130144453.png]]
![[Pasted image 20250130144551.png]]

### Reducing Stalls: Fast RF
![[Pasted image 20250130144649.png]]
![[Pasted image 20250130144654.png]]
![[Pasted image 20250130144742.png]]
### Reducing Stalls: Forwarding
![[Pasted image 20250130144751.png]]
* We can eliminate all stalls due to RAW hazards for all arithmetic and logical instructions.
![[Pasted image 20250130144758.png]]
* Though we can't reduce the stall when using `lw` because value of load is not available until end of the `MEM` stage.
![[Pasted image 20250130144844.png]]
* We need to have a one cycle stall when loading values.
![[Pasted image 20250130145119.png]]
