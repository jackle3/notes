# Key Topics
**Hardware-software interface**
* Key abstractions for programmable hardware
* Including data-level and thread-level parallelism
**Efficiency metrics**
* How to reason about performance, power/energy, and cost
**Programmable and custom hardware design**
* Memory system design For simple and multi-core or heterogeneous systems
**Operating system support**
* Virtual memory
**I/O devices and interfacing techniques**

# Logistics
## Textbook
* Computer Organization & Design, MIPS edition 6th Edition By David Patterson & John Hennessy
* Free eBook via Engineering Library
## Exams
* Midterm will likely be an online quiz, 24 hours to take it
* Final exam will be in person
## Workload and Grading
* 3 homework sets ⟶ 15%
* 4 laboratory assignments ⟶ 35%
* Midterm quiz (lectures 1-10) ⟶ 20%
* Final quiz (all lectures) ⟶ 20%
* Class Participation ⟶ 10%
* No late days

# What is a Digital System?
* Computers, appliances, credit cards, datacenters, cars, etc.

**Inside a typical system:**
![[Pasted image 20250107135057.png]]
![[Pasted image 20250107135223.png]]

**Why do we compute and store digital data when the world is mostly analog?**
* Computer stores and computes information in bits (zeros and ones)
* It is easier to build a more reliable digital computer than it is to build an analog one.
	* Digital computers have a wider margin of error (ranges are zero and one)
	* Analog requires you to keep track of a whole continuous range

**Why do we need both programmable processors and fixed-function hardware?**
* Programmable processors can be used for any algorithm ⟶ general purpose but less efficient.
* Fixed-function is very efficient and performant, good for algorithms that are commonly used.
	* E.g. encryption/decryptions, accelerators, etc.
* This is why FPGAs are becoming more interesting ⟶ provides custom hardware solution with reasonable performance while still allowing reasonable customizability (good middleground)

**Why don't we use separate memories for programs and data?**
* Explicitly partitioning memory for program and data prevents flexibility.
* Better to use a dynamically shared resource.
* Also allows you to load multiple applications into different parts of memory ⟶ multiple programs at once.l

## Semiconductor Chips
* **Dominant tech for integrated circuits**
* **Printer-like process**
	* On a semiconductor surface, we print transistors (switches) for logic, memory devices, and their interconnect
* **Print resolution has been improving over time**
	* I.e. print more transistors in the same amount of area
	* Can build more capable digital systems with similar costs (per chip area)
* **Moore's Law**
	* Number of transistors in an integrated circuit doubles roughly every 1-3 years
		* Original prediction 2x density per year
		* Reality is 2x density every 2-3 years

![[Pasted image 20250107140935.png]]
* The slide explores the capacity of a 10mm x 10mm logic chip (100mm²) manufactured with 14nm technology.
1. **Chip Dimensions**:
	* Each side of the chip is 10mm, creating a total area of 100mm².
2. **Manufacturing Technology**:
	* **14nm drawn gate length**: Refers to the transistor gate length, a key metric for chip performance and density.
	* **33nm wire pitch**: The spacing between adjacent wires (important for wiring density).
	* **14 wire levels**: Indicates the number of metal layers available for wiring.
3. **For comparison:**
	1. **32-bit RISC Integer Processor**:
		* Requires a grid of 1,000 x 2,000 wires.
		* Up to **45,000 processors** can fit on this 100mm² chip.
		* `(303,000)^2 / (1000 * 2000) = 45,904.5`
	2. **SRAM (Static RAM)**:
		* Approximately 4x4 wire grids are needed per bit.
		* The chip can accommodate up to **5.5 billion SRAM cells**.
	3. **DRAM (Dynamic RAM)**:
		* Requires a smaller 1x2 grid per bit.
		* The chip can contain **45 billion DRAM cells**.

# Complexity and Efficiency
**Why don't we implement all the functionality of the iPhone in a single chip?**
1. Complexity challenges
2. Efficiency challenges

## Complexity Challenge
* Complexity is the limiting factor in modern chip design
* We want to use all the transistors on a chip
	* Need HW components for: cellphone, camera, TV, computer, …
	* Too many applications to cast all into hardware logic
		* Cost: $75M and 1-3 years for a high-end design
	* Difficult to verify, fix bugs, and upgrade once design is out
* **Solutions:**
	* Hide complexity using abstraction
	* Design reusable hardware components

### Abstractions
* Use layers of abstraction to simplify the design and complexity of the processor.
* **Key Idea:** at various levels of the HW/SW stack, we want stable interfaces to expose functionality to the layers above. We can then optimize either side to simplify the design.
![[Pasted image 20250107141611.png]]

### Reusable Hardware
* We can rely heavily on programmable processors (aka cores, CPUs, or MCUs).
* Use software to program this hardware ⟶ software can be used for multiple hardware interfaces
![[Pasted image 20250107141827.png]]

## Processor Scaling
* Single-threaded performance has started to plateau ⟶ increasing number of transistors has not helped
* To address, we started **adding more logical cores** to take advantage of the higher number of transistors.
![[Pasted image 20250107142218.png]]

## Efficiency Challenge
### Moores + Dennard
**Historically, with every generation of processing manufacturing:**
* Moore's Law
	* We got 2x the number of transistors for the same cost
	* We got 1.4x the frequency
* Dennard scaling (power density remains constant as transistors shrink)
	* The voltage decreases to 0.7x because transistors shrink
* The capacitance decreases to 0.7x because transistors shrunk
* In total, we got 2.8x capability for the same power consumption and same cost.
$$
P_{1} = QCV^2f \tag{before}
$$
$$
P_{2} = (2Q)(0.7C)(0.7V)^2(1.4f) = 0.9604P_{1}\tag{after}
$$
where $Q$ is number of transistors, $C$ is capacitance, $V$ is voltage, $f$ is frequency.
![[Pasted image 20250107142428.png]]

### Moores but no Dennard
**Presently, we no longer have Dennard scaling**
* As transistors have shrunk to nanometer scales, leakage currents (where transistors lose power even when idle) have become a significant issue.
![[Pasted image 20250107143050.png]]

### Power Limited
* **Computers are now power-limited** ⟶ partially due to heat generation
* The goal is to reduce energy per operation (improve energy efficiency).
	* A popular approach is to have specialized hardware (e.g. Apple M1, Google TPU) with better efficiency.
![[Pasted image 20250107143136.png]]

# IMPORTANT: Key Tools for System Architects
![[Pasted image 20250107144300.png]]
## 1. Pipelining
* Break **tasks** into a **sequence of stages**
* We can **overlap** the execution of tasks through different stages (kinda like cars in a car wash)
* **Issues:**
	* stalling ⟶ some stages might bottleneck following stages which causes it to stall.
![[Pasted image 20250107143523.png]]![[Pasted image 20250107143640.png]]
## 2. Parallelism
* Use **multiple HW components** to execute multiple tasks completely in parallel
* **Issues:**
	* in order to execute things in parallel, we need true independence
	* Need to manage shared resources (e.g. need four checkout lines to checkout in parallel)
## 3. Out-of-Order Execution
* Process tasks in order they become ready for execution, not in the order defined.
* As soon as an instruction's dependencies are complete, execute it.
* **Issues:**
	* Need a way to put instructions back in order at the end ⟶ consistency and coherency
## 4. Prediction (or Speculation)
* Break dependencies between tasks by making educated guesses about missing information
* *Speculate* that a condition is likely to pass, so compute the body of that condition before we actually check it
* **Issues:**
	* If we guess wrong, we have to throw away or undo our work
## 5. Locality and Caching
* **Recent past is a very good indication of near future**
	* Temporal locality: if you just did something, it is very likely that you will do the same thing again soon
	* Spatial Locality: if you just did something, it is very likely you will do some thing related or similar next
* Leads to predictable patterns and allows us to **keep small amounts of important information nearby** for fast access
* **Issues:**
	* Hard to improve applications with random memory access patterns; cache's don't help and might even hurt performance
	* Also introduces some security issues related to caching
## 6. Indirection
* Instead of accessing an item directly, access through an additional name, reference, or container
* This can help us with security checks with we access memory, etc
## 7. Amortization
* Amortize the initial cost of an operation by performing multiple similar or dependent tasks together
* E.g. if I'm gonna cook once, just cook for the entire week and meal-prep
* **Issues:**
	* Tradeoff between latency of a single task and amortizing the initial task and related tasks
## 8. Redundancy
* Maintain spare copies of information and components to allow fault handling
* **Issues:**
	* Having spare copies consumes resources that generally might not be used
	* Updating copies for redundancy can decrease performance and efficiency
	* If copies differ, we need a scheme to figure out what copy is correct
## 9. Specialization
* Customize a component to a **small set of common tasks**
* Allows us to avoid the overheads associated with making component more general
* **Issues:**
	* Expensive to build and verify specialized hardware; doesn't make sense for apps/algs that change frequently
## 10. Focus on the Common case
* Optimize the aspects that most dominantly affects performance, energy, or cost
* Do some analysis of where you are spending the cost time, energy, and cost ⟶ optimize major contributors
## 11. Focus on the Uncommon case
* Uncommon execution behaviors may give rise to security vulnerabilities ⟶ these behaviors are less tested
