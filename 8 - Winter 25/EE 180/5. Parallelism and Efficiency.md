# Parallelism
1. Parallelism within a **single thread of control**
	* Data-level parallelism within a single core ⟶ vectorization
2. Parallelism across **multiple threads of control**
	* Thread-level parallelism across multiple cores.
# Data Parallelism (Vectorization)
* Vectorization allows us to perform $N$ operations using one instruction.![[Pasted image 20250116144124.png]]
## Instructions
* The basic **vector instructions** are:![[Pasted image 20250116144207.png]]
## Benefits
* Each instruction specifies parallel work ⟶ allows for higher performance
* Instruction cost is amortized better
	* There is overhead in energy for fetching and decoding instructions
	* Vector instructions can amortize energy cost over multiple arithmetic operations

## Example: SAXPY
* With scalar instructions, we need a loop and more instructions.
* With vector instructions, we can multiply the entire 32-element vector.
![[Pasted image 20250116144827.png]]

## Vector Length
* Depending on ISA, vector instructions are either:
	* **Fixed length**: The instruction operates on all elements in the vector
		* Example: `vadd`, Intel SSE instructions
	* **Variable length**: The number of operations is controlled by a vector length register (VL)
		* VL can be between 0 and the maximum number of elements
		* Example: `vadd.vv` with VL=10 performs:
			* `for (i=0;i<10;i++) V1[i] = V2[i] + V3[i]
## Strip Mining
* When the application vector length (`N`) > the maximum vector length (`MVL`):
	* We need to "strip mine" the computation into chunks
	* This involves two loops:
		1. Processes chunks of size MVL until remaining elements < MVL
		2. Processes the remaining elements (N mod MVL)
* Example of strip mining for SAXPY (`Y = aX + Y`):
	1. First handle `N mod MVL` elements with one vector operation
	2. Then handle remaining elements in chunks of `MVL`
	3. Uses vector length register (`VL`) to control number of elements processed
* In pseudocode:
![[Pasted image 20250121134655.png]]

## Conditional Execution
* To conditionally execute a vector instruction, precompute a mask figure out which lanes should be computed.
![[Pasted image 20250121134836.png]]

# Thread-level Parallelism
* Modern processors have multiple cores.
	* Each core has a private PC and registers.
	* All cores shared the same shared memory
	* Cores communicate with each other via load and stores
![[Pasted image 20250121135038.png]]
## Shared Memory Synchronization
* When all cores attempt to increment a shared variable, it can lead to race conditions.
![[Pasted image 20250121135244.png]]
* We need a way to do **read/modify/write** atomically
	* No other access to memory location allowed in between
	* Using these, we can build higher-level functions like locks, barriers, etc.

* There are **multiple types of synchronization instructions**
	* Atomic swap (register ↔ memory)
	* Atomic increment (load ⟶ increment ⟶ store)
	* Test & set (read old value & set to a new value if it hasn't changed)

* MIPS uses **load-linked** and **store-conditional**
	* `ll $rt, offset($rs)`
		* Loads value from memory into register and track contents
	* `sc $rt, offset($rs)`
		* Succeeds if location has not changed since last `ll`
			* Returns `1` in `$rt`, meaning memory was updated.
		* Otherwise, fails and does not update memory
			* Returns `0` in `$rt`, meaning memory was not updated.
* We can use this to allow multiple cores to increment a shared variable.
	* If the result of `sc` is `0`, then we loop and try again until it returns a `1`.s
![[Pasted image 20250121140122.png]]

# Efficiency Metric
* The main metrics we consider are:
	* Cost, Performance, Power, and Energy
	* Also consider Reliability and Predictability (see EE282)

# Cost
## Manufacturing Semiconductor Chips
* We start with silicon ingots, then cut them into wafers. Then we etch the circuit patterns onto the wafer. Then we dice the wafer into individual dies. Then we package the dies.
![[Pasted image 20250121141635.png]]
## Yield
![[Pasted image 20250121141741.png]]
## Cost of Semiconductor Chips
![[Pasted image 20250121141904.png]]
![[Pasted image 20250121142009.png]]
![[Pasted image 20250121142103.png]]

# Performance
* There are two main metrics in terms of performance:
	1. **Latency** (aka response/execution time)
		* How long it takes to do a task
	2. **Throughput**
		* Total work done per unit time (e.g. queries per second)
## Latency
![[Pasted image 20250121142358.png]]

## CPU Time
* One measure of latency is the **execution (or CPU) time.**
![[Pasted image 20250121142925.png]]
![[Pasted image 20250121142937.png]]

## Instruction Count and CPI
![[Pasted image 20250121143101.png]]

## Performance Summary
* The execution time is a factor of:
	* IC (instructions per program)
	* CPI (cycles per instruction)
	* Clock time (seconds per cycle)
		* Note that clock time is the inverse of clock rate (cycles per second)
![[Pasted image 20250121143144.png]]

## Calculating CPI
* When different instructions take different numbers of cycles, we need to do a weighted average to get the average cycles per instruction (CPI) for the program.
![[Pasted image 20250121143222.png]]
![[Pasted image 20250121143244.png]]

## Relative Performance
![[Pasted image 20250121143554.png]]
![[Pasted image 20250121143613.png]]

# Power and Energy
* This is important because of:
	1. **Power density (cooling)**
		* Limits compaction and integration on chips
		* E.g. a cellphone chip cannot exceed 1 to 2 watts
	2. **Reliability at high temperatures**
	3. **Battery life for mobile devices**
	4. **Cost**
		* Energy cost
		* Cost of power delivery, cooling system, packaging
	5. **Environmental issues**

## Power Consumption in Chips
* In terms of the formula:
	* `C` is capacitance
	* `Vdd` is the power supply voltage
	* `F` is the clock frequency
	* `I_leakage` is the leakage current
![[Pasted image 20250121143932.png]]

## Energy
![[Pasted image 20250121144327.png]]

## Scaling
* We can no longer scale chip capability by simply adding more transistors.
* As transistors have shrunk to nanometer scales, leakage currents (where transistors lose power even when idle) have become a significant issue.
* **Computers are now power-limited** ⟶ partially due to heat generation
![[Pasted image 20250121144537.png]]
 * We can increase `instructions per second` to get higher performance, but power will increase!
	 * To offset it, we minimize the `energy per instruction`

## Amdahl's Law
![[Pasted image 20250121144613.png]]

## Benchmarks
![[Pasted image 20250121145022.png]]
