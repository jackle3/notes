
# Memory and Performance
* Memory system is important for performance
* Cache access time often determines the clock cycle time
* Memory stalls is a large contributor to CPI
	* Stalls due to misses for instructions & data (reading & writing)
	* Stalls due to write buffers being full
	* Stalls due to coherence actions

## Caches and CPI
* $\text{Execution Time} = \text{IC} * \text{CPI} * \text{Clock Cycle Time}$
* $\text{CPI} = \text{CPI}_{pipe} + \text{AvgMemStallCycles}$
	* $\text{CPI}_{pipe} = 1 + \text{PipelineHazardStallsCycles}$
* $\text{MemStallCycles} = \text{ReadStallCycles} + \text{WriteStallCycles}$

## Read and Write Stalls
* Read stalls are fairly easy to understand
$$
\text{Read Stall} = \frac{\text{Reads}}{\text{Program}} \cdot \text{ReadMissRate} \cdot \text{ReadMissPenalty}
$$
* Write stalls depend upon the write policy
	* **Write-through** (write to cache and write buffer, write buffer drains into memory async)
$$
\text{Write Stall} = \left(\frac{\text{Writes}}{\text{Prog}} \cdot \text{WriteMissRate} \cdot \text{WriteMissPenalty}\right) + \text{Write Buffer Stalls}
$$
	* **Write-back** (write to cache only, write to memory when evicted)
$$
\text{Write Stall} = \left(\frac{\text{Writes}}{\text{Prog}} \cdot \text{WriteMissRate} \cdot \text{WriteMissPenalty}\right)
$$
* “Write miss penalty” can be complex:
	* Can be partially hidden if processor can continue executing
	* Can include extra time to write-back a value we are evicting

## Memory Stalls
* In the worst-case (and for simplicity), assume write/read misses cause the same delay
$$
\begin{align*}
\text{MemStallCycles} &= \text{ReadStallCycles} + \text{WriteStallCycles} \\\\
&= \frac{\text{Memory accesses}}{\text{Program}} \cdot \text{MissRate} \cdot \text{MissPenalty} \\\\
&= \frac{\text{Instructions}}{\text{Program}} \cdot \frac{\text{Misses}}{\text{Instructions}} \cdot \text{MissPenalty}
\end{align*}
$$
* In a single-level cache system:
	* Miss Penalty = latency of DRAM
* In a multi-level cache system:
	* MissPenalty is the latency of L2 cache etc
	* Calculate by considering MissRateL2, MissPenaltyL2 etc
	* Watch out: global vs local miss rate for L2

## Example 1: Memory Stalls
### Question
* Consider the following
	* Miss rate for instruction access is 5%
	* Miss rate for data access is 8%
	* Data references per instruction are 0.4
	* CPI with perfect cache is 2
	* Read and write miss penalty is 20 cycles
	* Including possible write buffer stalls
* What is the performance relative to a system without misses?
	* We may start by considering execution times (`IC * CPI * CCT`)
	* But `IC` and `CCT` are the same in both scenarios, so focus on `CPI`

### Solution
* CPI with a perfect cache is 2
* CPI with a cache with misses is `2 + AemStallCycles`
![[Pasted image 20250220140035.png]]

## Example 2: Write Buffers
### Question
* Given the following data
	* Base CPI of 1.5
	* 1 instruction reference per instruction fetch (read)
	* 0.27 loads/instruction (read)
	* 0.13 stores/instruction (write)
	* A 64KB set-associative cache with **4-word block size** has a miss rate of 1.7%
	* Memory access time = 4 cycles + # words/block
* Suppose the cache uses a **write through, write-around write strategy without a write buffer**.
* How much faster would the machine be with a **perfect write buffer**?

### Solution
* Notice that the CPU time is:
$$
\text{CPU Time} = \text{IC} * (\text{CPI}_\text{base} + \text{CPI}_\text{mem}) * \text{Clock Cycle Time}
$$
* The $\text{IC}$ and $\text{CCT}$ are constant, so the $\text{CPU Time}$ is proportional to $\text{CPI} = \text{CPI}_\text{base} + \text{CPI}_\text{mem} = 1.5 + \text{CPI}_\text{mem}$

* **Without any write buffers**, we do not incur the cost of the `WriteBufferStalls`
	* Notice that when we read, we read the entire 4-word block
	* When we write, since we write-around, we write the word to memory without reading the block
	* We **don't need to check the cache on writes** since we're writing directly to memory -- if the cache line is already in cache, then it is invalidated.
![[Pasted image 20250220140411.png]]

* With a **write buffer for memory**, we do incur the cost of the write buffer.
	* Notice we no longer incur a write miss penalty since we write to the buffer instead of waiting for memory
	* Instead, we incur a **write hit penalty** because:
		* We need to check if the data exists in the cache (tag comparison)
		* If it's a hit, we must update the cache to maintain consistency
![[Pasted image 20250220140522.png]]

* We can also create a **write buffer for the cache** to avoid the write hit penalty.
	* It allows us to:
		* Queue up writes to the cache without waiting for tag checks
		* Let the processor continue executing immediately
		* Have the cache controller process the buffered writes in the background
	* This effectively hides the write hit penalty since the processor doesn't wait for cache operations
![[Pasted image 20250220141113.png]]

# Virtual Memory
* What questions does virtual memory answer?
	* On a system with 32-bit virtual addresses but less than 4 GB of physical RAM, how can multiple processes each use (what appears to be) a full 4 GB of memory?
		* Each process has 4 GB of virtual memory, but only the portions that are used is actually loaded into physical memory.
	* Can I use a 32-bit computer with <$2^{32}$ = 4GB of memory?
		* Yes! Translation tables map virtual addresses to available physical memory
		* Same applies for 64-bit systems with less than $2^{64}$B memory
	* Can two programs co-exist in memory?
		* Yes - each program gets its own translation table
		* Programs can be mapped anywhere in physical memory
		* Translation includes protection bits (RWX) for security
	* Can I use main memory as a cache for the disk/Flash?
		* Yes - some translations point to memory, others to disk
		* Data moves between disk and memory as needed
		* Translation tables are updated accordingly

* The **key tool here is indirection**

## Physical Memory
![[Pasted image 20250220143026.png]]
![[Pasted image 20250220143038.png]]

## Virtual Memory
* Each process has a separate virtual address space (and page table), and pages are mapped to physical pages by the OS.
![[Pasted image 20250220143049.png]]
![[Pasted image 20250220144008.png]]

## Terminology
* Virtual address: the one generated by the program
	* PC or (register + offset)
* Physical address: the one send to memory
	* After translation from virtual address
* Page: the block size for memory
	* E.g., 8KB or 2MB
* Page fault: a miss for a certain page
	* Translation table points to the disk

## Goals
* Give every program the illusion of a 4GB address space
* Store a subset of these 4GB on actual DRAM
	* Which subset and why?
* Multiplex multiple program on DRAM
	* Without complicating program operation

* Design decisions for MIPS:
	* 4KB page size (block size)
	* Software managed page faults (misses)

# Address Translation
![[Pasted image 20250220143543.png]]
* Notice that the **page offset** (12 lowest bits) is not translated ⟶ only translate base address of page

## Page Table
![[Pasted image 20250220143623.png]]

## Translation Process
* **On valid page**
	* Check access rights (R, W, X) against access type
	* Generate physical address if allowed
	* Generate a protection fault (exception) if illegal access
* **On invalid page**
	* Page is not currently mapped and a *page fault is generated*
* **Faults are handled by the OS**
	* Due to program error (e.g., out of bounds access) => terminated
	* Desired data/code available on disk => refill & restart
	* Space allocated in DRAM, page copied from disk, page table updated
	* Replacement (eviction) may be needed

## Page Faults
* When CPU accesses a page that is not in memory (invalid), it generates a page fault
* The OS handles the fault by:
	* Checking if the page is present on disk. If so, copies the page to memory
	* Updates the page table with the new page
	* Resets the PC to the faulting instruction
	* Restarts the load/store instruction
![[Pasted image 20250220143756.png]]
