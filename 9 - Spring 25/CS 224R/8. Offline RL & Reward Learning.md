# 1 Recap: Offline RL
![](../../attachments/Pasted%20image%2020250507114833.png)
**Issue with existing off-policy critic objective:** OOD actions from $\pi_\theta$
* If we query an action out of the data support, the randomly initialized $Q$ values can be overestimate
![](../../attachments/Pasted%20image%2020250507114842.png)
![](../../attachments/Pasted%20image%2020250507115122.png)
## 1.1 IQL
IQL fixes this by **avoiding any queries beyond dataset** actions altogether.
* It does this by learning a value and Q function using only the states and actions in the data
* This will learn the $V$ and $Q$ for the *behavior policy* ⟶ in order to improve and learn a policy better than the behavior, it uses an asymmetric loss for the value function
	* The *value function* uses expectile loss, which penalizes underestimation more than overestimation
		* Pushes $V$ to estimate values for a policy that's better than the behavior policy
		* By being optimistic (but still grounded in data), it learns a value function that represents an improved policy
	* The *Q function* uses regular MSE loss to accurately estimate action values from the dataset
		* This Q function helps calculate advantages: $A(s,a) = Q(s,a) - V(s)$
		* Since V is optimistic (representing a better policy) but Q is accurate (representing the behavior policy), actions with positive advantage are truly better than the behavior policy's average action
	* This difference between V and Q creates a mechanism to identify which actions in the dataset are better than others, enabling policy improvement without querying OOD actions
![](../../attachments/Pasted%20image%2020250507115245.png)

## 1.2 Conservative Q-Learning
Alternatively, can we just push down the Q values that are OOD ⟶ be conservative about estimates for OOD actions, without affecting data supported actions.
![](../../attachments/Pasted%20image%2020250507115914.png)
The effectively regularize our loss that minimizes Q values for OOD actions:
* This expectation is over states from the dataset and actions from another distribution that covers the OOD actions that the policy might consider
* We want $\mu$ such that it has high Q values for OOD actions, so that when we minimize this term, we're specifically pushing down Q values for actions outside the dataset ⟶ learns a lower bound $\hat{Q}$
![](../../attachments/Pasted%20image%2020250507120641.png)

To correct for over-pessimistism, we can push back up Q-values for in-distribution actions
* The distribution now has high accuracy in support, and is pessimistic outside
![](../../attachments/Pasted%20image%2020250507120751.png)
![](../../attachments/Pasted%20image%2020250507120935.png)

This is the CQL algorithm!
![](../../attachments/Pasted%20image%2020250507120859.png)

We need to compute $L_{\text{CQL}}$ which requires the $\mu$ term ⟶ can we avoid computing it?
* We ideally want a $\mu$ that maximizes $Q$ but also has high entropy so it covers more of the distribution
* With a max entropy regularization, the optimal $\mu$ is proportional to $\exp(Q)$ ⟶ we use that instead of $\mu$
![](../../attachments/Pasted%20image%2020250507120945.png)

## 1.3 Summary
![](../../attachments/Pasted%20image%2020250507121647.png)

# 2 Where Do Rewards come From?
We've talked about learning policies that maximize rewards ⟶ but where do rewards come from?
* In computer games, we can simply use the score as a reward
* In real world scenarios, what is the reward? ⟶ usually need a proxy
* Alternative: just use **imitation learning** without a reward function
	* (-) no reasoning about outcomes or dynamics, might not be opssible in certain scenarios
* **Key Goal:** Can we reason about what the expert is trying to achieve?
![](../../attachments/Pasted%20image%2020250507132808.png)

# 3 Learning Rewards from Example Goals
## 3.1 Basic Goal Classifier
**Key Idea:** train a classifier to distinguish goal states from other states
* (-) very sparse rewards, only know whether we are in or out of goal states
* (-) reward hacking ⟶ RL algorithms will seek out states the classifier thinks is good, but to do this it may just find states that the classifier wasn't trained on
![](../../attachments/Pasted%20image%2020250507133007.png)

## 3.2 Reward Classifier
To **prevent reward hacking**, use a conservative approach similar to CQL:
* After policy updates, label all visited states as "unsuccessful" and retrain the classifier
	* This makes the classifier more conservative about what counts as a goal
* Key insight: Use balanced training batches (equal successful/unsuccessful examples)
	* Ensures true goals still get p≥0.5 even as we add more negative examples
	* Prevents the classifier from becoming too pessimistic while still blocking exploitation
![](../../attachments/Pasted%20image%2020250507133223.png)
![](../../attachments/Pasted%20image%2020250507133430.png)

This method is actually pretty much a GAN
* The classifier is the discriminator ⟶ discriminates between positive examples given and everything the policy visits
* The generator is the policy ⟶ tries to generate examples that the discriminator will classify as positive
![](../../attachments/Pasted%20image%2020250507133937.png)

## 3.3 Summary
![](../../attachments/Pasted%20image%2020250507134134.png)

# 4 Learning Rewards from Human Preferences
Instead of just reaching a goal state, what if we want to learn rewards for good behavior and bad behavior?
* **Key Idea:** Learn a reward function that matches (relative) human preferences
![](../../attachments/Pasted%20image%2020250507134319.png)

Our **reward should be higher for the preferred trajectory** compared to the alternative
![](../../attachments/Pasted%20image%2020250507134329.png)

We can do maximum likelihood estimation to maximize the probability of the human preferences
* The sigmoid function (σ) converts the reward difference into a probability between 0 and 1
* We maximize the log probability of the model **correctly predicting human preferences**
	* This is essentially a binary classification problem: "Will a human prefer trajectory A over B?"
	* Once trained, this reward function can be used in standard RL algorithms
![](../../attachments/Pasted%20image%2020250507134411.png)

## 4.1 Reward Learning Algorithm
![](../../attachments/Pasted%20image%2020250507134951.png)

## 4.2 Examples of RLHF
![](../../attachments/Pasted%20image%2020250507135049.png)
![](../../attachments/Pasted%20image%2020250507135104.png)
![](../../attachments/Pasted%20image%2020250507140034.png)

## 4.3 RLAIF
The providing of preferences could also be done by an AI
* If you provide the AI with a constitution, you can then ask it to critique which response is better
![](../../attachments/Pasted%20image%2020250507140007.png)

# 5 Unsupervised RL
**Key Idea:** the RL agents can learn to propose their own goals instead of having a human-provided goal
* E.g. have one agent be a goal-setter and another agent be a goal reacher ⟶ these two agents can self-play and improve upon themselves
![](../../attachments/Pasted%20image%2020250507140115.png)

# 6 Summary of Reward Learning
![](../../attachments/Pasted%20image%2020250507140050.png)
