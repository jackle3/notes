
# 1 What is Multi-Task RL?
So far, the RL algorithms focus on maximizing performance on a **single task** with rewards
![](../../attachments/Pasted%20image%2020250527140753.png)

Similar tasks and scenarios have **shared structures** ⟶ we can share the learning from one task to another
![](../../attachments/Pasted%20image%2020250527140813.png)

## 1.1 Formalizing Different Tasks
By formalizing as MDPs, we allow for much more **variety** than the semantic meaning of a "task"
* Many tasks may share the same reward function, but have the same dynamics and/or state distributions
* E.g. walking on grass and walking on concrete has similar dynamics but are two different tasks
![](../../attachments/Pasted%20image%2020250527140844.png)

In each task, the task parameters might vary depending on the situation, but many can still be **shared.**
![](../../attachments/Pasted%20image%2020250527141126.png)

## 1.2 Identifying Tasks
In this formulation, there is no way to tell our model what task it should be performing ⟶ use **task identifiers**
* We can think of this task identifer as **part of the state**
* Instead of having multiple MDPs per task, we can construct **a single MDP** that covers all of the tasks
![](../../attachments/Pasted%20image%2020250527141229.png)
![](../../attachments/Pasted%20image%2020250527141240.png)

If $p_i$ and $r_i$ are the dynamics and rewards for task $i$, we can model $p$ and $r$ as the MDPs for **every task**
![](../../attachments/Pasted%20image%2020250527141544.png)

# 2 Goal-Conditioned Learning
Special case of multi-task RL where the task identifier $z_i$ is replaced by a target **goal state** $s_g$. In this framework:
* Each task is defined by reaching a specific goal state $s_g$
* The reward function is defined as:
	* For discrete state spaces: $r(s, a, s_g) = \delta(s = s_g)$
	* For continuous state spaces: $r(s, a, s_g) = \delta(||s - s_g|| \leq \epsilon)$
**Advantages:**
* Self-supervised nature ⟶ no explicit reward function design needed
* Versatility ⟶ many real-world tasks can be naturally formulated as goal-reaching problems
* Transfer learning potential ⟶ skills learned for one goal can help with reaching similar goals
**Key challenges:**
* Training complexity ⟶ learning to reach arbitrary goals can be difficult
* Sparse rewards ⟶ the binary nature of goal-reaching rewards can make learning challenging
* Exploration ⟶ finding paths to distant goals requires effective exploration strategies

# 3 Goal of Multi-Task RL
![](../../attachments/Pasted%20image%2020250527141631.png)
* Naively, we can apply standard RL algorithms to each task separately and treat it like an MDP to train it
* In some cases, you can **leverage the shared structure** of the tasks to do better!

# 4 Multi-task Imitation Learning
If the number of tasks is relatively small, you can think of it as:
1. We have some demonstration trajectories $D_i$ for each task $i \in \{1, \dots, N\}$
2. Recall that in imitation learning, we were maximizing the log probability of expert actions given states
	* In gradient descent terminology, we minimize the negative log probability
3. If we write this as minimizing some supervised loss over all demonstrations $D$, we can decompose it as the sum of supervised losses for demonstrations from each task $D_i$
![](../../attachments/Pasted%20image%2020250527142359.png)

**Stratified Sampling**
* Suppose you had tasks that went against each other (e.g. one telling you to go forward, another to lie down)
* If your sampled minibatch had mostly only one task ⟶ high variance in the gradients across timesteps
* Solution: when sampling, stratify the minibatch to ensure that each task is well represented in the minibatch

## 4.1 Conditioning on the Task
Now that we have a multi-task policy, we need to **condition on the task** to get the right policy for the task
* Our policy takes in state $s$ and task descriptor $z_i$ and outputs actions $a$ for that task

### 4.1.1 Multimodal Task Descriptors
In this example, there is actually **multimodal task descriptors** as conditionining to the model
* There is the human demo video as well as the language encoder with the task prompt
![](../../attachments/Pasted%20image%2020250527143558.png)
![](../../attachments/Pasted%20image%2020250527143604.png)

### 4.1.2 OpenVLA
![](../../attachments/Pasted%20image%2020250527143745.png)

## 4.2 Examples
![](../../attachments/Pasted%20image%2020250527143915.png)
![](../../attachments/Pasted%20image%2020250527143924.png)

# 5 Multi-Task RL Algorithms
Multi-Task RL is pretty much the same as before, except:
* we additionally condition our policy and Q-functions on the task identifer
* we may have a per-task replay buffer for stratified sampling
![](../../attachments/Pasted%20image%2020250527144519.png)

## 5.1 Relabeling
If we wanted to **share data between tasks**, we would need some form of **relabeling** between tasks
![](../../attachments/Pasted%20image%2020250527144557.png)
![](../../attachments/Pasted%20image%2020250527144430.png)
## 5.2 Multi-task RL with Relabeling
**Task-Specific Replay Buffers**
0. Pick a task $i$ and get task identifier $z_i$
1. Collect data using our policy $\pi_\theta(a \mid s, z_i)$
2. Store our data $(s_t, a_t, s_{t+1}, r_t, z_i)$ in our *task-specific replay buffer* $D_i$
3. Relabel that same data for task $j$ as $(s_t, a_t, s_{t+1}, r_j(s_t, a_t), z_j)$ and store in $D_j$
	* This allows us to reuse experiences for task $i$ to learn another task $j$
	* Is this experience useful? If $r_j$ is high for that example it might be good
4. Update $\pi_\theta$ and go back to step 1

**Unified Replay Buffers**
![](../../attachments/Pasted%20image%2020250527145309.png)

**When can we apply relabeling?**
* When we have a shared state and action space between tasks (so that replay buffer matches)
* When the dynamics are consistent across goals/tasks
	* We are not relabeling the dynamics $s_{t+1} \mid s_t, a_t$ ⟶ the dynamics should be consistent across tasks
	* E.g. initial state distribution $p_i (s_1)$ are similar for different tasks $i$
* When the reward function form is known and can be evaluated
* When we are using an **off-policy** algorithm
	* This is related to the consistent dynamics: our data is from policy conditioned on $z_i$ ⟶ $\pi(\cdot \mid s_t, z_i)$
	* When conditioned on $z_j$ this might be a diff distribution, so the relabeled data is off-policy
	* We should be using an off-policy algorithm to update $\pi_\theta$

## 5.3 Goal-conditioned RL with Relabeling
Basically the same as before, but we don't need an explicit reward function ⟶ reward is just negative distance between states $r_t' = -d(s_t, s_T)$
* There are various relabeling strategies for the goal-conditioned setting
* This can alleviate exploration challenges
![](../../attachments/Pasted%20image%2020250527144752.png)
![](../../attachments/Pasted%20image%2020250527144819.png)
