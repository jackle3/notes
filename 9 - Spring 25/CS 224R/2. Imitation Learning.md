
# 1 Recap
![](../../attachments/Pasted%20image%2020250404103813.png)

# 2 Imitation Learning
> [!NOTE] Given trajectories collected by exper demonstrations, learn a policy that performs at the level of the export policy by mimicking it.
> ![](../../attachments/Pasted%20image%2020250404103919.png)

# 3 Behavior Cloning: Learning Expressive Policy Distributions
## 3.1 Version 0: Deterministic Policy
![](../../attachments/Pasted%20image%2020250404112804.png)

### 3.1.1 What's Wrong with This?
* The equation above is L2 regression: minimizing the square error between the expert's action and the predicted action
* Suppose some experts go left and some experts go right. The learned policy will take the mean, which is a straight line in the middle.
![](../../attachments/Pasted%20image%2020250404104455.png)

## 3.2 Learning Distributions with NNs
* Neural networks can be used to learn probability distributions over actions
* For **discrete actions**:
	* NN outputs probabilities for each action based on state: $p(\text{up}), p(\text{down}), …$
		* Example: Video game controls (up, down, left, right)
	* Represents a categorical distribution ⟶ maximally expressive for discrete spaces
* For **continuous actions**:
	* Neural net outputs parameters of a distribution (e.g., $\mu, \sigma$ for Gaussian)
		* Example: Steering angle in driving
	* Simple Gaussian distributions are not very expressive for complex behaviors
![](../../attachments/Pasted%20image%2020250404105645.png)

## 3.3 Generative Models for Policies
* **Generative modeling approaches** can also be used:
	* Image diffusion models: learning $p(\text{image}|\text{text description})$
	* Autoregressive models: learning $p(\text{next word}|\text{words so far})$
* Our goal in imitation learning is similar: learn $p(\text{action}|\text{observations})$
![](../../attachments/Pasted%20image%2020250404110055.png)

### 3.3.1 Mixture of Gaussians
* Combine multiple Gaussian components
* NN outputs parameters for each Gaussian: $\mu_1, \sigma_1, w_1, \mu_2, \sigma_2, w_2, …$
* More expressive than a single Gaussian, can capture multi-modal behavior
* Good for continuous action spaces with multiple valid solutions

### 3.3.2 Discretize + Autoregressive
* Discretizes continuous action space into bins.
* Models action probability autoregressively ⟶ similar to an LLM
	* Example: when you are driving, the actions are (steering angle, acceleration, …)
		1. Suppose steering angles were a continuous variable. Discretize it into bins to produce a discrete distribution $P(\text{steering angle}) = P(a_{t, 1})$.
		2. Sample a steering angle $\hat{a}_{t, 1} \sim P(a_{t, 1})$.
		3. Condition this to create a distribution over acceleration (the next action) $P(a_{t, 2} | \hat{a}_{t, 1})$.
		4. Sample an acceleration $\hat{a}_{t, 2} \sim P(a_{t, 2} | \hat{a}_{t, 1})$.
		5. Repeat this process for all actions.
* Neural network outputs probabilities for each discretized action component
* Can capture complex dependencies between action dimensions

### 3.3.3 Diffusion Models (over Actions, not Images)
* Iteratively refines noisy actions into precise ones
* Network outputs noise prediction $\epsilon_n$ at each denoising step
* Particularly useful for high-dimensional action spaces
* Leverages the same techniques that have been successful in image generation

## 3.4 Version 1: Expressive Policies
* These models can mimic expert behaviors much better than deterministic policies
![](../../attachments/Pasted%20image%2020250404112221.png)

* Most modern RL systems use imitation learning with expressive systems!
![](../../attachments/Pasted%20image%2020250404112623.png)
![](../../attachments/Pasted%20image%2020250404112702.png)
![](../../attachments/Pasted%20image%2020250404112710.png)

## 3.5 Summary so Far
* If you have data from:
	* one consistent expert, unimodal policy distribution (e.g. deterministic) is good enough
	* multiple experts, you need expressive generative models for your policy

* **Offline vs Online learning:**
	* **Offline**: using only an existing dataset, no new data from learned policy
	* **Online**: using new data from learned policy

* **Key Idea**: train expressive policy class via generative modeling on dataset of demonstrations
	* **Pro**: no need for data from policy (alg is fully offline ⟶ online data can be unsafe or expensive)
	* **Pro**: no need to define an explicit reward function
	* **Con**: may need a lot of data for reliable performance

# 4 DAgger: Learning from Online Interventions
## 4.1 Compounding Errors
* Imitation learning suffers from **compounding errors**
	* Standard supervised learning: inputs are independent of predicted labels $\hat{y}$
	* Imitation learning:
		* Predicted actions affect the next state
		* Errors can lead to drift away from the data distribution ⟶ if policy drifts to a state with no data, model can't really correct itself
* **Covariate shift**: $p_{expert}(s) \neq p_π(s)$
	* Policy makes errors → states visited by expert not equal to states visited by policy
	* Makes larger errors in unfamiliar states ⟶ errors compound, potentially causing failure
![](../../attachments/Pasted%20image%2020250404113648.png)

## 4.2 Solutions
1. Collect a lot of demo data and hope for the best
	* This is still an offline model ⟶ hoping that expert covers unseen state space

2. Collect **corrective behavior data** ⟶ DAgger (Dataset Aggregation)
	* This makes the model online:
		1. Train initial policy on expert demonstrations
		2. Roll out the learned policy (which may make mistakes and visit new states)
		3. Query the expert for correct actions at these policy-visited states
		4. Aggregate these corrections with the original dataset
		5. Retrain policy on the combined dataset and repeat
![](../../attachments/Pasted%20image%2020250404114101.png)

3. Collect **corrective behavior data** but give the **expert full control**
	* This approach is sometimes called "human-gated DAgger"
	* Advantages:
		* Much more practical interface for providing corrections
		* Easy to catch mistakes quickly in some application domains
		* Example: human driver in self driving car steps in to take control and corrects the policy
![](../../attachments/Pasted%20image%2020250404114830.png)

# 5 How to Collect Demonstrations
![](../../attachments/Pasted%20image%2020250404115046.png)
![](../../attachments/Pasted%20image%2020250404115057.png)

# 6 Summary
![](../../attachments/Pasted%20image%2020250404115110.png)
