# 1 High-Level Algorithm Recap
**Online** ⟶ scenarios where current policy is used to collect more data
* **Off-policy RL** ⟶ can reuse data from other policies
	* **Multiple gradient steps** ⟶ use data from one policy in multiple gradient steps
	* **Replay buffer** ⟶ even more off-policy, use a replay buffer to get data from very diverse policies
* **On-policy RL** ⟶ collect and only use data from the current policy being trained

**Offline** ⟶ static dataset, and we can't do any additional data collection in the environment
* **Contraints to actions in data** ⟶ prevent out-of-distribution actions
* **Implicitly constrain via conservatism** ⟶ minimize Q-values of out-of-distribution actions
![](../../attachments/Pasted%20image%2020250527105536.png)

# 2 MBRL: Key Idea
The key idea is **can we learn a simulator?** ⟶ predicting the future given current state and actions
* Think world models, where we represent the world as a latent state, and we can predict the future given the current state and actions
![](../../attachments/Pasted%20image%2020250527110214.png)
![](../../attachments/Pasted%20image%2020250527110245.png)

# 3 How to Learn a Dynamics Model?
![](../../attachments/Pasted%20image%2020250527110437.png)

If our state space is image observations (e.g. location of cars):
* One option is to generate entire future states $s_{t+1}$ conditioned on $s_t$ and $a_t$
![|350](../../attachments/Pasted%20image%2020250527111111.png)
* Another option is to learn a lower-dim latent representation (autoencoder), then learn model over latents
![|500](../../attachments/Pasted%20image%2020250527111154.png)

During learning, we are optimizing to learn the best possible distribution $P(s_{t+1} \mid s_t, a_t)$
* If we have more deterministic dynamics, this is how well we can minimize error $\min \sum || \hat{s}_{t+1} - s_{t+1}||^2$

# 4 Planning with Learned Dynamics Model
## 4.1 Approach 1A: Backprop
If we trained **both** a reward model and a dynamics model, we can **directly backprop** from rewards into the actions ⟶ objective is to determine action that maximizes future sum of rewards
* i.e. we **plan** a sequence of actions that maximizes rewards
$$
\hat{a}_{t:t+H} \leftarrow \arg\max \sum_{t'=t}^{t+H}r_t'
$$
![](../../attachments/Pasted%20image%2020250527111314.png)
To find these actions, the steps are:
1. Initialize $a_{t:t+H}$ to be random actions
2. Compute gradient $\nabla_{\hat{a}_{t:t+H}} \sum_{t'=t}^{t+H}r_t' = \frac{\partial r_t'}{\partial s_{t:t+H}} \frac{\partial s_{t:t+H}}{\partial \hat{a}_{t:t+H}}$
3. Do gradient descent $\hat{a}_{t:t+H} \leftarrow \hat{a}_{t:t+H} - \alpha \nabla_{\hat{a}_{t:t+H}} \sum_{t'=t}^{t+H}r_t'$
4. Execute $\hat{a}_{t:t+H}$ and observe $s_{t+1}$ and repeat steps 2-4 until convergence

## 4.2 Gradient Vs Sampling Optimization
* With gradient optimization, we generally start at a random location and use the gradient to **gradually move our actions** to maximize the total sum of rewards
* With sampling methods, we sample a **bunch of actions in parallel** and see what the total sum of rewards are (no need for gradients any more)
	* **Cross entropy method**: sample from distribution, pick samples with best loss, fit Gaussian distribution to best samples and sample more around the **elite samples** in the next iteration
![](../../attachments/Pasted%20image%2020250527110713.png)
![](../../attachments/Pasted%20image%2020250527112712.png)

## 4.3 Approach 1B: Sampling
Another method to optimize this is to use sampling ⟶ gradient free optimizations
![](../../attachments/Pasted%20image%2020250527110758.png)
* The two main methods to sampling actions are **random shooting** and **iterative CEM**
	* CEM has a better distribituion to sample from ⟶ we iteratively refine our sampling space
![](../../attachments/Pasted%20image%2020250527110746.png)

**Using the version so far:**
* Suppose your reward is based on how you go
* If we use this method so far, we will learn that going right will get us higher on the cliff
* **Issue:** data collection policy does not match the policy we are using to take actions
![](../../attachments/Pasted%20image%2020250527113150.png)

**Simple fix:** execute the planned actions from $\pi_f$ and add that to our dataset to learn a better dynamics model
![](../../attachments/Pasted%20image%2020250527113156.png)
* The data will now show that if we go past the edge of the cliff, we will fall off ⟶ this allows us to learn a model to stop at the top of the cliff
![](../../attachments/Pasted%20image%2020250527113206.png)

## 4.4 Approach 2: Model-Predictive Control
* So far, we've had an **open loop** ⟶ we plan an entire horizon $a_{1:H}$ and then execute it
	* This would be pretty bad when the environment changes a lot (e.g. driving on the road)
* An alternative is **closed loop** ⟶ plan the horizon, but only execute $a_t$ and get back $s_{t+1}$, then replan
	* This allows us to correct for model errors in the environment
![](../../attachments/Pasted%20image%2020250527113907.png)
![](../../attachments/Pasted%20image%2020250527114042.png)

## 4.5 Summary
![](../../attachments/Pasted%20image%2020250527114131.png)

# 5 Data Generation with Learned Dynamics
We can also use this learned dynamics model to **learn a policy**
## 5.1 Distillation
We can train a policy to **match actions taken by the planner**
* Recall that planner has a short horizon. How to fix this?
	1. Plan with a terminal value function ⟶ $\sum_{t'=t}^{t+H} r_t' + V(s_{t+H})$
		* We still plan for a short horizon $H$, then we extend that with the value of the terminal state
	2. Augment model-free RL methods using additional data collected by the model
![](../../attachments/Pasted%20image%2020250527114610.png)

## 5.2 Augmented Data
**Key Idea:** augment our dataset with model-simulated rollouts ⟶ generate short horizon trajectories starting from states in real data
* This allows us to get better coverage of the environment using both real and augmented data
* Unlike planning, this is **not more compute intensive** at test time ⟶ we augment data during training, but just use learned policy during test time rollout
![](../../attachments/Pasted%20image%2020250527135219.png)
**Full Algorithm**
1. Collect real data using current policy $\pi_\theta$ and add it to $D_\text{env}$
2. Update learned dynamics model $p_\psi(s' \mid s, a)$ using the real data $D_\text{env}$
3. Generate synthethic roll-outs
	1. Sample $s$ from real data $D_\text{env}$
	2. Roll out $\pi_\theta$ starting from $s$ inside our learned model of the world $p_\psi$ ⟶ add to $D_\text{model}$
4. Update policy $\pi_\theta$ using both real and generaed data $D_\text{env} \cup D_\text{model}$
![](../../attachments/Pasted%20image%2020250527135305.png)

# 6 When to Use Model-based RL?
![](../../attachments/Pasted%20image%2020250527135803.png)
Some new hyperparams include:
* all the regular ML hyperparams for training a model $p_\psi$ (e.g. LR, architecture, etc)
* the length of the synthethic rollouts, etc

## 6.1 Other Kinds of Models
![](../../attachments/Pasted%20image%2020250527140342.png)

# 7 Case Study in Robotics
This is a case study of **using planning** for dexterous manipulation
![](../../attachments/Pasted%20image%2020250527115053.png)
The model of the world is an ensemble of 3 neural networks ⟶ minimizes chance of errors
![](../../attachments/Pasted%20image%2020250527115103.png)
![](../../attachments/Pasted%20image%2020250527115243.png)
![](../../attachments/Pasted%20image%2020250527115326.png)
![](../../attachments/Pasted%20image%2020250527115335.png)
