# 1 Recap
## 1.1 Policy Gradients
**Key Idea:** Improve policy by directly adjusting its parameters based on **observed trajectories**.
* Optimizes the policy in the direction that increases expected rewards, leveraging the actual outcomes from the agent's interactions with the environment.
![](../../attachments/Pasted%20image%2020250421200146.png)

## 1.2 Value Functions
**Key Idea:** Value and Q-value functions provide **estimates of expected future rewards** by averaging over **all possible trajectories.**
* Reduces the variance compared to relying on single trajectory rewards ⟶ more stable learning
![](../../attachments/Pasted%20image%2020250421200150.png)

## 1.3 On-Policy Actor-Critic Methods
**Key Idea:** Actor-critic methods integrate policy and value-based strategies.
* Actor component selects actions to collect data (similar to policy gradient)
* Critic evaluates these actions, providing feedback that enhances learning and policy improvement.
	* Instead of directly using observed trajectories as reward, try to learn value functions
* This is on-policy because the policy that is being evaluated and improved is the same policy that is used to generate the data.
	* Actor uses the current policy to interact with the environment and collect trajectories
	* Critic evaluates these trajectories to provide feedback for updating the same policy
![](../../attachments/Pasted%20image%2020250421200141.png)

### 1.3.1 Steps
![](../../attachments/Pasted%20image%2020250421201946.png)

# 2 Off-Policy Actor-Critic Methods
So far ⟶ use one batch of policy data for **one gradient step** (fully on-policy)
Goal now ⟶ use one batch of policy data for **multiple gradient steps** (starting to be off-policy)
## 2.1 Multiple Gradient Steps
The most straightforward off-policy method is to take multiple gradient steps on the same batch of data.
* We use importance sampling to adjust the gradient for the different policies
* Weight each gradient by the ratio of $\frac{\text{new policy's probability of taking the action}}{\text{old policy's probability of taking the action}}$
![](../../attachments/Pasted%20image%2020250421202348.png)

## 2.2 Surrogate Objective
When using data from an old policy to learn a new policy, we need to approximate objective function $J(\theta')$.
* Importance sampling adjusts for the differences between the behavior policy and the target policy.

To derive the surrogate objective, the convenient identity states that:
$$
\pi_{\theta'}(a_t \mid s_t) \nabla_{\theta'} \log \pi_{\theta'}(a_t \mid s_t) = \pi_{\theta'}(a_t \mid s_t) \frac{\nabla_{\theta'} \pi_{\theta'}(a_t \mid s_t)}{\pi_{\theta'}(a_t \mid s_t)} = \nabla_{\theta'} \pi_{\theta'}(a_t \mid s_t)
$$
This indicates that the gradient above is equivalent to:
$$
\begin{aligned}
\nabla_{\theta'} J(\theta') &\approx \sum_{t, i} \frac{\pi_{\theta'}(a_t \mid s_t)}{\pi_{\theta}(a_t \mid s_t)} \nabla_{\theta'} \log \pi_{\theta'}(a_t \mid s_t) A^{\pi_{\theta}}(s_t, a_t) \\
&= \sum_{t, i} \frac{\nabla_{\theta'} \pi_{\theta'}(a_t \mid s_t)}{\pi_{\theta}(a_t \mid s_t)} A^{\pi_{\theta}}(s_t, a_t)
\end{aligned}
$$
Thus, the surrogate objective is:
$$
\tilde{J}(\theta') \approx \sum_{t, i} \frac{\pi_{\theta'}(a_t \mid s_t)}{\pi_{\theta}(a_t \mid s_t)} A^{\pi_{\theta}}(s_t, a_t)
$$

## 2.3 Problem: Too Many Steps
When optimizing, the goal is to maximize the surrogate objective:
* The policy can only control $\theta'$.
* $A^{\pi_{\theta}}(s_t, a_t)$ and $\pi_{\theta}(a_t \mid s_t)$ are based on the old parameters $\theta$.
* To maximize the objective, the policy will aim to increase $\pi_{\theta'}(a_t \mid s_t)$ for actions with high advantages.
	* This means the policy is **incentivized to differ significantly** from old policy.

**Problem:** Taking too many gradient steps can cause significant changes in the policy, making the advantages outdated. This can lead to overfitting on the old policy and degrade performance, resulting in instability.

**Solution 1: Use KL constraint on policy**
* Add constraint to objective, similar to including regularization in supervised learning
	* Common approach, seen in LLM preference optimization
	* Maintains stability by preventing drastic policy changes
$$
\tilde{J}(\theta')_\text{constrained} \approx \tilde{J}(\theta') - \beta \cdot \text{KL}(\pi_{\theta}(\cdot \mid s) \| \pi_{\theta'}(\cdot \mid s))
$$
* Ensures that the divergence between the new and old policies remains within a threshold $\delta$:
$$
\mathbb{E}_{s \sim \rho} [D_{KL}(\pi_{\theta}(\cdot \mid s) \| \pi_{\theta'}(\cdot \mid s))] \leq \delta
$$

**Solution 2: Can we bound the importance weights?**
* This doesn't directly constrain the policy's objective function but removes incentives for large updates
* Key idea behind proximal policy optimization (PPO) ⟶ stable learning by bounding changes in policy

# 3 Proximal Policy Optimization (PPO)
**Key idea:** to stably take more gradient steps, keep new policy close to the old policy
* Ensures that the advantage estimates (from the old policy) remain valid
* Similar to the off-policy actor-critic method with a few modifications to the surrogate objective.
![](../../attachments/Pasted%20image%2020250421223215.png)
![](../../attachments/Pasted%20image%2020250421223222.png)
![](../../attachments/Pasted%20image%2020250421223229.png)
![](../../attachments/Pasted%20image%2020250421223836.png)

## 3.1 Algorithm
![](../../attachments/Pasted%20image%2020250421224842.png)
* With a batch size of 64 and 2000 timesteps collected, we can take $\frac{2000}{64} \approx 30$ gradient steps per epoch
* Since we want 10 epochs of updates, we run for $30 \times 10 = 300$ gradient steps

# 4 Replay Buffer
**Key Idea:** maintain a buffer of all the past trial-and-error data, learn from it multiple times
* Even more off policy ⟶ can we reuse data from previous batches to update in the current batch?

Notice that we do not take multiple gradient steps after collecting each batch of data
* We collect data from the environment and store it in a replay buffer
* We then sample a batch of data from the replay buffer and use it to update the policy
* This allows us to learn from the same data multiple times, which is more sample efficient
![](../../attachments/Pasted%20image%2020250421225434.png)

## 4.1 Problem: Value Function
**Problem**: it is not right to fit $V^\pi$ on all the data in the replay buffer
* All the data is collected from different policies. It does not make sense to fit a value function $V^\pi$ for the current policy $\pi$ using data from older policies
![](../../attachments/Pasted%20image%2020250421225924.png)
* The value function $V^\pi(s)$ represents the expected return when following policy $\pi$ from state $s$:
$$
V^\pi(s) = \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \mid s_0 = s \right]
$$
* The transitions $(s_t, a_t, r_t, s_{t+1})$ in the replay buffer are collected under different policies $\pi_{\text{old}}$
* The bootstrapping target becomes incorrect:
	* Target assumes that future actions will be taken according to the current policy $\pi$, but the data was collected using different policies
$$
V^\pi(s_t) \approx y_t = r_t + \gamma V^\pi(s_{t+1})
$$

## 4.2 Solution: Q-Value Function
**Goal:** how do we fit a value function $\pi_\theta$ using a replay buffer of data from past policies?

Our data points are $(s_t, a_t, r_t, s_{t+1})$ and the future reward, both collected under an old policy
* We do not want to use the future reward as the target because it is collected under an old policy
* We can pass the action from the past policy as input to fit $Q(s, a)$
![](../../attachments/Pasted%20image%2020250421230538.png)
![](../../attachments/Pasted%20image%2020250421230824.png)

### 4.2.1 **Approach**
1. Sample $(s_i, a_i, s_i')$ from the replay buffer
2. Sample an action from our current policy $\bar{a}_i' \sim \pi_{\theta}(\cdot \mid s_i')$
3. Fit estimate of the Q-value function using the sampled data
$$
Q^{\pi_\theta}(s_i, a_i) \approx r_i + \gamma Q^{\pi_\theta}(s_i', \bar{a}_i')
$$
![](../../attachments/Pasted%20image%2020250421231158.png)

### 4.2.2 **Why Does This work?**
* Since we are sampling $\bar{a}_i'$ from the current policy, we are getting an estimate for the current policy's Q-value function
* We can use $y_i = r(s_i, a_i) + \gamma Q^{\pi_\theta}(s_i', \bar{a}_i')$ as our target for the supervised learning problem to learn the Q-value function

![](../../attachments/Pasted%20image%2020250421233821.png)

### 4.2.3 Advantange
* Since we learnt the Q-value function instead of the value function, we can no longer use the advantage as our "reward".
* We can instead just use the Q-value function, might actually be better since current policy's actions are probably better than the past policy's actions
![](../../attachments/Pasted%20image%2020250421233930.png)

## 4.3 Performance
SAC is the off-policy actor-critic method that uses a replay buffer. PPO is the same as before, slightly off-policy.
![](../../attachments/Pasted%20image%2020250421234158.png)
