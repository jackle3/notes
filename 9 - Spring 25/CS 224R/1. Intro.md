# 1 Logistics
## 1.1 Coursework
![](../../attachments/Pasted%20image%2020250402200645.png)

## 1.2 Grading
![](../../attachments/Pasted%20image%2020250402200542.png)

# 2 What is Deep RL?
* RL is sequential decision making problems and the solutions to such problems ⟶ system needs to make **multiple** decisions based on stream of information
	* Observe ⟶ take action ⟶ observe ⟶ take action ⟶ …

## 2.1 How Does it Differ from other ML?
![](../../attachments/Pasted%20image%2020250402200327.png)

## 2.2 Why Study Deep RL?
1. Going beyond supervised learning with $(x, y)$ examples
	1. AI model predictions have consequences. How can we take them into account?
	2. When direct supervision isn't available, how to learn from **any objective**?
2. Widely used and deployed for performant AI systems
3. Learning from experience seems **fundamental to intelligence** ⟶ RL can discover new solutions
4. Plenty of exciting open research problems

# 3 Modeling Behavior and RL
## 3.1 How to Represent Experience as Data?
* **state** $s_t$ ⟶ the state of the "world" at time $t$
* **observation** $o_t$ ⟶ what the agent observes at time $t$
	* Different from state in that it may not fully specify the state; only what the agent can observe
	* Only used when state is not fully observable and there is missing information
* **action** $a_t$ ⟶ the action taken by the agent at time $t$
* **trajectory** $\tau$ ⟶ the sequence of states/observations and actions
	* $\tau = (o_1, a_1, o_2, a_2, \dots, o_T, a_T)$ or $(s_1, a_1, s_2, a_2, \dots, s_T, a_T)$
* **reward function** $r(s, a)$ ⟶ how good is $s, a$?
![](../../attachments/Pasted%20image%2020250402202423.png)
### 3.1.1 Examples
![](../../attachments/Pasted%20image%2020250402202521.png)
* Consider autonomous driving:
	* State $s$ ⟶ the full state of the car
	* Observation $o$ ⟶ the camera feed or lidar data (you're missing things like velocity of cars around you, etc.)
	* Action $a$ ⟶ the steering wheel angle and acceleration
	* Trajectory $\tau$ ⟶ for 10 seconds of camera readings at 10 Hz, that's 100 data points of $(o, a)$ pairs
	* Reward $r(s, a)$ ⟶ can have many, including distance to center line, speed limit, etc.

## 3.2 How to Respresent Behavior as a NN?
* Our RL model has two main probability distributions:
	* **policy** $\pi_\theta(a_t \mid s_t)$ ⟶ prob of taking action $a_t$ given state $s_t$
		* If we don't have the full state (i.e. only observations), we can give the policy memory of the past observations to get closer to the full state: $\pi_\theta(a_t \mid o_{t-m}, \dots, o_t)$
	* **world dynamics** $p(s_{t+1} \mid s_t, a_t)$ ⟶ prob of transitioning to state $s_{t+1}$ given current state $s_t$ and action $a_t$
* We can sample from these distributions to get a trajectory $\tau$
![](../../attachments/Pasted%20image%2020250402205236.png)

## 3.3 What is the Goal of RL?
### 3.3.1 Maximize Sum of Rewards
* In a deterministic world, the goal is to maximize the total reward
![](../../attachments/Pasted%20image%2020250402205653.png)

### 3.3.2 Maximize Expected Sum of Rewards
* Since the world is stochastic, we instead maximize the **expected** total reward
	* We like **stochastic** policies because they:
		* allow for **exploration** ⟶ to learn from experience, must try different actions and make mistakes
		* can model **stochastic behavior** ⟶ stochastic policy can model deterministic, not vice versa
		* can leverage tools from **generative models** ⟶ generate actions given states/observations
	* If we want to write down what rewards *look like*, we need to create a probability distribution over trajectories (because the rewards depend on the trajectory)
$$
\underbrace{p(s_1, a_1, \dots, s_T, a_T)}_{p_\theta(\tau)} = \underbrace{p(s_1)}_{\text{initial state}} \cdot \prod_{t=1}^{T} \underbrace{\pi_\theta(a_t \mid s_t)}_{\text{policy}} \cdot \underbrace{p(s_{t+1} \mid s_t, a_t)}_{\text{transition dynamics}}
$$
* To find the maximum expected reward, we find parameters $\theta$ (for policy $\pi_\theta$) that **maximize the expected sum of rewards** under all possible trajectories
$$
\max_\theta \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[ \sum_t^T r(s_t, a_t) \right]
$$
* **Note that $T$ does not have to be fixed ⟶ you can also write it as $|\tau|$**

### 3.3.3 Discounting
* So far, we weight the rewards over the entire trajectory equally
* In practice, we often want to discount future rewards ⟶ we care more about immediate rewards than future rewards
$$
\max_\theta \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[ \sum_t^T \gamma^t \cdot r(s_t, a_t) \right]
$$
* $\gamma \in (0, 1]$ is the discount factor. If $\gamma = 1$, equivalent to before. If $\gamma < 1$, the sum of rewards is finite

### 3.3.4 How Good is a Policy?
* **Value function**: Quantifies how good a policy is from a given state.
	* Measures the expected total reward when following the policy from that state.
* **Q-function**: Evaluates how good a specific action is in a given state.
	* Measures expected total reward when taking that action and then following the policy.
	* Allows us to evaluate actions not prescribed by the policy, which enables exploration.
![](../../attachments/Pasted%20image%2020250402211614.png)

# 4 Types of Algorithms
All of these algorithms aim to maximize the expected sum of rewards.
$$
\max_\theta \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[ \sum_t^T \gamma^t \cdot r(s_t, a_t) \right]
$$
1. **Imitation learning**: mimic a policy that achieves high reward
2. **Policy gradients**: directly differentiate the above objective to find the optimal policy
3. **Actor-critic**: estimate the value of the current policy and use it to make the policy better
4. **Value-based**: estimate value of the optimal policy to figure out a good policy
5. **Model-based**: learn to model the dynamics of the world, and use it for planning or policy improvement

# 5 Recap
* In this lecture, we basically formulated a Markov Decision Process (MDP)
	* If we only have observations, it's called a Partially-Observed Markov Decision Process (POMDP)
![](../../attachments/Pasted%20image%2020250402212735.png)
![](../../attachments/Pasted%20image%2020250402212741.png)
