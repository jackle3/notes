
# 1 Recap: Policy Gradients
**Online RL**
1. Run policy in the environment to collect a new batch of data
2. Use that data to improve the policy
3. Repeat ⟶ improve from practice

**Policy Gradient**: calculate direction of maximum improvement in total rewards
1. Pick samples from our policy, giving us $N$ difference trajectories
2. Evaluate the gradient of the log likelihood of the trajectories
3. This log likelihood is weighted by the rewards and a baseline
Intuitively, we increase probability of trajectories that lead to high rewards, and decrease probability of trajectories that lead to low rewards
![](../../attachments/Pasted%20image%2020250416133513.png)

**Off-policy**
* To evaluate this gradient, we need to run the policy $\pi_\theta$ itself to collect the trajectories
* This means we can only run **one gradient update per batch of data** ⟶ inefficient
* Importance weights: use samples from our old policy and weigh it by a ratio between the new and old policy
![](../../attachments/Pasted%20image%2020250416134432.png)

# 2 Value Functions
**Value Function:** expected sum of rewards of following a policy $\pi$ given start in state $s_t$
* The value function is effectively the expectation over all actions of the Q-function
$$
V^\pi(s_t) = \sum_{t' = t}^{T} \mathbb{E}_{s_t', a_t' \sim \pi} \left[ r(s_t', a_t') \mid s_t\right]
$$
**Q-Function:** expected sum of rewards of following a policy $\pi$ given start in state $s_t$ and take action $a_t$
$$
Q^\pi(s_t, a_t) = \sum_{t' = t}^{T} \mathbb{E}_{s_t', a_t' \sim \pi} \left[ r(s_t', a_t') \mid s_t, a_t\right]
$$
![](../../attachments/Pasted%20image%2020250416134652.png)

### 2.1.1 Example
![](../../attachments/Pasted%20image%2020250416135546.png)
* **Value function:** $V^\pi(s_t) = 0$ because the policy always chooses to go to the beach ($a_3$), resulting in zero reward since you don't play the drums
* **Q-function:**
	* $Q^\pi(s_t, a_1) > 0$ for taking the action to play drums
	* $Q^\pi(s_t, a_{\{2, 3\}}) = 0$ for taking the guitar or beach action
* **Advantage:**
	* $A^\pi(s_t, a_1) > 0$ for taking the drums action in Q-function
	* $A^\pi(s_t, a_{\{2, 3\}}) = 0$ for taking the guitar or beach action in Q-function

# 3 Improving Policy Gradients
## 3.1 What's Wrong with Policy Gradients?
![](../../attachments/Pasted%20image%2020250416135954.png)

Consider a trajectory where an agent takes a **small step forward but then falls backwards**
* The total reward is *negative*
* Policy gradient will decrease the likelihood of all actions in this trajectory
	* This includes decreasing the likelihood of taking a step forward
	* But taking a step forward is actually good progress!
* We lack the ability to distinguish which specific actions were good or bad within a trajectory
![](../../attachments/Pasted%20image%2020250416140002.png)

## 3.2 Estimating with Q-function
![](../../attachments/Pasted%20image%2020250416140721.png)
So far, we computed the policy gradient by using the empirical "reward-to-go":
* The $r(s_{i,t'}, a_{i,t'})$ term is *actual observed* rewards at each time step $t'$ in the trajectory
$$
\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_{i,t} | s_{i,t}) \left( \sum_{t'=t}^{T} r(s_{i,t'}, a_{i,t'}) \right)
$$
We can get a better estimate by using the Q-function because it is an expected sum of future rewards over *all possible* trajectories:
$$
\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_{i,t} | s_{i,t}) \; Q(s_{i,t}, a_{i,t})
$$
**Key difference:**
* **Reward-to-go** (original approach): Uses actual rewards from a single trajectory
	* High variance because it depends on one specific outcome
* **Q-function** (improved approach): Uses expected rewards averaged over all possible trajectories
	* Lower variance because it averages over many possible futures

## 3.3 Estimating with Advantage
* Common baseline is the average reward over all trajectories ⟶ this is pretty much the value function
![](../../attachments/Pasted%20image%2020250416141348.png)

# 4 Actor Critic Methods
Actor-critic methods combine policy-based and value-based approaches by using two components:
1. **Actor**: The policy network that decides which actions to take in each state
2. **Critic**: The value function approximator that evaluates how good those actions are ⟶ feedback to actor
![](../../attachments/Pasted%20image%2020250416143723.png)

**Actor-critic methods follow this general process:**
1. The actor selects an action based on the current policy
2. The critic evaluates this action using a learned value function
3. The critic's evaluation is used to update both networks:
	* The actor is updated to select actions that maximize the estimated value ⟶ better policy $\pi_\theta$
	* The critic is updated to better estimate the true value function ⟶ better $V^\pi, Q^\pi, A^\pi$

## 4.1 Policy Evaluation: Estimating Expected Return
We want to estimate the expected return $V^\pi$ for a given policy $\pi$
* Looking at the equation, $A^\pi(s_t, a_t) = r(s_t, a_t) + \mathbb{E}_{s_{t+1} \sim p(s_{t+1} \mid s_t, a_t)} [V^\pi(s_{t+1})] - V^\pi(s_t)$
* However, *we don't know the transition probabilities* $p(s_{t+1} \mid s_t, a_t)$
* But when we ran our policy to collect data, we did sample a single $s_{t+1}$ from the transitions
* We can approximate the expected return by using the sampled $s_{t+1}$

The goal then is to create an NN that takes in a state $s_t$ and outputs an estimate of $V^\pi(s_t)$
![](../../attachments/Pasted%20image%2020250416143926.png)

### 4.1.1 Monte Carlo Estimation
* Ideally, we would reset the world to some initial state and run the policy to get the total return.
* This is not feasible in practice. Instead, we use single-sample estimates from our batch of trajectories
	1. Run policy $\pi$ to collect $N$ trajectories (this is the Monte Carlo part)
	2. Aggregate all of the single sample estimates from all trajectories $i$ starting from $s_{i, t}$
		* num of points in dataset is the num of trajectories $N$ times the avg length of trajectories $T$
	3. Supervised learning to fit an estimate of $V^\pi$
![](../../attachments/Pasted%20image%2020250416144604.png)

### 4.1.2 Bootstrapping (TD Learning)
* Bootstrapping: using existing value estimates to update our value function instead of waiting for complete trajectories
	* In Monte Carlo, we need to wait until the end of an episode to get the true return
* With bootstrapping, we can update our value estimates incrementally using:
	* The immediate reward $r(s_t, a_t)$
	* The estimated value of the next state $V^\pi(s_{t+1})$
* This creates a recursive relationship: $V^π(s_t) \approx y_t = r(s_t, a_t) + V_\phi^π(s_{t+1})$
	* This is a better estimate than Monte Carlo because it considers all the possible future rewards in the next state (i.e. the value function) as opposed to just the single sampled trajectory
* Also known as **TD learning** because it uses the difference between consecutive time steps
![](../../attachments/Pasted%20image%2020250416145133.png)

### 4.1.3 Monte Carlo vs. Bootstrapping
![](../../attachments/Pasted%20image%2020250416145730.png)
![](../../attachments/Pasted%20image%2020250416145950.png)
In the example above:
* $\hat{V}^\pi_\text{MC}(s_\text{blue}) = 0$ ⟶ it will average the rewards of the two trajectories (one with +1 and one with -1)
* $\hat{V}^\pi_\text{TD}(s_\text{blue}) = 0$ ⟶ it bootstraps from both possible next states, averaging their values
* $\hat{V}^\pi_\text{MC}(s_\text{pink}) = -1$ ⟶ the only trajectory from this state leads to a reward of -1
* $\hat{V}^\pi_\text{TD}(s_\text{pink}) = 0 + \hat{V}^\pi(s_\text{blue}) = 0$ ⟶ uses the immediate reward plus the estimated value of future state

### 4.1.4 N-step Returns
* Monte Carlo is high variance but unbiased: only uses the actual rewards from the trajectories
* TD is low variance but can be incorrect: estimated value of future state will have some error in it
* **Middle-ground: N-step returns**
	* Sum of rewards from time $t$ to $t+N$ plus the estimated value of the state $t+N$
$$
V^\pi(s_t) \approx y_t = \sum_{t'=t}^{t+n-1} r(s_{t'}, a_{t'}) + V^\pi(s_{t+n})
$$
![](../../attachments/Pasted%20image%2020250416150613.png)

## 4.2 Discount Factors
We might not want to blindly sum up all of the rewards in the future
* For infinite length rewards, the estimate of the value can get very large!
* We discount future rewards to put more weight on immediate rewards

This is equivalent to saying that with probability $1 - \gamma$, the agent will stop the episode
![](../../attachments/Pasted%20image%2020250416150952.png)

## 4.3 Full Actor-Critic Algorithm
![](../../attachments/Pasted%20image%2020250416151106.png)

# 5 Summary
![](../../attachments/Pasted%20image%2020250416151150.png)
