
# 1 Recap
## 1.1 Value Functions
![](../../attachments/Pasted%20image%2020250427140646.png)
## 1.2 Policy Gradient and Actor-Critic
![](../../attachments/Pasted%20image%2020250427140710.png)
## 1.3 Off-Policy Policy Evaluation
![](../../attachments/Pasted%20image%2020250427140757.png)

# 2 Q-Learning
## 2.1 Thought Exercise
We can treat the policy as a greedy algorithm that always maximizes the Q-value at each step.
![](../../attachments/Pasted%20image%2020250427141133.png)
* The original policy is to simply move right in all states
* The new policy will take the action with the maximum Q-value in that state
	* The new policy is better than the original, but it is not the optimal because it doesn't go up more from the bottom row.
	* **Idea:** if we iterate this a couple times (new policy becomes current, learn a newer one), we can eventually get to the optimal policy
![](../../attachments/Pasted%20image%2020250427141547.png)

## 2.2 Critic-Only
Assuming that $Q^\pi$ is accurately, taking $\arg \max_{a_t} Q^\pi$ is *at least as good* as the original policy
![](../../attachments/Pasted%20image%2020250427142020.png)
**Key Idea:** instead of taking the policy gradient (i.e. learning a NN to update the policy), just greedily update the policy based on the one that maximizes Q-values.
![](../../attachments/Pasted%20image%2020250427142057.png)

### 2.2.1 Policy Iteration
![](../../attachments/Pasted%20image%2020250427142252.png)
Policy iteration alternates between:
1. **Policy evaluation**: Compute $Q^\pi$ for current policy $\pi$
2. **Policy improvement**: Update policy to $\pi'(s) = \arg\max_a Q^\pi(s,a)$
3. Repeat until convergence
This process is guaranteed to converge to the optimal policy.

### 2.2.2 Q-Learning
![](../../attachments/Pasted%20image%2020250427142333.png)
Q-Learning simplifies policy iteration by:
* Skipping the explicit policy representation entirely
	* Learning the **optimal Q-function directly** ⟶ targets $y_i$ are the Q-values for the new policy
	* Making greedy deterministic decisions based on current Q-values during execution
* Using **off-policy learning** (can learn from any exploration data)
	* Allowing continuous improvement without alternating between evaluation and improvement phases
* It is an **online algorithm** because it collects data into a replay buffer as it interacts with the environment
![](../../attachments/Pasted%20image%2020250427172417.png)
![](../../attachments/Pasted%20image%2020250427172605.png)

## 2.3 Collecting Data
In order for our Q-value estimates to be accurate, we need to collect data over many state-action pairs.
* Since Q-learning works off-policy, it may be good to collect data from an exploration policy
![](../../attachments/Pasted%20image%2020250427172651.png)

## 2.4 Summary
![](../../attachments/Pasted%20image%2020250427172708.png)

# 3 Target Networks: Stable Q-Learning
Recall that so far, the target Q-values **change during training** ⟶ may lead to instability
![](../../attachments/Pasted%20image%2020250427173359.png)
**Solution:** supervised learning on the critic (Q-learning $\phi$) is done with a *frozen target*
1. Save the target network parameters $\phi'$ and don't update them during supervised learning
2. Run supervised learning on the Q-network parameters $\phi$ with the frozen targets
3. Periodically update the target network parameters (e.g. via an exponential moving average)
![](../../attachments/Pasted%20image%2020250427174029.png)
In HW2, we implemented this frozen target with actor-critic!
* Similar to actor critic algorithm from before, but the critic learns with frozen targets

# 4 Are the Q-values Accurate?
**Answer:** yes they are! They closely match the average reward, and they often show the most ideal action.
![](../../attachments/Pasted%20image%2020250427174416.png)
* Note: during training, the loss of the critic network will often increase as you continue to train
	* This is okay! As the policy improves, the collected data has higher Q-values, so the scale of the loss function will increase
	* We see this in HW2! The actor loss decreases, reward increases, but the critic loss increases

## 4.1 Overestimation in Q-Learning
As seen in the graphs below, the Q-values often overestimate how well the policy is actually doing.
![](../../attachments/Pasted%20image%2020250427174504.png)
![](../../attachments/Pasted%20image%2020250427175255.png)
Because the action selected according to $Q_\phi$ is the same as the one being evaluated to calculate $Q_\phi$:
* If there is noise in $Q_\phi$, it can be exploited to select actions that have higher value than they actually do because of the noise.

**Solution: Double Q-Learning**
* Use two Q-networks, one to select actions and one to evaluate them
![](../../attachments/Pasted%20image%2020250427175506.png)
![](../../attachments/Pasted%20image%2020250427175528.png)

# 5 N-Step Returns
The target is composed of two parts: the reward and the Q-value estimate
* If $Q_\phi'$ is good, then the estimates are useful for improving the critic network
* If $Q_\phi'$ is bad (e.g. when the network starts), the reward is the main signal to improve the network
![](../../attachments/Pasted%20image%2020250427175735.png)
![](../../attachments/Pasted%20image%2020250427175901.png)

# 6 When to Use Which Online RL Algorithm?
**PPO & Variants:** On-policy algorithm using clipped surrogate objective to prevent large policy changes
* Best for: Stability and ease of use
* Trade-off: Lower data efficiency

**DQN & Variants:** Off-policy Q-learning with experience replay and target networks
* Best for: Discrete or low-dimensional continuous action spaces

**SAC & Variants:** Off-policy actor-critic that learns both a policy and a Q-function
* Best for: Data efficiency
* Trade-off: Requires hyperparameter tuning, less stable
