# 1 Recap
RL problems were formulated based on the following details:
![](../../attachments/Pasted%20image%2020250410135832.png)

**Imitation Learning:** maximize the likelihood of the expert's actions given the current policy
![](../../attachments/Pasted%20image%2020250410135902.png)

# 2 Policy Gradients
This is out first **online RL** algorithm!
1. Initialize a policy
2. Run the policy to collect batch of data (*this is the online part*)
3. Use that data to further improve the policy
![](../../attachments/Pasted%20image%2020250410140007.png)
![](../../attachments/Pasted%20image%2020250410205301.png)

## 2.1 RL Objective
To evaluate the RL objective, we try to find the expected total sum of rewards over all trajectories
* However, we can't just go through every trajectory, because the number of trajectories is infinite!
	* The distribution of trajectories is defined as $p_\theta(\tau)$, which is the probability of a trajectory $\tau$ given the policy $\pi_\theta$
* So we approximate the objective with a sum over a batch of trajectories sampled from the policy $\pi_\theta$
![](../../attachments/Pasted%20image%2020250410140440.png)

## 2.2 Gradient of RL Objective
Our goal is to maximize the expected total sum of rewards:
* Similar to supervised learning, we can take the gradient of the objective with respect to the policy parameters $\theta$
* Note: the expectation here uses the integral because the trajectory space is continuous, so $\tau$ is a continuous random variable
![](../../attachments/Pasted%20image%2020250410140952.png)
![](../../attachments/Pasted%20image%2020250410141001.png)
This final gradient is known as the **vanilla policy gradient**
$$
\begin{align*}
\nabla_\theta J(\theta) &= \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[ \nabla_\theta \log p_\theta(\tau) \cdot r(\tau) \right] \\
&= \int_{\tau} p_\theta(\tau) \left[ \nabla_\theta \log p_\theta(\tau) \right] r(\tau) d\tau \\
&\approx \frac{1}{N} \sum_{i=1}^{N} \left[ \nabla_\theta \log p_\theta(\tau^{(i)}) \right] r(\tau^{(i)})
\end{align*}
$$

### 2.2.1 Estimating the Gradient
Since the number of trajectories is infinite, we can't compute the gradient directly.
Instead, we approximate it:
1. Run the policy to collect/sample a batch of trajectories
2. Use the batch of trajectories to estimate the gradient
3. Use this estimated gradient to update the policy via gradient ascent/descent
**This is the REINFORCE algorithm!**
![](../../attachments/Pasted%20image%2020250410141120.png)

## 2.3 What is the Gradient?
Notice that the first term is the gradient of the log probability of the trajectory $\tau$ under the policy $\pi_\theta$
* This is the **same as the gradient of the imitation learning objective** (i.e. behavior cloning)
* The second term is the reward function $r(\tau)$, which weights this gradient by how good the trajectory was
* This means we increase the probability of trajectories that lead to high rewards, and decrease the probability of trajectories that lead to low rewards
![](../../attachments/Pasted%20image%2020250410141842.png)
![](../../attachments/Pasted%20image%2020250410141848.png)

# 3 Improving the Gradient
## 3.1 What Does the Gradient Do?
If we define the reward as the forward velocity:
1. $\tau^2$ which falls forwards will have positive reward
2. $\tau^5$ which takes a large step backwards then a small step forwards will have negative total reward
In this situation, the *current* policy gradient will:
* Increase the probability of $\tau^2$
* Decrease the probability of $\tau^5$
This is not what we want because $\tau^5$ actually takes steps instead of falls
![](../../attachments/Pasted%20image%2020250410142242.png)

## 3.2 Improving Using Causality
Currently, we sum rewards over the entire trajectory, which creates a causality issue:
* Actions at time $t$ cannot affect rewards that occurred at earlier times ($t' < t$)
	* Our gradient calculation incorrectly allows past rewards to influence future policy decisions
* **Principle of Causality:** we should only consider the sum of future rewards when evaluating an action at time $t$
![](../../attachments/Pasted%20image%2020250410143322.png)

The issue is now the **scale of the reward:**
* Even if it falls forward, it is still a positive (though small) reward
![](../../attachments/Pasted%20image%2020250410143213.png)

## 3.3 Improving Using Baselines
The simple fix for the reward scale problem is to add a **baseline** constant
* We can set this $b$ equal to the average reward
* Trajectories better than average get increased likelihood, otherwise decreased likelihood
![](../../attachments/Pasted%20image%2020250410143340.png)

As long as the baseline is constant, it will not change the gradient in expectation!
![](../../attachments/Pasted%20image%2020250410143347.png)

In this case, the rewards are:
* For $\tau^1, \tau^2, \tau^3$ the reward is zero ⟶ in all three the jacket is not folded
* For $\tau^4$ the reward is positive ⟶ only trajectory where the jacket is folded
![](../../attachments/Pasted%20image%2020250410143458.png)
Looking at the gradient formula $\nabla_\theta J(\theta)$:
* For $\tau^1, \tau^2, \tau^3$, the term $(\sum_t r(s_t^i,a_t^i) - b)$ equals $(0 - b)$, which is a negative constant
* For $\tau^4$, the term $(\sum_t r(s_t^i,a_t^i) - b)$ is positive since its reward exceeds the baseline
Therefore, the gradient will:
* Equally discourage behaviors in $\tau^1, \tau^2, \tau^3$ (non-folding trajectories)
* Encourage behaviors in $\tau^4$ (the folding trajectory)

# 4 How to Implement Policy Gradients?
Naively, you can run a backward pass for each $\nabla_\theta \log \pi_\theta(a|s)$ in the batch to compute the gradients.
![](../../attachments/Pasted%20image%2020250410205107.png)
![](../../attachments/Pasted%20image%2020250410205148.png)

# 5 Off-Policy Policy Gradient
Our gradient calculates $\mathbb{E}_{\tau \sim p_\theta(\tau)}$.
* This means we need to sample trajectories from the current policy $\pi_\theta$
* For each gradient update, we need to throw away the trajectories and sample new ones, which is inefficient!
![](../../attachments/Pasted%20image%2020250410205444.png)

## 5.1 Importance Sampling
We want to take *more than one gradient step* per batch of collected data
* Importance sampling estimates the expected value of a function under one distribution, using samples from another distribution.
* In our case, we want to estimate the gradient of the expected reward under the new policy, using samples from the old policy.
* We weight each gradient by the ratio of $\frac{\text{new policy's probability of taking the action}}{\text{old policy's probability of taking the action}}$

![](../../attachments/Pasted%20image%2020250410205459.png)
We can use samples from our old policy $q(x) = \bar{p}(\tau)$ and weight it by the new probabilities $p(x) = p_\theta(\tau)$
$$
J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[ r(\tau) \right] = \mathbb{E}_{\tau \sim \bar{p}(\tau)} \left[ \frac{p_\theta(\tau)}{\bar{p}(\tau)} r(\tau) \right]
$$
![](../../attachments/Pasted%20image%2020250410210508.png)
![](../../attachments/Pasted%20image%2020250410210500.png)

## 5.2 Algorithm
![](../../attachments/Pasted%20image%2020250410210653.png)
![](../../attachments/Pasted%20image%2020250410210631.png)

# 6 Summary
![](../../attachments/Pasted%20image%2020250410210707.png)
