
# 1 Recap
## 1.1 Methods
![](../../attachments/Pasted%20image%2020250502213536.png)
## 1.2 Estimating Value
The first two methods (estimating $V^\pi$) are on-policy methods:
* Dataset $D$ is sampled from current policy $\pi$
The last method (estimating $V\pi$) is an off-policy method:
* Dataset $D$ is sampled from an old policy $\pi_{\text{old}}$
* The next action $a'$ used for the TD learning is sampled from our *current* policy
![](../../attachments/Pasted%20image%2020250502213604.png)
## 1.3 Off-Policy Actor-Critic (SAC)
![](../../attachments/Pasted%20image%2020250502213713.png)

## 1.4 Q-Learning
* Model-free RL algorithm that learns the optimal action-value function $Q^*$
* The Q-learning update rule:
$$
Q_\phi(s_t, a_t) \leftarrow Q_\phi(s_t, a_t) + \alpha [r_t + \gamma \max_{a'} Q_{\phi'}(s_{t+1}, a') - Q_\phi(s_t, a_t)]
$$
* Key components:
	* $r_t$ is the immediate reward
	* $\gamma \max_{a'} Q_{\phi'}(s_{t+1}, a')$ represents the maximum future value (using target network $\phi'$)
	* The target is $y_i = r_t + \gamma \max_{a'} Q_{\phi'}(s_{t+1}, a')$
	* In practice, we minimize the loss: $L(\phi) = \mathbb{E}_{(s,a,r,s') \sim D}[(Q_\phi(s,a) - y_i)^2]$
* Q-learning is off-policy because it directly approximates the optimal value function, regardless of which policy is being followed
* It uses the greedy policy (taking the max over actions) for updates, even if following an exploratory policy

# 2 What is Offline RL
![](../../attachments/Pasted%20image%2020250502214650.png)

**Goal:** given static dataset collected by unknown policy $\pi_\beta$, learn a new policy $\pi_\theta$ that maximizes rewards
* This is a *distribution shift* between learned and behavior policy ⟶ can't collect more data
	* The policy we're learning ($\pi_\theta$) will visit different states than those in our dataset
	* Challenge: must learn good actions for states not well-represented in the dataset
* Unlike imitation learning, we don't just copy behavior policy but aim to exceed its performance
	* We use reward signals in the dataset to determine which behaviors to keep/improve
* Note: environment dynamics remain constant, so any differences in trajectories come solely from policy differences
![](../../attachments/Pasted%20image%2020250502214918.png)

## 2.1 Why Offline RL
Might be more useful when:
* leverage datasets collected by other people, existing systems
* online policy collection may be risky or unsafe (e.g. self-driving cars)
* reuse previously collected data (e.g. previous experiments, projects) rather than recollecting it

You can have a **blend** of offline RL first then using online RL after deployment.

## 2.2 Why Can't We Use Off-policy Algorithms
Because of the distribution shift:
* Behavior policy $\pi_\beta$ and learned policy $\pi_\theta$ explore different state-action distributions
* Q-value estimate from the data is not a good proxy for the true Q-value
![](../../attachments/Pasted%20image%2020250502215501.png)

When we start training:
* The $Q$ function is randomly initialized function.
	* For a given state, the $Q(s, a)$ function for that state is a random variable
* Data **only covers subset of action space** in that state ⟶ Q-values for remaining actions are random
	* When you do a policy update, it will try to increase likelihood of actions that have high Q-values
	* But if the action is out-of-distribution, that Q-value is random ⟶ policy update is not actually improving policy
* This leads to **overestimation** in the Q-values.
![](../../attachments/Pasted%20image%2020250502220028.png)
![](../../attachments/Pasted%20image%2020250502215529.png)

> [!NOTE] Core Goal of Offline RL Methods
> How to mitigate overestimation of the Q value function?

# 3 Implicit Policy Constraint Methods
## 3.1 Imitation Learning
The **simplest** approach to offline RL is to do imitation learning:
* Since we do behavior cloning on the data, our policy will implicitly **avoid out-of-distribution** actions.
* This mitigates the overestimation issue!
**Issue:** imitation methods can't outperform the expert
* When the *offline data is not optimal*, imitation learning cannot overcome this
* It also can't stitch together good behaviors: when two trajectories lead to different outcomes, it will average them instead of stitching the subtrajectories together
![](../../attachments/Pasted%20image%2020250502222416.png)

## 3.2 Baseline: Filtered Behavior Cloning
If we have reward labels, **imitate only the good trajectories** with high rewards
![](../../attachments/Pasted%20image%2020250502222659.png)

## 3.3 Advantage-Weighted Regression
Some of the best offline RL methods are not too different from this ⟶ they use **imitation learning**
* This method allows for stitching:
	* In dataset, we have two trajectories that branch from state $s_3$, one bad and one good
	* A policy learned under this way will get to state $s_3$ and see that the advantage of one action is higher than the other ⟶ it will take the good action
	* This effectively stitches the two trajectories together
![](../../attachments/Pasted%20image%2020250502222831.png)

The advantage that we use is $A^{\pi_{\beta}}$⟶ advantage of behavior policy. To estimate the advantage, we can approximate it with Monte Carlo estimation.
* Fit a value function the behavior policy $V^{\pi_\beta}$ with MC (i.e. supervised learning on empirical returns)
* Approximate $A^{\pi_\beta}(s_t, a_t) \approx \sum_{t'=t}^T r(s_{t'}, a_{t'}) - V^{\pi_\beta}(s_t)$
	* Recall that the exact formula for advantage is $A^{\pi_\beta}(s_t, a_t) = Q^{\pi_\beta}(s_t, a_t) - V^{\pi_\beta}(s_t)$
	* The empirical return $\sum_{t'=t}^T r(s_{t'}, a_{t'})$ is an estimate of $Q^{\pi_\beta}(s_t, a_t)$
![](../../attachments/Pasted%20image%2020250502223146.png)
![](../../attachments/Pasted%20image%2020250502224139.png)

Using advantage-weighted regression, if the behavior policy $\pi_\beta$ is deterministic:
* If you're always taking the same action in a given state (deterministic policy), then:
	* The empirical return $\sum_{t'=t}^T r(s_{t'}, a_{t'})$ will be the same every time you visit that state
	* The value function $V^{\pi_\beta}(s_t)$ is trained to predict exactly this return
	* Therefore, $A^{\pi_\beta}(s_t, a_t) = \sum_{t'=t}^T r(s_{t'}, a_{t'}) - V^{\pi_\beta}(s_t) \approx 0$
* Since $\exp(A) = \exp(0) = 1$ ⟶ policy update becomes vanilla imitation learning
	* With no variance in actions, there's no way to determine which actions are better than others
	* In this case, it's better to just imitate the behavior policy $\pi_\beta$

When we have variablility, the advantage function between different actions will differ
![](../../attachments/Pasted%20image%2020250502225708.png)

## 3.4 Advantage-Weighted Actor Critic
* The Monte Carlo estimation is noisy. We ideally want to **use TD learning** to estimate the advantage.
* We can't do it with off-policy objective because that queries **Q-values for out-of-distribution actions**
* **Key Idea:** instead of sampling actions from our policy ($a' \sim \pi_\theta$) just sample from data ($a' \sim D$)
	* Advantage: can now use bootstrapping to estimate the advantage ⟶ less variance
	* Disadvantage: we are estimating the Q-function for $\pi_\beta$ and not $\pi_\theta$
![](../../attachments/Pasted%20image%2020250502230115.png)

## 3.5 Implicit Q-Learning
* Observe that $\mathbb{E}_{a' \sim D} Q(s', a')$ is pretty close to $V^{\pi_\beta}(s')$ since $D$ is sampled from $\pi_\beta$
* **Idea**: instead of sampling from the mean of $V^{\pi_\beta}$, we can try to get a higher percentile (higher value)
![](../../attachments/Pasted%20image%2020250502230254.png)
* By using an asymmetric loss function, we can penalize less for having higher values ⟶ lead to a value function that is better than behavior policy
![](../../attachments/Pasted%20image%2020250502230658.png)
![](../../attachments/Pasted%20image%2020250502230902.png)

The full algorthm for **implicit Q-learning** is below:
1. Fit a value function with expectile loss ⟶ this $V$ estimates a value function for a policy $\pi$ that is *better* than our behavior policy $\pi_\beta$
	* Will be penalize for both underestimating and overestimate, but penalize less for overestimation
2. Fit a $Q$ function with regular MSE loss ⟶ used to calculate advantage, determine better actions
3. Use advantage-weighted regression to get a policy.
![](../../attachments/Pasted%20image%2020250502230726.png)

# 4 Summary
![](../../attachments/Pasted%20image%2020250502231629.png)
![](../../attachments/Pasted%20image%2020250502232150.png)
