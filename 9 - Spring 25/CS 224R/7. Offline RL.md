
# 1 Recap
## 1.1 Methods
![](../../attachments/Pasted%20image%2020250502213536.png)
## 1.2 Estimating Value
The first two methods (estimating $V^\pi$) are on-policy methods:
* Dataset $D$ is sampled from current policy $\pi$
The last method (estimating $V\pi$) is an off-policy method:
* Dataset $D$ is sampled from an old policy $\pi_{\text{old}}$
* The next action $a'$ used for the TD learning is sampled from our *current* policy
![](../../attachments/Pasted%20image%2020250502213604.png)
## 1.3 Off-Policy Actor-Critic (SAC)
![](../../attachments/Pasted%20image%2020250502213713.png)

## 1.4 Q-Learning
* Model-free RL algorithm that learns the optimal action-value function $Q^*$
* The Q-learning update rule:
$$
Q_\phi(s_t, a_t) \leftarrow Q_\phi(s_t, a_t) + \alpha [r_t + \gamma \max_{a'} Q_{\phi'}(s_{t+1}, a') - Q_\phi(s_t, a_t)]
$$
* Key components:
	* $r_t$ is the immediate reward
	* $\gamma \max_{a'} Q_{\phi'}(s_{t+1}, a')$ represents the maximum future value (using target network $\phi'$)
	* The target is $y_i = r_t + \gamma \max_{a'} Q_{\phi'}(s_{t+1}, a')$
	* In practice, we minimize the loss: $L(\phi) = \mathbb{E}_{(s,a,r,s') \sim D}[(Q_\phi(s,a) - y_i)^2]$
* Q-learning is off-policy because it directly approximates the optimal value function, regardless of which policy is being followed
* It uses the greedy policy (taking the max over actions) for updates, even if following an exploratory policy

# 2 What is Offline RL
![](../../attachments/Pasted%20image%2020250502214650.png)

**Goal:** given static dataset collected by unknown policy $\pi_\beta$, learn a new policy $\pi_\theta$ that maximizes rewards
* This is a *distribution shift* between learned and behavior policy ⟶ can't collect more data
	* The policy we're learning ($\pi_\theta$) will visit different states than those in our dataset
	* Challenge: must learn good actions for states not well-represented in the dataset
* Unlike imitation learning, we don't just copy behavior policy but aim to exceed its performance
	* We use reward signals in the dataset to determine which behaviors to keep/improve
* Note: environment dynamics remain constant, so any differences in trajectories come solely from policy differences
![](../../attachments/Pasted%20image%2020250502214918.png)

## 2.1 Why Offline RL
Might be more useful when:
* leverage datasets collected by other people, existing systems
* online policy collection may be risky or unsafe (e.g. self-driving cars)
* reuse previously collected data (e.g. previous experiments, projects) rather than recollecting it

You can have a **blend** of offline RL first then using online RL after deployment.

## 2.2 Why Can't We Use Off-policy Algorithms
Because of the distribution shift:
* Behavior policy $\pi_\beta$ and learned policy $\pi_\theta$ explore different state-action distributions
* Q-value estimate from the data is not a good proxy for the true Q-value
![](../../attachments/Pasted%20image%2020250502215501.png)

When we start training:
* The $Q$ function is randomly initialized function.
	* For a given state, the $Q(s, a)$ function for that state is a random variable
* Data **only covers subset of action space** in that state ⟶ Q-values for remaining actions are random
	* When you do a policy update, it will try to increase likelihood of actions that have high Q-values
	* But if the action is out-of-distribution, that Q-value is random ⟶ policy update is not actually improving policy
* This leads to **overestimation** in the Q-values.
![](../../attachments/Pasted%20image%2020250502220028.png)
![](../../attachments/Pasted%20image%2020250502215529.png)

> [!NOTE] Core Goal of Offline RL Methods
> How to mitigate overestimation of the Q value function?

# 3 Implicit Policy Constraint Methods
## 3.1 Imitation Learning
The **simplest** approach to offline RL is to do imitation learning:
* Since we do behavior cloning on the data, our policy will implicitly **avoid out-of-distribution** actions.
* This mitigates the overestimation issue!
**Issue:** imitation methods can't outperform the expert
* When the *offline data is not optimal*, imitation learning cannot overcome this
* It also can't stitch together good behaviors: when two trajectories lead to different outcomes, it will average them instead of stitching the subtrajectories together
![](../../attachments/Pasted%20image%2020250502222416.png)

## 3.2 Baseline: Filtered Behavior Cloning
If we have reward labels, **imitate only the good trajectories** with high rewards
![](../../attachments/Pasted%20image%2020250502222659.png)

## 3.3 Advantage-Weighted Regression
Some of the best offline RL methods are not too different from this ⟶ they use **imitation learning**
* This method allows for stitching:
	* In dataset, we have two trajectories that branch from state $s_3$, one bad and one good
	* A policy learned under this way will get to state $s_3$ and see that the advantage of one action is higher than the other ⟶ it will take the good action
	* This effectively stitches the two trajectories together
![](../../attachments/Pasted%20image%2020250502222831.png)

The advantage that we use is $A^{\pi_{\beta}}$⟶ advantage of behavior policy. To estimate the advantage, we can approximate it with Monte Carlo estimation.
* Fit a value function the behavior policy $V^{\pi_\beta}$ with MC (i.e. supervised learning on empirical returns)
* Approximate $A^{\pi_\beta}(s_t, a_t) \approx \sum_{t'=t}^T r(s_{t'}, a_{t'}) - V^{\pi_\beta}(s_t)$
	* Recall that the exact formula for advantage is $A^{\pi_\beta}(s_t, a_t) = Q^{\pi_\beta}(s_t, a_t) - V^{\pi_\beta}(s_t)$
	* The empirical return $\sum_{t'=t}^T r(s_{t'}, a_{t'})$ is an estimate of $Q^{\pi_\beta}(s_t, a_t)$
![](../../attachments/Pasted%20image%2020250502223146.png)
![](../../attachments/Pasted%20image%2020250502224139.png)

Using advantage-weighted regression, if the behavior policy $\pi_\beta$ is deterministic:
* If you're always taking the same action in a given state (deterministic policy), then:
	* The empirical return $\sum_{t'=t}^T r(s_{t'}, a_{t'})$ will be the same every time you visit that state
	* The value function $V^{\pi_\beta}(s_t)$ is trained to predict exactly this return
	* Therefore, $A^{\pi_\beta}(s_t, a_t) = \sum_{t'=t}^T r(s_{t'}, a_{t'}) - V^{\pi_\beta}(s_t) \approx 0$
* Since $\exp(A) = \exp(0) = 1$ ⟶ policy update becomes vanilla imitation learning
	* With no variance in actions, there's no way to determine which actions are better than others
	* In this case, it's better to just imitate the behavior policy $\pi_\beta$

When we have variablility, the advantage function between different actions will differ
![](../../attachments/Pasted%20image%2020250502225708.png)

## 3.4 Advantage-Weighted Actor Critic
* The Monte Carlo estimation is noisy. We ideally want to **use TD learning** to estimate the advantage.
* We can't do it with off-policy objective because that queries **Q-values for out-of-distribution actions**
* **Key Idea:** instead of sampling actions from our policy ($a' \sim \pi_\theta$) just sample from data ($a' \sim D$)
	* Advantage: can now use bootstrapping to estimate the advantage ⟶ less variance
	* Disadvantage: we are estimating the Q-function for $\pi_\beta$ and not $\pi_\theta$
![](../../attachments/Pasted%20image%2020250502230115.png)

## 3.5 Implicit Q-Learning
* **Goal of IQL**: Learn a policy that is better than the behavior policy while avoiding out-of-distribution actions
* Observe that $\mathbb{E}_{a' \sim D} Q(s', a')$ is pretty close to $V^{\pi_\beta}(s')$ since $D$ is sampled from $\pi_\beta$
* **Key Idea**: Instead of sampling from the mean of $V^{\pi_\beta}$, we can try to get a higher percentile (higher value)
	* At a given state $s$, there is an underlying distribution of returns for the value $V(s)$, coming from different trajectories
	* When we do Monte Carlo estimation with L2 loss on the behavior policy, we get a single estimate of the value ⟶ mean of the distribution
	* The value function for the best policy given the data might be at a higher percentile of the distribution ⟶ we want to try to learn this
![](../../attachments/Pasted%20image%2020250502230254.png)

* IQL attempts to learn a Q-function while never querying the Q-value of actions not seen in the dataset (avoiding overestimation)
* The method relies on the learned Q-function's ability to generalize and then considering the top expectile of actions
* To do this, we can use an asymmetric loss function called expectile regression
* The expectile loss uses a parameter $\tau \in [0, 1]$ to control the asymmetry:
	* For $\text{diff} = Q(s,a) - V(s)$, the loss is:
		* $L_V(\text{diff}) = |1-\tau| \cdot \text{diff}^2$ if $\text{diff} < 0$ (underestimation)
		* $L_V(\text{diff}) = |\tau| \cdot \text{diff}^2$ if $\text{diff} \geq 0$ (overestimation)
	* For $\tau > 0.5$: overestimation is penalized more heavily than underestimation
		* Penalize more when $Q(s,a) > V(s)$
		* This causes $L_V$ to push $V(s)$ toward higher values, closer to the upper values of $Q(s,a)$ in the dataset
	* For $\tau < 0.5$: underestimation is penalized more heavily than overestimation
		* Penalize more when $Q(s,a) < V(s)$
		* This causes $L_V$ to push $V(s)$ toward lower values, closer to the lower values of $Q(s,a)$ in the dataset
	* For $\tau = 0.5$: symmetric loss (equivalent to MSE)
![](../../attachments/Pasted%20image%2020250502230658.png)

![](../../attachments/Pasted%20image%2020250502230902.png)

In IQL, we typically use $\tau > 0.5$ (e.g., $\tau = 0.7$ or $0.9$) to bias the value function toward higher Q-values, effectively learning from better-than-average actions. This pushes the value function towards the upper expectile of the Q-values in the dataset.

The full algorithm for **Implicit Q-Learning (IQL)** is:
1. Learn a value function $V_\psi(s)$ by minimizing:
$$
L_V(\psi) := \mathbb{E}_{s,a \sim D}[L_\tau(Q_\theta(s,a) - V_\psi(s))]
$$
   * This pushes $V_\psi(s)$ towards the upper expectile of Q-values
   * Only samples states and actions from the dataset, not next states (more stable)
1. Fit a Q-function with regular MSE loss:
$$
L_Q(\theta) = \mathbb{E}_{s,a,s' \sim D}[(r(s,a) + \gamma V_\psi(s') - Q_\theta(s,a))^2]
$$
2. Use advantage-weighted regression to get a policy:
$$
\pi_\phi(a|s) \propto \exp(\beta (Q_\theta(s,a) - V_\psi(s)))
$$
This approach balances two key goals in offline RL:
* Improving beyond the behavior policy by focusing on higher-value actions
* Avoiding out-of-distribution actions by only considering actions present in the dataset
![](../../attachments/Pasted%20image%2020250502230726.png)

# 4 Summary
![](../../attachments/Pasted%20image%2020250502231629.png)
![](../../attachments/Pasted%20image%2020250502232150.png)
