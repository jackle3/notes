# 1 Sequence Modeling
* Vanilla neural networks are **one to one**: one input maps to one output.
* There are also other types of modeling:
	* one to many: image captioning ⟶ input is image, output is sequence of words
	* many to one: action prediction ⟶ input is sequence of frames, output is an action
	* many to many: video captioning ⟶ input is sequence of frames, output is sequence of words
	* many to many: video classification ⟶ input is frames, output is classification for each frame
* RNNs can work in any of these types. For this lecture, they typically work in **many to many**.
![](../../attachments/Pasted%20image%2020250501161231.png)

# 2 Recurrent Neural Networks
Given a sequence of inputs $\vec{x}$, the RNN recurrently updates its internal state as it predicts each token of $\vec{y}$
![](../../attachments/Pasted%20image%2020250501161330.png)
If we **unroll** the diagram above, it looks like this. Notice the **hidden state** being passed along
![](../../attachments/Pasted%20image%2020250501161524.png)

## 2.1 Mathematical Formulation
To process a sequence of vectors $x$, we apply the recurrence $h_t = f_W(h_{t-1}, x_t)$ at each time step.
* **Important**: the same function and the same set of parameters are used at each time step.
![](../../attachments/Pasted%20image%2020250501161451.png)
![](../../attachments/Pasted%20image%2020250501161506.png)

## 2.2 Vanilla RNN
The state consists of a single hidden vector calculated as:
$$
h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t)
$$
The output is calculated as:
$$
y_t = W_{hy}h_t + b_y
$$

# 3 Learning an RNN
## 3.1 Example: Manually Creating Weight Matrices
* Consider an RNN where we want to output $y=1$ if the last two $x$ values were 1.
* We can use the hidden state for this.
	* When calculating $y_t$, we need to know current $x$ and previous $x$ ⟶ store in $h_t$
![](../../attachments/Pasted%20image%2020250501162256.png)

To keep track of current value, we can set $W_{xh}$ to identity for top row
* When $x=0$, $W_{xh} x = 0$
* When $x=1$, $W_{xh} x = e_1$
![](../../attachments/Pasted%20image%2020250501162726.png)

To keep track of previous values, we set $W_{hh}$ to:
* Zero out the top row (i.e. discard "current" value in previous hidden state)
* Move "current" value in previous hidden state to previous value for current hidden state)
![](../../attachments/Pasted%20image%2020250501162733.png)

To produce the output, simply do algebra with the hidden state:
![](../../attachments/Pasted%20image%2020250501162756.png)

## 3.2 Computational Graph
In an RNN, the **weight matrix is re-used at every time step**

In the **many-to-many** scenario, you can compute a loss for each output at each timestep.
* This loss sums up to produce the final loss. When we backprop, we go backwards from this.
![](../../attachments/Pasted%20image%2020250501163015.png)

In the **many-to-one** scenario, you can either:
* Compute output $y$ using every hidden state (e.g. video classification)
* Compute output $y$ using only the last hidden state
![](../../attachments/Pasted%20image%2020250501163026.png)

In the **one-to-many** scenario, there is only one input. However, you still need an input to $f_W$:
* You can just input zeros as your input and have all the computation be from the previous hidden state
* Autoregressive ⟶ input to next state is output of previous state
![](../../attachments/Pasted%20image%2020250501163440.png)

## 3.3 Backpropagation
When using same $W$ at all time steps, backprop through time is difficult if there are many time steps.
![](../../attachments/Pasted%20image%2020250501163451.png)

You can use **truncated backprop** to deal with this
* Run forward and backward through **chunks of the sequence** instead of the whole sequence
* Note: we lose information here because we're not backpropagating through the entire sequence.
	* The hidden states at chunk boundaries still contain information from previous chunks, but gradients don't flow beyond chunk boundaries.
![](../../attachments/Pasted%20image%2020250501163658.png)

If you only had **one output**, you can backprop through each hidden state to see how a previous hidden state contributed to the final loss.
* We still go in chunks backwards through (the green)
![](../../attachments/Pasted%20image%2020250501164012.png)

# 4 Language Modeling
## 4.1 Training
* Language modeling with RNNs: train the network to predict the next token in a sequence.
	* At each timestep, the RNN outputs a probability distribution over the vocabulary.
	* We compare this prediction to the actual next token using cross-entropy loss.
* This frames language modeling as a sequence of classification problems.
![](../../attachments/Pasted%20image%2020250501164209.png)

## 4.2 Inference
* To produce output during inference, model autoregressively generates each subsequent token.
	* Sample from the model's predicted probability distribution and feed that output to the next state.
* Common sampling strategies:
	* Greedy: Always choose the most likely next token
	* Temperature sampling: Control randomness by scaling logits before softmax
		* Divide logits by temperature parameter τ before applying softmax
		* Higher temperature (τ > 1) → more uniform distribution → more randomness
		* Lower temperature (τ < 1) → more peaked distribution → less randomness
	* Pure sampling: Simply sample from the full probability distribution (equivalent to temperature=1)
![](../../attachments/Pasted%20image%2020250501164706.png)
![Untitled 33 22.png](../../attachments/Untitled%2033%2022.png)

## 4.3 Embedding Layer
* The input to the RNN is a one-hot encoded vector.
* We can use an embedding layer to map each input token to a dense vector.
* This allows the RNN to learn a more efficient representation of the input.
![](../../attachments/Pasted%20image%2020250501164944.png)

# 5 RNN Tradeoffs
### 5.1.1 **Advantages**
* Can process any length of the input (theoretically no context length)
* Computation for step $t$ can (in theory) use information from many steps back
* Model size does not increase for longer input
* The same weights are applied on every timestep, so there is symmetry in how inputs are processed.
### 5.1.2 **Disadvantages**
* Recurrent computation is slow ⟶ sequential nature prevents parallelization *during training*
	* Each step depends on the previous step's output
	* Cannot compute multiple timesteps simultaneously
* In practice, difficult to use information from many steps back ⟶ vanishing/exploding gradients
	* Hidden state is fixed size, so the longer you go we'll eventually lose information
	* Gradients can become extremely small or large when backpropagating through many time steps

# 6 RNN Applications
## 6.1 Image Captioning
Historically this was done using a CNN and an RNN.
![](../../attachments/Pasted%20image%2020250501170612.png)

Use a CNN to extract features from the image.
* Discard the final classification layer and use transfer learning.
* Output of CNN becomes an input to the first hidden state of the RNN.
![](../../attachments/Pasted%20image%2020250501170740.png)
![](../../attachments/Pasted%20image%2020250501170837.png)

# 7 RNN Variants
## 7.1 Multilayer RNNs
* In practice, most of the RNNs shown are multi-layer
* There are now multiple layers of hidden states ⟶ dependence is now from both depth and time
![](../../attachments/Pasted%20image%2020250501171115.png)

## 7.2 LSTM
### 7.2.1 Exploding and Vanishing Gradients
![](../../attachments/Pasted%20image%2020250501171252.png)
![](../../attachments/Pasted%20image%2020250501171307.png)
![](../../attachments/Pasted%20image%2020250501171325.png)
![](../../attachments/Pasted%20image%2020250501171342.png)
* To fix **exploding gradients**,  we can use gradient clipping ⟶ scale gradient if norm is too big
* To fix **vanishing gradients**, we need to *change the architecture*

### 7.2.2 LSTM Model
The LSTM architecture addresses the vanishing gradient problem in traditional RNNs:
* It uses a cell state and multiple gates (input, forget, output) to control information flow
* The forget gate (f) and input gate (i) are particularly important:
	* When f = 1 and i = 0, information in the cell is preserved indefinitely
	* This makes it easier to maintain information over many timesteps
* By contrast, vanilla RNNs struggle to learn a recurrent weight matrix (Wh) that preserves information in the hidden state
* While LSTMs don't completely eliminate vanishing/exploding gradients, they provide a more effective mechanism for learning long-distance dependencies

![](../../attachments/Pasted%20image%2020250501171518.png)
![](../../attachments/Pasted%20image%2020250501171647.png)
![](../../attachments/Pasted%20image%2020250501171705.png)

Because the top flow has no activation, we can minimize the vanishing gradient problem!
![](../../attachments/Pasted%20image%2020250501171938.png)

# 8 Summary
* RNNs allow a lot of flexibility in architecture design
* Vanilla RNNs are simple but don't work very well
* More complex variants (e.g. LSTMs, Mamba) can introduce ways to selectively pass information forward
* Backward flow of gradients in RNN can explode or vanish.
	* Exploding is controlled with gradient clipping.
	* Backpropagation through time is often needed.
* Better/simpler architectures are a hot topic of current research, as well as new paradigms for reasoning over sequences
