# 1 Multi-layer Perceptrons
So far, our scores have been linear functions of the input $x$
$$
f = Wx + b
$$
Now, we introduce non-linearity by introducing multiple layers with non-linear activations.
$$
f = W_2 \cdot \max(0, W_1 \cdot x + b_1) + b_2
$$
* Dimensions are $x \in \mathbb{R}^D$, $W_1 \in \mathbb{R}^{H \times D}$, $b_1 \in \mathbb{R}^H$, $W_2 \in \mathbb{R}^{C \times H}$, $b_2 \in \mathbb{R}^C$.
* These weight matrices are known as fully-connected layers or linear layers.
![](../../attachments/Pasted%20image%2020250413191927.png)

By introducing hidden layers, we can learn more complex features or templates.
![](../../attachments/Pasted%20image%2020250413192324.png)

## 1.1 Activation Functions
The $\max$ is important because otherwise we still have a linear model: $f = W_2 \cdot (W_1 \cdot x) = (W_2 W_1) \cdot x$
![](../../attachments/Pasted%20image%2020250413192100.png)

## 1.2 Simple Implementation
![](../../attachments/Pasted%20image%2020250413192314.png)

# 2 Backpropagation
## 2.1 Forward Pass
Given the input $x$, we compute the total loss by running the network forward.
1. Non-linear score function
$$
s = f(x; W_1, W_2) = W_2 \max(0, W_1 x + b_1) + b_2
$$
2. Loss function
$$
L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + 1)
$$
3. Regularization
$$
R(W) = \sum_k W_k^2
$$
4. Total loss
$$
L = \frac{1}{N} \sum_{i=1}^N L_i + \lambda R(W_1) + \lambda R(W_2)
$$
## 2.2 Backward Pass
In order to run gradient descent, we need to compute grad of loss wrt to params $\frac{\partial L}{\partial W_1}$ and $\frac{\partial L}{\partial W_2}$.
* (Bad Idea) We can compute the gradient analytically. This is not scalable for large networks.
* (Good Idea) Use computational graphs and the chain rule to automatically compute the gradients.
![](../../attachments/Pasted%20image%2020250413193054.png)
### 2.2.1 Local, Upstream, and Downstream Gradients
Consider a function $f$ that takes inputs $x, y$ and outputs $z$.
* Local gradients: grad of **output** wrt to **input**  (e.g. $\frac{\partial z}{\partial x}$, $\frac{\partial z}{\partial y}$)
* Upstream gradients: grad of **loss** wrt to **output** (e.g. $\frac{\partial L}{\partial z}$)
* Downstream gradients: grad of **loss** wrt to **input** (e.g. $\frac{\partial L}{\partial x}$, $\frac{\partial L}{\partial y}$)
![](../../attachments/Pasted%20image%2020250413195528.png)

## 2.3 Simple Example
Consider the following scenario:
![](../../attachments/Pasted%20image%2020250413193314.png)

We want to compute $\frac{\partial f}{\partial x}$, $\frac{\partial f}{\partial y}$, $\frac{\partial f}{\partial z}$. Trivially, we know that $\frac{\partial f}{\partial f} = 1$.

| ![](../../attachments/Pasted%20image%2020250413193426.png) | ![](../../attachments/Pasted%20image%2020250413193445.png) |
| ---------------------------------------------------------- | ---------------------------------------------------------- |
| ![](../../attachments/Pasted%20image%2020250413193455.png) | ![](../../attachments/Pasted%20image%2020250413193508.png) |

## 2.4 Sigmoid Example
### 2.4.1 Entire Backward Pass
![](../../attachments/Pasted%20image%2020250413195934.png)
![](../../attachments/Pasted%20image%2020250413195945.png)
![](../../attachments/Pasted%20image%2020250413195954.png)
![](../../attachments/Pasted%20image%2020250413200004.png)
![](../../attachments/Pasted%20image%2020250413200031.png)
![](../../attachments/Pasted%20image%2020250413200044.png)

### 2.4.2 Sigmoid
![](../../attachments/Pasted%20image%2020250413200123.png)
![](../../attachments/Pasted%20image%2020250413200129.png)

## 2.5 Patterns in Gradient Flow
![](../../attachments/Pasted%20image%2020250413200232.png)

# 3 Backprop with Vectors
## 3.1 Vector Derivatives
![](../../attachments/Pasted%20image%2020250413200358.png)
## 3.2 Backprop with Vectors
1. The loss is **still a scalar**
2. The gradient of variables wrt to the loss have the **same shape** as the original
![](../../attachments/Pasted%20image%2020250413200447.png)
![](../../attachments/Pasted%20image%2020250413200558.png)
![](../../attachments/Pasted%20image%2020250413200634.png)

## 3.3 Backprop with Tensors
![](../../attachments/Pasted%20image%2020250413200655.png)
![](../../attachments/Pasted%20image%2020250413200717.png)
![](../../attachments/Pasted%20image%2020250413200741.png)
![](../../attachments/Pasted%20image%2020250413200820.png)

# 4 Summary
1. FC neural networks are stacks of linear functions and nonlinear activation functions
2. Backpropagation: recursive application of chain rule along computational graph to compute gradients
	* Implementations maintain a graph structure, where nodes implement local `forward()` and `backward()` functions
	* `forward()` computes the output of the node given the input, saves intermediates for gradient computation
