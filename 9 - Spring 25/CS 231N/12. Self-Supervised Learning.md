
# 1 Learned Representations
Also known as features or embeddings, these vectors give a good representation of the input.
**Problem:** large scale training needs a lot of labeled data. Can we train it without labeled datasets?
![](../../attachments/Pasted%20image%2020250520140014.png)

# 2 Self Supervised Learning
## 2.1 Pretext Task
* Define a general task based on the data itself (e.g. next word prediction)
	* No manual annotation needed
* Could be considered an **unsupervised** task, but we learn with supervised learning objectives like classification or rgression
* **Use the pretext task to train an encoder (i.e. learning the learned representations)**
![](../../attachments/Pasted%20image%2020250520140231.png)
![](../../attachments/Pasted%20image%2020250520140426.png)

## 2.2 Downstream Task
* The application you actually care about, but you do not have a large dataset for
* The dataset for downstream task is labeled
* **Fine-tune or transfer the pretext trained model for the downstream task**
![](../../attachments/Pasted%20image%2020250520140248.png)

# 3 Evaluating Self-Supervised Learning
Since the goal of self-supervised learning is to learn on cheap data to improve performance on downstream tasks, we evaluate on that!
![](../../attachments/Pasted%20image%2020250520140521.png)

1. self sup gives us a good feature extractor, then we use that for supervised learning
![](../../attachments/Pasted%20image%2020250520140849.png)

## 3.1 Applications
self sup is used for many things including LLMs and even robots and reinforcement learning
* E.g. self driving cars are just collecting data and learning self-supervised
![](../../attachments/Pasted%20image%2020250520140955.png)

# 4 Pretext Tasks from Image Transformations
## 4.1 Predicting Rotations
Hypothesis: a model could recognize the correct rotation of an object only if it has the “visual commonsense” of what the object should look like unperturbed.
![](../../attachments/Pasted%20image%2020250520141123.png)
![](../../attachments/Pasted%20image%2020250520141134.png)
They were able to get very good results on predicting the rotation ⟶ indicates model learning visual understanding
* It also shows that semi-supervised can usually match performance of full supervised training
![](../../attachments/Pasted%20image%2020250520141152.png)

* In this example, `ImageNet labels` is full supervised training with all of the ground truth labels
* The RotNet is able to almost reach that, and outperforms all other methods, **without needing labels**
![](../../attachments/Pasted%20image%2020250520141408.png)

* The attention maps between supervised and self-supervised look similar but also looks a little better too
![](../../attachments/Pasted%20image%2020250520141518.png)

## 4.2 Predicting Relative Patch Locations
This is pretty much an 8-way classification ⟶ given reference (blue), which location is frame (red)
![](../../attachments/Pasted%20image%2020250520141555.png)

## 4.3 Jigsaw Puzzles
Instead of just predicting which patch it is, we permute the patches and have the model try to predict the permutation
![](../../attachments/Pasted%20image%2020250520141658.png)
![](../../attachments/Pasted%20image%2020250520141748.png)

## 4.4 Inpainting
The task is to predict missing pixels (mask parts of it, and have model inpaint it)
![](../../attachments/Pasted%20image%2020250520141803.png)

The encoder turns the image into feature space, then the decoder tries to inpaint and reconstruct the image
* Loss function compares the output with the ground truth missing patch
* This is effectively **learning to reconstruct the missing pixels**
* This is an example of an **autoencoder** ⟶ encodes input image, then decodes to output, training with masking objective
![](../../attachments/Pasted%20image%2020250520141906.png)

Since there are many ways to inpaint an area, the loss is often a combination of reconstruction and adversarial learning loss between real and inpainted images ⟶ leads to better performance (seen below)
![](../../attachments/Pasted%20image%2020250520142045.png)
![](../../attachments/Pasted%20image%2020250520142006.png)
![](../../attachments/Pasted%20image%2020250520142125.png)

## 4.5 Image Coloring
Colored images have a $L$ grayscale channel and two color $ab$ channels. Task is to predict $ab$ given $L$.
* We can then concatenate $(L, ab)$ to get our final colored image.
![](../../attachments/Pasted%20image%2020250521115840.png)
**Image coloring as a pretext task is useful not just as pretraining but also as a downstream task**
* We can use it to colorize grayscale images using the pretrained data

Learning from this task can get us a **split-brain autoencoder**
1. We first split the input image into the $L$ and $ab$ channels
2. Each *split* autoencoder predicts the other channel (L ⟶ ab, ab ⟶ L)
3. We can then reconstruct the image from each split
![](../../attachments/Pasted%20image%2020250521115914.png)
![](../../attachments/Pasted%20image%2020250521120044.png)
![](../../attachments/Pasted%20image%2020250521120031.png)

## 4.6 Video Coloring
![](../../attachments/Pasted%20image%2020250521120651.png)
![](../../attachments/Pasted%20image%2020250521120709.png)

This works using attention as well ⟶ learn attention maps on reference frame onto target frames
![](../../attachments/Pasted%20image%2020250521120738.png)

We can use the learned attention from colorization for **tracking applications**
![](../../attachments/Pasted%20image%2020250521120905.png)
![](../../attachments/Pasted%20image%2020250521120911.png)

## 4.7 Masked Auto Encoders (MAE)
This has been a great framework for pretraining for many different downstream tasks
* Similar to the masking strategy from before, but more advanced and aggressive ⟶ **reconstruction**
![](../../attachments/Pasted%20image%2020250521121610.png)

* This method has an **asymmetric** encoder and decoder
![](../../attachments/Pasted%20image%2020250521121751.png)
![](../../attachments/Pasted%20image%2020250521121812.png)
![](../../attachments/Pasted%20image%2020250521121836.png)
![](../../attachments/Pasted%20image%2020250521121850.png)

### 4.7.1 Linear Probing vs. Full Fine-Tuning
To apply the encoder on the downstream tasks, there are two main methods to do so:
![](../../attachments/Pasted%20image%2020250521121957.png)

## 4.8 Summary
![](../../attachments/Pasted%20image%2020250521122059.png)

# 5 Contrastive Representation Learning
**Problems with pretext tasks:** (1) coming up with individual pretext tasks is tedious, and (2) the learned representations may not be general ⟶ tied to a specific pretext task

**Solution:** contrastive representation learning can serve as a **general pretext task**
![](../../attachments/Pasted%20image%2020250521122220.png)

## 5.1 Formulation
![](../../attachments/Pasted%20image%2020250521122338.png)
![](../../attachments/Pasted%20image%2020250521122404.png)
![](../../attachments/Pasted%20image%2020250521122420.png)
![](../../attachments/Pasted%20image%2020250521122456.png)

## 5.2 SimCLR
![](../../attachments/Pasted%20image%2020250521122527.png)
![](../../attachments/Pasted%20image%2020250521122606.png)

### 5.2.1 Algorithm
There are $2N$ samples because for each sample in $N$, we generate a positive pair
![](../../attachments/Pasted%20image%2020250521122712.png)

We take each image (gray) and its data augmented pair (green) and put them together in the batch
* If we look at the affinity matrix, for the first row (first grey image), the second image (first green image) is a positive sample, while the rest is negative
![](../../attachments/Pasted%20image%2020250521122739.png)

### 5.2.2 Fine Tuning
Using SimCLR features can pretty much match the supervised network (though needs a lot of params)
![](../../attachments/Pasted%20image%2020250521122935.png)
![](../../attachments/Pasted%20image%2020250521123007.png)

### 5.2.3 Design Choice: Projection Head
![](../../attachments/Pasted%20image%2020250521123050.png)

### 5.2.4 Design Choice: Large Match Size
![](../../attachments/Pasted%20image%2020250521123233.png)

## 5.3 MoCo
The large batch sizes of SimCLR required a lot of memory ⟶ MoCo tries to fix that with a queue
* We now have a separate encoder for positive samples and a **momentum encoder** for negative samples
![](../../attachments/Pasted%20image%2020250521123321.png)
![](../../attachments/Pasted%20image%2020250521123459.png)
![](../../attachments/Pasted%20image%2020250521123512.png)

### 5.3.1 Key Takeaways
* Non-linear projection head and strong data augmentation are crucial for contrastive learning.
* Decoupling mini-batch size with negative sample size allows MoCo-V2 to outperform SimCLR with smaller batch size (256 vs. 8192).
* … all with much smaller memory footprint! (5G per GPU versus 93G per GPU in SimCLR)

# 6 Sequence Contrastive Learning
So far we've looked at **instance-level contrastive learning** ⟶ positive and negative instances
Now we look at contrastive learning based on **sequential / temporal orders**
![](../../attachments/Pasted%20image%2020250521150126.png)
![](../../attachments/Pasted%20image%2020250521150211.png)
![](../../attachments/Pasted%20image%2020250521150546.png)
![](../../attachments/Pasted%20image%2020250521150717.png)

# 7 DINO: Self-Distillation with No Labels
DINO is a self-supervised learning approach that uses **knowledge distillation** without requiring labeled data
* Trains a student network to match predictions of a teacher network
* Processes different augmented views of the same image through both networks
* Updates teacher weights using exponential moving average of student weights
* Creates a form of self-distillation through this teacher-student relationship
* Learns meaningful visual representations by enforcing consistency across different views
* Achieves this without requiring any manual annotations
![](../../attachments/Pasted%20image%2020250521150801.png)
![](../../attachments/Pasted%20image%2020250521150956.png)

# 8 Summary
![](../../attachments/Pasted%20image%2020250521151014.png)
![](../../attachments/Pasted%20image%2020250521151027.png)
![](../../attachments/Pasted%20image%2020250521151043.png)
![](../../attachments/Pasted%20image%2020250521151100.png)
