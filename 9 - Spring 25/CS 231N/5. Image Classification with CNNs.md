
# 1 Recap: Deep Learning Basics
Given an image $x$ ⟶ we compute scores (likelihood) for each class
![](../../attachments/Pasted%20image%2020250417202920.png)

Loss function: given a weight matrix and a dataset, how well does the matrix solve the problem?
![](../../attachments/Pasted%20image%2020250417203147.png)

We use optimization algorithsm to find weights that give minimum loss
![](../../attachments/Pasted%20image%2020250417203917.png)

Linear classifiers are linear in their parameters $W$
* We can have non-linearity on the inputs with feature maps, but it will still be linear on $W$
![](../../attachments/Pasted%20image%2020250417204204.png)

We can generalize to neural networks to have more expressitivity ⟶ non-linearity with more params
![](../../attachments/Pasted%20image%2020250417204411.png)

To optimize these complex networks, we use **backpropagation**
* Each node in the graph is a function (e.g. linear layer, ReLU, etc)
* We move forwards through the graph to compute the loss
* We move backwards thru each node to compute gradients of loss wrt to node
* The gradient of the loss wrt to a param has the same shape as the param
![](../../attachments/Pasted%20image%2020250417204426.png)
![](../../attachments/Pasted%20image%2020250417204435.png)
![](../../attachments/Pasted%20image%2020250417204549.png)
![](../../attachments/Pasted%20image%2020250417204556.png)

# 2 Feature Extraction
## 2.1 Feature Engineering
So far, we've computed the scores for each class using a linear function of the pixels values (**pixel space**)
![](../../attachments/Pasted%20image%2020250417204935.png)

An advancement of this is to use **feature representations**:
* Extract useful features from the pixels, and then use those *better* features to compute scores
* Eg: color histograms of the image, histogram of oriented gradients, etc.
![](../../attachments/Pasted%20image%2020250417205147.png)

However, this requires humans to select and design the features, which can introduce error:
* Feature extraction is **designed by humans** and training is done based on those features
* With CNNs, the training is end-to-end ⟶ the model learns to extract features
![](../../attachments/Pasted%20image%2020250417205307.png)

## 2.2 Convolutional Networks
Recall that in neural networks, the **spatial information is destroyed** because it flattens the image
![](../../attachments/Pasted%20image%2020250417205533.png)

Convolutional networks can **learn spatial information** and extract useful feature representations
* There is a traditional MLP after the convolution to compute scores based on the learned features
* The entire model is **trained end-to-end** with backprop and gradient descent
![](../../attachments/Pasted%20image%2020250417205610.png)

CNNs are **less used now** because **transformers** are dominating, but they are still useful
![](../../attachments/Pasted%20image%2020250417205756.png)
![](../../attachments/Pasted%20image%2020250417205818.png)

# 3 Convolutional Neural Networks
![](../../attachments/Pasted%20image%2020250417211133.png)

## 3.1 Fully Connected Layers
There is no spatial information in fully connected layers ⟶ this is bad for images
* $W$ stores a $10$ templates, each with the same shape as our image ($3072$)
* When computing the score, we check alignment between the image and the template
	* Recall that dot product is pretty much checking how much two vectors align
![](../../attachments/Pasted%20image%2020250417204857.png)
![](../../attachments/Pasted%20image%2020250417205942.png)

## 3.2 Convolution Layers
![](../../attachments/Pasted%20image%2020250417204903.png)

The convolutional layer allows us to learn spatial information in the templates
* We convolve the *filter* with the image by sliding it over the image spatially, computing dot products
* At each stride, we ask how much that chunk of the image aligns with our template (via the filter)
* The filters are the **learnable parameters** of the network
![](../../attachments/Pasted%20image%2020250417210203.png)

Each stride of the filter produces a single scalar $w^T x + b$.
* Once we slide all locations, this produces an **activation map**. If we have multiple filters, we create a **set of activation maps**.
* Each activation map represents the response of a specific filter across that image.
* Activation maps contain **richer feature information** than original pixel values, capturing patterns like edges, textures, and higher-level structures.
![](../../attachments/Pasted%20image%2020250417210629.png)
![](../../attachments/Pasted%20image%2020250417210656.png)

For a batch of $N$ images, we compute $N$ sets of activation maps.
* $C_\text{in}$ is the number of channels in the input image (e.g. 3 for RGB)
* $C_\text{out}$ is the number of filters that we apply
![](../../attachments/Pasted%20image%2020250417210926.png)

## 3.3 What Do Conv Filters Learn?
In linear classifiers, **each row** of the weight matrix $W$ is a **template for a class**, where the template has the same shape as the original image.
![|500](../../attachments/Pasted%20image%2020250417211409.png)

In convolutional layers, each filter is a template for a small sub-chunk of the image.
* These filters learn to detect specific local patterns (like edges, textures, or simple shapes) that appear anywhere in the image, rather than learning a template for the entire image.
* To learn different patterns, **each filter has to be initialized to different random values**

| ![](../../attachments/Pasted%20image%2020250417211729.png) | ![\|700](../../attachments/Pasted%20image%2020250417211647.png) |
| ---------------------------------------------------------- | --------------------------------------------------------------- |
|                                                            |                                                                 |

## 3.4 Stride and Padding
As we apply the filters, the output activation map is smaller than the input image.
* For input size $W \times H$, filter size $K \times K$, and stride $S$:
* Output dimensions: $\left(\frac{W-K}{S}+1\right) \times \left(\frac{H-K}{S}+1\right)$

**Padding** addresses the shrinking problem by adding extra border pixels (usually zeros) around the input.
* With padding $P$, output dimensions become: $\left(\frac{W-K+2P}{S}+1\right) \times \left(\frac{H-K+2P}{S}+1\right)$
* Common setting with single stride: $P = (K - 1) /2$ ⟶ output dimensions equal input dimensions

## 3.5 Receptive Fields
The region of the input image that affects the output of a specific activation in the output image.
* With a kernel size $K$, the receptive field of an output element is $K \times K$ of the input
* The receptive field grows with each successive convolutional layer
![](../../attachments/Pasted%20image%2020250417212857.png)

For larger images, we need **many layers** for each output to see the entire image.
* Solution: use striding to downsample inside the network, allowing the receptive field to grow exponentially

## 3.6 Convolution Summary
![](../../attachments/Pasted%20image%2020250417213328.png)
![](../../attachments/Pasted%20image%2020250417213335.png)

# 4 Pooling Layers
We want to **downsample** because this allows us to build **larger receptive fields** quicker
* One method was to use strided convolutions to make smaller activation maps ⟶ this was expensive
* Pooling layers are a very cheap way to downsample the image inside the network.
![](../../attachments/Pasted%20image%2020250417213530.png)

## 4.1 Methods of Pooling
One common method is **max-pooling** ⟶ take a chunk and return the maximum value of that chunk
* Other methods include average pooling, etc.
![](../../attachments/Pasted%20image%2020250417213626.png)

## 4.2 Pooling Summary
![](../../attachments/Pasted%20image%2020250417213659.png)

# 5 Translation Equivariance
Recall: we want operators that respect the **spatial structure** of our images
* Both convolution and pooling layers are translation equivariant ⟶ if we shift the input, the output shifts by the same amount
* In other words, these two order of operations lead to the same output:
	1. Conv/Pool then Translate
	2. Translate then Conv/Pool
* **Implications**: when processing images, the features that we extract from the image only depends on the content of the image, not the location of the content of the image
![](../../attachments/Pasted%20image%2020250417213904.png)
