![](../../attachments/Pasted%20image%2020250524160055.png)

# 1 Generative Adversarial Networks
GANs are implicit density models ⟶ we can sample from underlying learned distribution
![](../../attachments/Pasted%20image%2020250524155940.png)

**Setup:** have data $x_i$ drawn from distribution $p_\text{data}(x)$. We want to sample $p_\text{data}$
**Idea:** introduce a latent variable $z$ with a simpe prior $p(z)$ (e.g. a unit Gaussian)
* Sample $z \sim p(z)$ and pass to a **Generator Network** $x = G(z)$
* Then $x$ is a sample from the **Generator distribution** $p_G$ ⟶ we want $p_G = p_\text{data}$
* The generator network **converts** a sample from our known distribution $p_z$ into a sample from the data distribution $p_\text{data}$
How to force $p_G$ to match $p_\text{data}$?
* Train a **discriminator network** to classify data as real or fake, and have the two networks fight
* Generator is trained to fool the discriminator, discriminator is trained to detect that, both jointly improve
![](../../attachments/Pasted%20image%2020250524160137.png)

## 1.1 Training Objective
The feedback through both networks are via gradients, allowing them to learn from each other.
![](../../attachments/Pasted%20image%2020250524160538.png)
![](../../attachments/Pasted%20image%2020250524160811.png)
![](../../attachments/Pasted%20image%2020250524160818.png)

**Important:** GANs are really hard to train because there are no training curves to look at
* This $V$ is not a loss function ⟶ does not tell us anything about how well $p_G$ matches $p_\text{data}$
* There are different settings of $G$ and $D$ that leads to same values for $V$, even if its better
![](../../attachments/Pasted%20image%2020250524160918.png)

## 1.2 Objective Trick
**Problem:** At the start, generator is producing random noise and discriminator can easily tell it apart
* From the POV of generator, we are at the left edge of blue curve ⟶ gradients are close to zero at beginning

**Solution:** use a different objective for generator with better gradients
![](../../attachments/Pasted%20image%2020250524161748.png)

Why might this be a good objective?
* We can theoretically write down the optimal objectives for both the max and the min
* Even if we can write it down, we can't compute it because we need $p_\text{data}$
![](../../attachments/Pasted%20image%2020250524161929.png)

## 1.3 DC-GAN
This was a 5-layer convnet architecture ⟶ first GAN to gave non-trivial results
![](../../attachments/Pasted%20image%2020250524162250.png)

## 1.4 StyleGAN
Uses a much more complicated architecture but gets pretty nice results
![](../../attachments/Pasted%20image%2020250524162622.png)

## 1.5 GANs: Latent Space Interpolation
GANs tend to learn smooth latent spaces, allowing us to easily interpolate between latent vectors
![](../../attachments/Kapture%202025-05-24%20at%2016.28.23.gif)

## 1.6 Summary
The **generator** gives a mapping from latent space $Z$ into data space $X$
* There is no way to map back from data space $X$ to latent space $Z$ (unlike VAEs)
* Advantage over VAEs: your samples are now very crisp and clean ⟶ much better generations
* At inference, you throw the discriminator away and just use the generator
![](../../attachments/Pasted%20image%2020250524162742.png)

# 2 Diffusion Models: Rectified Flow
We cover the basics of a modern “clean” implementation (**Rectified Flow**)
* These models replaced GANs ⟶ much better results now
* The noise distribution has the same shape as our data (e.g. HxWx3 for images)
* The $t$ parameter **interpolates smoothly** between the noise distribution and the data distribution
* The NN removes a little bit of noise (slightly moves from noise towards data)
![](../../attachments/Pasted%20image%2020250524162955.png)

## 2.1 Rectified Flow: Training
This is a particular category of diffusion models ⟶ geometric intuition
* On each training iteration:
	1. sample a noise sample $z \sim p_\text{noise}$
	2. sample an image $x$ from our training set
	3. sample a noise level $t \sim \text{Uniform}[0, 1]$
* Let $v$ be the velocity of the flow field ⟶ line pointing from image $x$ to noise $z$
* Let $x_t$ be a point along the velocity line ⟶ interpolation based on noise level $t$
	* This is a noisy/corrupted piece of data based on $x$ ⟶ interpolated from noise and data
* Train a NN to predict the velocity vector $v$ given $x_t$ and noise level $t$
![](../../attachments/Pasted%20image%2020250524171422.png)
![](../../attachments/Pasted%20image%2020250524172008.png)

## 2.2 Rectified Flow: Sampling
The loop marches linearly from full noise ($t=1$) to image ($t=0$) based on the number of steps $T$
![](../../attachments/Pasted%20image%2020250524172111.png)

Then using that predicted $v$ from the flow model, take a little step backwards to get a new $x_{2/3}$
![](../../attachments/Pasted%20image%2020250524172209.png)
![](../../attachments/Pasted%20image%2020250524172237.png)
![](../../attachments/Pasted%20image%2020250524172258.png)

Once we get to $t=0$, that is our final output image
![](../../attachments/Pasted%20image%2020250524172308.png)

## 2.3 Rectified Flow Summary
The code for rectified flow is super simple ⟶ converges to reasonable results
![](../../attachments/Pasted%20image%2020250524172339.png)

# 3 Conditional Rectified Flow
So far, we had unconditional generative modeling ⟶ no way to control what image is output
Now, we **condition the generation**:
* Suppose our data distribution has two subparts ⟶ squares and triangles
* We condition the model on the extra $y$ (e.g. we want squares or triangles) to move closer to desired control
![](../../attachments/Pasted%20image%2020250524172556.png)

## 3.1 Classifier-Free Guidance
We now have the conditioning $y$ but **can we control how much we emphasize the conditioning?**
* Model is conceptually forced to learn two different kinds of velocity vectors: conditional and unconditional
![](../../attachments/Pasted%20image%2020250524172657.png)
![](../../attachments/Pasted%20image%2020250524172753.png)
![](../../attachments/Pasted%20image%2020250524172833.png)
![](../../attachments/Pasted%20image%2020250524172857.png)

## 3.2 Optimal Prediction
![](../../attachments/Pasted%20image%2020250524172924.png)
![](../../attachments/Pasted%20image%2020250524172932.png)
![](../../attachments/Pasted%20image%2020250524172943.png)
![](../../attachments/Pasted%20image%2020250524172951.png)
![](../../attachments/Pasted%20image%2020250524173004.png)

## 3.3 Noise Schedules
Our noise schedules so far are uniform ⟶ we put equal importance on all noise, even though figuring out middle noise is harder because its more ambiguous
![](../../attachments/Pasted%20image%2020250524173017.png)
In practice, we use logit-normal sampling to put more weight in the middle ⟶ relatively little weight on the $t =0$ and $t=1$
![](../../attachments/Pasted%20image%2020250524173025.png)
![](../../attachments/Pasted%20image%2020250524173037.png)

## 3.4 Summary
![](../../attachments/Pasted%20image%2020250524173217.png)
**Key Problem:** does not scale naively to higher resolution data
* Notice how above, we needed a different noise schedule for higher resolution data

# 4 Latent Diffusion Models (LDMs)
![](../../attachments/Pasted%20image%2020250524173255.png)
![](../../attachments/Pasted%20image%2020250524173304.png)
![](../../attachments/Pasted%20image%2020250524173322.png)
![](../../attachments/Pasted%20image%2020250524173331.png)

## 4.1 Training Encoder-Decoder
![](../../attachments/Pasted%20image%2020250524173405.png)
We need high quality decoded outputs, since that bottlenecks the quality of the downstream generations from the latent diffusion model ⟶ use discriminator to improve decoder!
![](../../attachments/Pasted%20image%2020250524173412.png)

## 4.2 Summary
The modern generative modeling pipeline includes all three models: VAE + GAN + diffusion
![](../../attachments/Pasted%20image%2020250524173520.png)

# 5 Diffusion Transformer (DiT)
Relatively straightforward transformers can be applied to diffusion and works quite well
* There are now three inputs to the transformer block: namely noisy image, timestep $t$, and conditioning signal)
	* How do we inject the timestep? **Predict scale and shift** to modulate activations of transformer
	* How do we inject the conditioning? **Use cross or joint attention** to jam it into the inputs
![](../../attachments/Pasted%20image%2020250524173614.png)

## 5.1 Text-to-Image
![](../../attachments/Pasted%20image%2020250524173801.png)
![](../../attachments/Pasted%20image%2020250524173809.png)

## 5.2 Text-to-Video
Observe that we include an **additional time dimension** into our latents
* The decoder is typically now a spatio-temporal autoencoder ⟶ video consistent over space and time
* Note: very expensive to train due to high sequence length
![](../../attachments/Kapture%202025-05-24%20at%2017.39.12.gif)
![](../../attachments/Pasted%20image%2020250524174219.png)

# 6 Diffusion Distillation
**Problem**: diffusion sampling is super slow
* Not only is inference slow due to transformers scaling quadratically with sequence length, but also we need to iterate over many steps in order to get good results with rectified flow

**Solution**: use a **diffusion distillation** approach to speed up inference
* Train a student model to mimic the behavior of the teacher diffusion model
* Instead of running many denoising steps, the student model learns to predict the final output directly
* This allows us to generate high-quality samples in just a few steps or even a single step
* The student model is trained using knowledge distillation, where it learns to match the outputs of the teacher model
![](../../attachments/Pasted%20image%2020250524174147.png)

# 7 Generalized Diffusion
Rectified flow is just a form of generalized diffusion with certain parameters
* If we use different parameters, we come out with **different forms of diffusion**
![](../../attachments/Pasted%20image%2020250524174452.png)
![](../../attachments/Pasted%20image%2020250524174604.png)
![](../../attachments/Pasted%20image%2020250524174608.png)
![](../../attachments/Pasted%20image%2020250524174613.png)

# 8 Interpretations of Diffusion
There are three primary interpretations of diffusion:

**Diffusion is a latent variable model**
* We know the forward process: just simply add Gaussian noise to an image
* We learn a network to approximate the backward process: iteratively remove noise to generate an image
* Recall in VAE: we had latent $z$ and image $x$, and we train model to predict $z$ from $x$
![](../../attachments/Pasted%20image%2020250524174709.png)

**Diffusion Learns the Score Function**
* Given a data distribution $p(x)$, there is a score function $s(x)$
![](../../attachments/Pasted%20image%2020250524174857.png)

**Diffusion Solves Stochastic Differential Equations**
![](../../attachments/Pasted%20image%2020250524175021.png)

## 8.1 Perspectives on Diffusion
There are actually eight different ways to think of diffusion models
![](../../attachments/Pasted%20image%2020250524175056.png)

# 9 Autoregressive Models Strike Back
![](../../attachments/Pasted%20image%2020250524175138.png)
![](../../attachments/Pasted%20image%2020250524175134.png)

# 10 Summary
![](../../attachments/Pasted%20image%2020250524175155.png)
