
# 1 What is Video Understanding?
Video can be thought of as a sequence of images ⟶ 4D tensor with a time dimension $T$
![](../../attachments/Pasted%20image%2020250507151557.png)

# 2 Video Classification
![](../../attachments/Pasted%20image%2020250507151628.png)

## 2.1 Problem: Videos Are Big
![](../../attachments/Pasted%20image%2020250507151657.png)

## 2.2 Solution: Training on Clips
To address the problem, we can break the video into short clips and have *sub-classifications* for those
![](../../attachments/Pasted%20image%2020250507151719.png)
There are a few methods for training on clips:

### 2.2.1 Single-Frame CNN
* Train a normal 2D CNN to classify video frames independently
* Average the predicted probabilities of the frames at test time
* Often a very strong baseline for video classification
![](../../attachments/Pasted%20image%2020250507151908.png)

### 2.2.2 Late Fusion (with FC layers)
* Instead of simply averaging the predicted probabilities, we can concatenate the frame features and feed it into a **learned MLP** to classify it
* **Problem:** clip features has a huge dimension, MLP will have a lot of parameters
![](../../attachments/Pasted%20image%2020250507152258.png)

### 2.2.3 Late Fusion (with pooling)
* This is similar but instead of simply flattening and concatenating the features, we use an **average pooling over space (H, W) and time (T)** before feeding it into a learned MLP
	* Now the MLP has a much smaller dimension, and the number of parameters is reduced
![](../../attachments/Pasted%20image%2020250507152253.png)

### 2.2.4 Early Fusion
* **Problem with late fusion:** since we process each frame independently, it is hard to compare low-level motion between frames (e.g. motion of feet between frames)
* **Solution:** treat the time dimension as the channel dimension (reshape to `3T x H x W`) and apply a regular 2D CNN
	* The **first convolutional layer** can then learn filters that span across multiple frames
		* This processes the temporal information early in the network, allowing the CNN to detect motion patterns directly from pixel-level changes
		* After this initial fusion, the rest of the network is just a regular CNN architecture
	* **Problem:** this collapses all temporal information after the first layer
![](../../attachments/Pasted%20image%2020250507152643.png)

### 2.2.5 3D CNNs
* **Problem with early fusion:** one layer of temporal processing may not be enough to capture complex temporal dynamics across the entire video
* **Solution:** slowly fuse temporal information via 3D convolutions
	* Instead of 2D filters (H×W), use 3D filters (T×H×W) that operate across both space and time
	* This allows the network to learn spatiotemporal features at multiple levels of abstraction
	* Each 3D convolutional layer processes a chunk of consecutive frames and preserves temporal information throughout the network
	* Temporal information is gradually integrated as the signal moves deeper into the network
![](../../attachments/Pasted%20image%2020250507152912.png)

# 3 What is a 3D CNN?
In a 2D CNN, you go from `C x H x W` to `F x H' x W'` where `F` is the number of filters.
* Each filter is of size `C x k x k` where `k` is the kernel size.
![](../../attachments/Pasted%20image%2020250507153404.png)

In a 3D CNN, you go from `C x T x H x W` to `F x T' x H' x W'` where `F` is the number of filters.
* Each filter is of size `C x k_t x k x k`
	* `k` is the spatial kernel size
	* `k_t` is the temporal kernel size.
![](../../attachments/Pasted%20image%2020250507153410.png)

## 3.1 Receptive Fields
**In late fusion:**
* We slowly build up the spatial receptive field, without really caring for the temporal dynamics
* The temporal receptive field is built all-at-once at the end via the global average pooling
![](../../attachments/Kapture%202025-05-07%20at%2015.39.37.gif)

**In early fusion:**
* We build up the temporal receptive field all-at-once via the first convolutional layer
* The spatial receptive field is built up gradually via the remaining convolutional layers
**In 3D CNN:**
* We do slow fusion by gradually building up the spatial and temporal receptive fields together
![](../../attachments/Pasted%20image%2020250507153647.png)

## 3.2 Early Diffusion Vs 3D CNN
Both of these methods build their receptives slowly in space. Whats the difference?

In early fusion, the filter slides over spatial dimensions $x$ and $y$
* no temporal sliding, the filter extends the full time dimension
* **Problem:** there is no temporal-shift invariance
	* This means the network cannot detect the same action if it occurs at different times in the video
	* For example, if a person raises their hand at the beginning vs. middle of a video clip, early fusion would treat these as completely different patterns
	* This is in contrast to how 2D CNNs provide spatial translation invariance for images (e.g. a dog in the left half of an image is the same as a dog in the right half)
![](../../attachments/Pasted%20image%2020250507154323.png)
![](../../attachments/Pasted%20image%2020250507154352.png)

In **3D CNNs**, the filter is now smaller and slides in all three dimensions
* Since the filter now slides over time, we have temporal shift-invariance ⟶ same filter can be used at multiple times
![](../../attachments/Pasted%20image%2020250507154428.png)

# 4 C3D: The VGG of 3D CNNs
![](../../attachments/Pasted%20image%2020250507154756.png)
![](../../attachments/Pasted%20image%2020250507154801.png)

# 5 Optical Flow
We can measure and predict the motion between adjacent frames using optical flow:
![](../../attachments/Pasted%20image%2020250507155521.png)
You can have a two-stream network to get a more informed video classification
* One stream is a CNN that processes the spatial information in the video frames
* The other stream is a CNN that processes the optical flow (motion between frames)
![](../../attachments/Pasted%20image%2020250507155530.png)

# 6 Long-term Temporal Structure
* So far all our temporal CNNs **only model local motion between frames** in very short clips of ~2-5 seconds. What about *long-term structure?*
![](../../attachments/Pasted%20image%2020250507154948.png)

## 6.1 Solution 1: Combining CNN and RNN
We know how to handle sequences! How about modeling it as **recurrent networks**?
* **Inside CNN:** Each value is a function of a fixed temporal window (local temporal structure)
* **Inside RNN:** Each vector is a function of all previous vectors (global temporal structure)
![](../../attachments/Pasted%20image%2020250507155000.png)
![](../../attachments/Pasted%20image%2020250507155024.png)

## 6.2 Solution 2: Recurrent Convolution Network
* Instead of having a separate RNN and CNN, we can have a single network that performs both operations.
* Similar to the multi-layer RNN, each layer is a convolution and each column is a time step
![](../../attachments/Pasted%20image%2020250507155343.png)
![](../../attachments/Pasted%20image%2020250507155401.png)

### 6.2.1 What is the RCN Layer?
![](../../attachments/Pasted%20image%2020250507155413.png)

**Key Idea:** replace all the affine matrix multiplications in RNNs with convolutions
![](../../attachments/Pasted%20image%2020250507155419.png)
![](../../attachments/Pasted%20image%2020250507155433.png)

### 6.2.2 Performance
While useful, this was not used very much because RNNs are already slow, and this is even slower.
![](../../attachments/Pasted%20image%2020250507160128.png)

# 7 Spatio-Temporal Self-Attention
This is another method for modeling long-term temporal structure, but much more efficiently.
![](../../attachments/Pasted%20image%2020250507160335.png)

This is similar to the self-attention we use in transformers, but we include a **temporal dimension**.
![](../../attachments/Pasted%20image%2020250507160357.png)
![](../../attachments/Pasted%20image%2020250507160437.png)

# 8 3D CNN Architectures
**Key Question:** can we reuse or inflate a 2D CNN directly for 3D (video) applications? Yes!

We can transfer the **architectures** by replacing 2D layers with 3D layers.
![](../../attachments/Pasted%20image%2020250507160618.png)
![](../../attachments/Pasted%20image%2020250507160626.png)

We can also transfer the **weights** by repeating the existing 2D weights across the temporal dimension.
* This gives the same affect as a 2D convolution given a constant video input
* We can then extend by training the new 3D layers on the time dimension
![](../../attachments/Pasted%20image%2020250507160800.png)

# 9 Vision Transformers for Video
Vision Transformers have been successfully adapted for video understanding tasks. The key adaptation is handling the temporal dimension in addition to spatial dimensions.
![](../../attachments/Pasted%20image%2020250507160949.png)

# 10 Temporal Action Localization
![](../../attachments/Pasted%20image%2020250507161322.png)
![](../../attachments/Pasted%20image%2020250507161334.png)

# 11 Other Modalities of Video Understanding
## 11.1 Audio
Videos include not only the visual and temporal information, but also audio. Can we use this?
* E.g. when doing speech recognition, we can use the mouth movements to help with the recognition
![](../../attachments/Pasted%20image%2020250507161444.png)
![](../../attachments/Pasted%20image%2020250507161804.png)

## 11.2 Efficient Video Understanding
The methods so far process each clip, and then aggregate them to get a video level understanding
* On edge devices, we can't really do this for every clip. How to be more efficient?
![](../../attachments/Pasted%20image%2020250507161831.png)
![](../../attachments/Pasted%20image%2020250507161840.png)

## 11.3 Video Understanding + LLMs
![](../../attachments/Pasted%20image%2020250507161930.png)
