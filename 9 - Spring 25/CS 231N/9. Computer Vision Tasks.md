
# 1 Recap: Transformers
![](../../attachments/Pasted%20image%2020250504150225.png)
![](../../attachments/Pasted%20image%2020250504150233.png)

## 1.1 Vision Transformers
1. Break image into input patches, each of shape 16x16x3
2. Linear project the patches to create *tokens* ⟶ these are input to the transformer
3. With classification, we include a `[CLS]` token in the input. The output for this token will be the predicted class scores.
![](../../attachments/Pasted%20image%2020250504150302.png)
4. Alternatively, instead of having a `[CLS]` token, we apply pooling to output vectors and project them to predict the class scores.
![](../../attachments/Pasted%20image%2020250504150313.png)

# 2 Computer Vision Tasks
![](../../attachments/Pasted%20image%2020250504150418.png)

# 3 Semantic Segmentation
![](../../attachments/Pasted%20image%2020250504150948.png)
To train:
* We have a loss for each pixel (i.e. pixel-wise classification loss)
* We can just take the mean or sum of those to get the total loss for the image
**Important:** we only care about classifying each pixel, not separating the instances of each object.

## 3.1 Idea: Sliding Window
**Key Steps:**
1. Extract a patch from the image (so that we have context)
2. Use a CNN to predict the class score for the center pixel of the patch
3. Slide the patch around in order to predict class scores for every pixel in the image (i.e. segmentation)
![](../../attachments/Pasted%20image%2020250504151216.png)

## 3.2 Idea: Fully Convolutional
**Goal:** encode the entire image with a conv net, and do semantic segmentation on top
* **Problem:** CNNs often reduce feature spatial sizes to go deeper, but semantic segmentation requires the output size to be the same as input size.

**Idea 1:** convolve without any downsampling ⟶ expensive
![](../../attachments/Pasted%20image%2020250504151414.png)

**Idea 2:** design a network that downsamples to start then upsamples before prediction
![](../../attachments/Pasted%20image%2020250504151909.png)

## 3.3 How to Upsample?
### 3.3.1 Unpooling
The first method is to perform an **unpooling operation** ⟶ reverse the pooling operation.
![](../../attachments/Pasted%20image%2020250504151550.png)
![](../../attachments/Pasted%20image%2020250504151556.png)

### 3.3.2 Learnable Upsampling
Another method is to perform **learnable upsampling** with transposed convolutions.
* We learn a set of weights to upsample from the input to the output!
![](../../attachments/Pasted%20image%2020250504151646.png)
![](../../attachments/Pasted%20image%2020250504151654.png)
![](../../attachments/Pasted%20image%2020250504151746.png)
![](../../attachments/Pasted%20image%2020250504151808.png)

## 3.4 U-Net Architecture
![](../../attachments/Pasted%20image%2020250504152023.png)
![](../../attachments/Pasted%20image%2020250504152029.png)
![](../../attachments/Pasted%20image%2020250504152035.png)

# 4 Object Detection
Similar to semantic segmentation, but we now want to segment **multiple instances of the same object.**

## 4.1 Single-Object Detection
**Idea 1:** dual objective of segmentation class scores and regression to find a bounding box for the class
* **Problem:** does not scale well when we have multiple objects ⟶ each object requires different outputs
![](../../attachments/Pasted%20image%2020250504152436.png)

## 4.2 R-CNN: Multi-Object Detection (Multi-pass)
**Idea 2:** sliding window of different crops of the image, and have a CNN classify it as object or background
* **Problem:** need to apply CNN to huge number of locations, scales, and aspect ratios ⟶ expensive
![](../../attachments/Pasted%20image%2020250504152528.png)

**Idea 3:** selectively find regions with high probability of objects ⟶ faster to run idea 2
* **Problem:** requires independent forward passes for each region of interest ⟶ no reuse of spatial info
![](../../attachments/Pasted%20image%2020250504152832.png)
![](../../attachments/Pasted%20image%2020250504152904.png)

**Idea 4:** same as before but pass image through CNN first ⟶ region proposals in feature dimension
![](../../attachments/Pasted%20image%2020250504153047.png)

### 4.2.1 Region Proposal Network
1. Randomly selects a bounding box in the image
2. Predict whether the bounding box contains an object (binary)
3. For positive boxes, also regress to the ground truth box
4. In practice, use K different bounding boxes of different size/scales at each location, pick top proposals
![](../../attachments/Pasted%20image%2020250504153315.png)

## 4.3 YOLO: Multi-Object Detection (single pass)
![](../../attachments/Pasted%20image%2020250504153447.png)
![](../../attachments/Pasted%20image%2020250504153457.png)
YOLO works by dividing image into grid of cells, and predicting bounding boxes for each cell. It predicts:
1. Probability that the box contains an object
2. $B$ potential bounding boxes
3. Class scores for each bounding box
This outputs many bounding boxes with different object probabilities ⟶ apply threshold to get final box
![](../../attachments/Pasted%20image%2020250504153622.png)

## 4.4 DETR
This technique purely uses transformers ⟶ no need for bounding box regression and etc
* Attention can generate the bounding boxes and object detections for us
![](../../attachments/Pasted%20image%2020250504162330.png)

The steps are:
1. Turn the image into patches and pass that through a CNN to get image tokens
2. Combine that with positional encodings and pass into a transformer encoder
3. Use a transformer decoder to generate the set of bounding boxes and object detections
	* To generate bounding boxes, decoder takes in trainable query vectors and attends to the image tokens
	* Outputs a set of bounding boxes and object detections
![](../../attachments/Pasted%20image%2020250504162340.png)

# 5 Instance Segmentation
With Fast R-CNN, we were able to get object detection ⟶ outputs bounding boxes and class scores
* Instance segmentation is similar, but we want to segment each instance individually
* We want to get a mask for each instance
![](../../attachments/Pasted%20image%2020250504163056.png)
## 5.1 Mask R-CNN
**Key Idea:** extends the Fast R-CNN by adding a small mask network that operates on each bounding box and predicts a binary mask for segmentation
![](../../attachments/Pasted%20image%2020250504163110.png)
The last conv network is the mask network ⟶ combines with output of previous conv network
![](../../attachments/Pasted%20image%2020250504163351.png)
![](../../attachments/Pasted%20image%2020250504163435.png)
![](../../attachments/Pasted%20image%2020250504163447.png)
![](../../attachments/Pasted%20image%2020250504163452.png)

# 6 Visualizing Neural Networks
## 6.1 Model Layer Visualization
Recall that with the linear classifiers, we can look at the linear weights to get a "template" for each class
![](../../attachments/Pasted%20image%2020250504163704.png)

We can do the same for CNNs ⟶ filters in the conv layers reveal what they are focusing on (e.g. edges)
![](../../attachments/Pasted%20image%2020250504163833.png)

## 6.2 Saliency Maps
Allows us to visualize which pixels in an image most strongly influence the network's classification
* For example, when classifying a medical image for a tumor, saliency maps can highlight which specific regions the model is focusing on to make its diagnosis

The mathematical approach works as follows:
1. For a given input image and target class, we want compute **how sensitive the class score is to changes in each pixel**
2. Specifically, we calculate the gradient of the class score with respect to each input pixel:
   * In traditional backpropagation: we compute ∂Loss/∂weights to update model parameters
   * For saliency maps: we compute ∂Score_class/∂input_pixels to identify important regions
3. Pixels with larger gradient magnitudes have more influence on the classification decision
![](../../attachments/Pasted%20image%2020250504164145.png)
![](../../attachments/Pasted%20image%2020250504164151.png)

## 6.3 Class Activation Mapping
One of the most widely used methods for understanding CNNs ⟶ visualizes which regions of an input image contribute most to a specific class prediction.
1. Extract feature maps from the final convolutional layer
2. Weight these maps using the classification layer weights
3. Create a heatmap that highlights discriminative regions
Mathematically:
* Let $f_k(x,y)$ be the activation of unit $k$ at spatial location $(x,y)$ in the last convolutional layer
* The class score $S_c$ for class $c$ is: $S_c = \sum_k w_k^c \cdot \frac{1}{Z} \sum_{x,y} f_k(x,y)$
* The class activation map $M_c$ for class $c$ is: $M_c(x,y) = \sum_k w_k^c \cdot f_k(x,y)$

CAM reveals the network's "attention" without requiring architectural changes or retraining.
![](../../attachments/Pasted%20image%2020250504164400.png)
![](../../attachments/Pasted%20image%2020250504164523.png)

## 6.4 Gradient-Weighted CAM (Grad-CAM)
**Problem:** Traditional CAM has limitations:
* Can only be applied to the last convolutional layer
* Requires a specific network architecture (global average pooling followed by a fully connected layer)
* Not useful for visualizing features in deeper layers
**Solution:** Grad-CAM generalizes CAM to any CNN architecture without requiring architectural changes:
1. Forward pass the image through the network to obtain class score
2. Backpropagate to compute gradients of the class score with respect to feature maps of a target convolutional layer
3. Global-average-pool these gradients to obtain importance weights for each channel
4. Create a weighted combination of forward activation maps using these weights
5. Apply ReLU to highlight only features that have a positive influence on the class of interest
![](../../attachments/Pasted%20image%2020250504164558.png)
![](../../attachments/Pasted%20image%2020250504164606.png)

## 6.5 Visualizing ViT Features
Since transformers already have the attention mechanism, we can simply visualize the attention weights to understand what the model is focusing on.
![](../../attachments/Pasted%20image%2020250504164841.png)
