
# 1 Layers in CNNs
![](../../attachments/Pasted%20image%2020250418172004.png)

## 1.1 Recap: Convolution and Pooling
1. The main layer of CNNs is **convolution layers**, which create activation maps given input.
![](../../attachments/Pasted%20image%2020250418171823.png)

2. Pooling layers are still a sliding filter, but they allow us to downsample the image to get receptive fields.
![](../../attachments/Pasted%20image%2020250418171838.png)

## 1.2 Normalization Layers
**Key Idea:** Normalization layers compute statistics over specific dimensions and normalize the data accordingly, followed by applying learnable scale ($\gamma$) and shift ($\beta$) parameters.
![](../../attachments/Pasted%20image%2020250418172351.png)
### 1.2.1 **Batch Normalization**
- Computes statistics across the **batch ($N$) dimension** for **each channel**
- Blue region in visualization: spans the entire $N$ dimension and a single $C$ position
- Formula: For each position $(c,h,w)$, normalize using:
	- $\mu_c = \frac{1}{N} \sum_{n=1}^{N} x_{n,c,h,w}$
	- $\sigma_c = \sqrt{\frac{1}{N} \sum_{n=1}^{N} (x_{n,c,h,w} - \mu_c)^2}$
	- $y_{n,c,h,w} = \gamma_c \cdot \left(\frac{x_{n,c,h,w} - \mu_c}{\sigma_c}\right) + \beta_c$
### 1.2.2 **Layer Normalization**
- Computes statistics across the **channel ($C$) dimension** for **each sample**
- Blue region in visualization: spans the entire $C$ dimension and a single $N$ position
	- I.e. normalizes across all features (activations) for a single sample
- Formula: For each sample $n$, normalize using:
	- $\mu_n = \frac{1}{C} \sum_{c=1}^{C} x_{n,c,h,w}$
	- $\sigma_n = \sqrt{\frac{1}{C} \sum_{c=1}^{C} (x_{n,c,h,w} - \mu_n)^2}$
	- $y_{n,c,h,w} = \gamma_c \cdot \left(\frac{x_{n,c,h,w} - \mu_n}{\sigma_n}\right) + \beta_c$
### 1.2.3 **Instance Normalization**
- Like Layer Norm but applied to each channel independently for each sample
- Blue region in visualization: spans a single channel for a single sample
- Formula: For each sample $n$ and channel $c$, normalize using:
	- $\mu_{n,c} = \frac{1}{H \times W} \sum_{h=1}^{H} \sum_{w=1}^{W} x_{n,c,h,w}$
	- $\sigma_{n,c} = \sqrt{\frac{1}{H \times W} \sum_{h=1}^{H} \sum_{w=1}^{W} (x_{n,c,h,w} - \mu_{n,c})^2}$
	- $y_{n,c,h,w} = \gamma_c \cdot \left(\frac{x_{n,c,h,w} - \mu_{n,c}}{\sigma_{n,c}}\right) + \beta_c$
### 1.2.4 **Group Normalization**
- Groups channels together and normalizes within each group for each sample
- Blue region in visualization: spans a group of channels for a single sample
- Formula: For each sample $n$ and group $g$, normalize using:
	- $\mu_{n,g} = \frac{1}{C_g \times H \times W} \sum_{c \in g} \sum_{h=1}^{H} \sum_{w=1}^{W} x_{n,c,h,w}$
	- $\sigma_{n,g} = \sqrt{\frac{1}{C_g \times H \times W} \sum_{c \in g} \sum_{h=1}^{H} \sum_{w=1}^{W} (x_{n,c,h,w} - \mu_{n,g})^2}$
	- $y_{n,c,h,w} = \gamma_c \cdot \left(\frac{x_{n,c,h,w} - \mu_{n,g}}{\sigma_{n,g}}\right) + \beta_c$ (where $c$ belongs to group $g$)

## 1.3 Regularization: Dropout
**Key Idea:** add randomization during the training process, and we'll remove that randomization during inference ⟶ harder to train but model will generalize better
![](../../attachments/Pasted%20image%2020250418173136.png)

Dropout forces redundant representations: it's training a **large ensemble** of models that share parameters
- Each binary mask (i.e. enabled neurons) represents one model
![](../../attachments/Pasted%20image%2020250418173151.png)

**At test time**, we reenable all neurons so that there is not randomness
- Note: we must scale the activations so that for each neuron, the output at test time is the expected output at training time ⟶ multiply activations by probabiliy of dropout
![](../../attachments/Pasted%20image%2020250418173412.png)

### 1.3.1 Implementation
![](../../attachments/Pasted%20image%2020250418173422.png)

# 2 Activation Functions
**Key Idea:** introduce non-linearities into the neural network to learn more complex representations

## 2.1 Sigmoid
Common activation function but suffers from the **vanishing gradient problem**
- For a large majority of inputs, the gradient is close to 0 ⟶ the network hardly learns anything
![](../../attachments/Pasted%20image%2020250418173614.png)

## 2.2 ReLU
Replaced sigmoid, but still has zero gradients when $x < 0$
![](../../attachments/Pasted%20image%2020250418173807.png)

## 2.3 Other Forms of ReLU
To fix the previous issue, we introduce smoothness near zero so neurons don't die
- (+) Computes $f(x) = x \times \phi(x)$ where $\phi(x)$ is some function like Gaussian
- (+) Smoothness facilitates training in practice
- (-) More computationally expensive than ReLU
- (-) Large negative values can still have zero gradients
**Transformers tend to use GeLU**
![](../../attachments/Pasted%20image%2020250418174001.png)

# 3 CNN Architectures
Current architectures beat humans at image classification ⟶ deeper layers lead to lower error rates
![](../../attachments/Pasted%20image%2020250418174304.png)

## 3.1 VGGNet
![](../../attachments/Pasted%20image%2020250418174348.png)
There were a few things that allowed VGG to perform very well:
1. Why use smaller filters? (3x3 conv)
	- Stack of three 3x3 conv layers have **the same effective receptive field** as one 7x7 filter
	- (+) The stack is *deeper* and have *more non-linearities*
	- (+) It has less parameters ⟶ $3 \times (3^2 C^2) = 27 C^2$ vs $7^2 C^2 = 49 C^2$ for $C$ channels per layer
![|500](../../attachments/Pasted%20image%2020250418174536.png)

## 3.2 ResNet
Ideally, we want **deeper models** to have more representation power (more parameters).
- We can't naively stack deeper layers onto CNNs because **deeper models are harder to optimize**
- A 56-layer naive CNN performs worse on both test and training error than a 20-layer model

**Solution:** use network layers to fit a **residual mapping** instead of direct mapping
- In ResNet, we add skip connections that bypass layers: $\text{output} = F(x) + x$
- If layers are not useful for a certain task, the residual mapping will learn **identity mappings** ($F(x) \approx 0$)
	- Effectively allows you to skip unnecessary layers instead of optimizing them, which is easier for optimization
- If we wanted identity mappings:
	- In traditional CNN, we have to learn weights that precisely transform input to output (difficult)
	- In ResNet, we can just have the residual function F(x) learn to output values close to zero, and the skip connection automatically preserves the input
![](../../attachments/Pasted%20image%2020250419153825.png)

ResNet used **residual connections** to create very **deep networks**, significantly improving performance
![](../../attachments/Pasted%20image%2020250419154821.png)

# 4 Weight Initialization
If we initialize weights **too small**, the activations will eventually **vanish** as we go deeper
![](../../attachments/Pasted%20image%2020250419155923.png)

If we initialize weights **too large**, the activations will **explode** as we go deeper
![](../../attachments/Pasted%20image%2020250419155938.png)

## 4.1 Kaiming Initialization
**Key Idea:** Weight initialization scales with layer dimensions to maintain healthy signal flow
- Larger layers (more neurons) mean each neuron has more inputs
	- If we used fixed initialization, layers with more inputs would have **larger output variance**
	- By **scaling weights based on input size**, we ensure consistent signal strength
	- Common to use $\sigma = \sqrt{2/D_\text{in}}$ for ReLU networks
		- For CNNs, $D_\text{in} = \text{kernel\_size}^2 \times \text{input\_channels}$ (number of weights per filter)
		- ReLU "turns off" roughly half the neurons (sets to zero). This cuts the signal strength in half
		- Compensates by using sqrt(2/n) instead of the traditional sqrt(1/n)
- Think of it like adjusting speaker volume based on how many speakers you have
	- With more speakers (neurons), each one needs to be quieter to avoid overwhelming noise
	- With fewer speakers, each can be louder to maintain the same overall volume
![](../../attachments/Pasted%20image%2020250419160444.png)

## 4.2 Does Normalization Solve This?
Normalization layers can help mitigate the effects of poor weight initialization, but they don't completely solve the problem:
- **Partial solution**: Normalization layers (like BatchNorm, LayerNorm) can reduce the harmful effects of bad initialization by stabilizing activations
	- They normalize the distribution of layer outputs, keeping activations within a reasonable range
		- BatchNorm: normalizes each feature across the batch dimension to zero mean and unit variance
		- LayerNorm: normalizes all features for each example independently
	- This normalization prevents signals from growing too large or becoming too small
- **Still suboptimal**: Despite normalization, models with poor initialization:
	- Train slower (require more epochs to converge)
	- Often reach worse final performance
	- May struggle with optimization in very deep networks
- **Best practice**: Use proper initialization (like Kaiming) even with normalization layers
	- The combination of good initialization and normalization typically yields the best results
	- For some tasks, normalization may actually hurt performance if the exact magnitude of input values is important

# 5 How to Train CNNs?
## 5.1 Data Preprocessing
We want to **normalize** our data so that the network is easier to optimize
![](../../attachments/Pasted%20image%2020250419161126.png)

## 5.2 Data Augmentation
Recall that with **regularization**, we add randomness during training and average it out during testing
![](../../attachments/Pasted%20image%2020250419161214.png)

We can do the same thing to **augment** our data to improve generalization
- Apply some transformations (e.g. randomness) to make our data look different
- This effectively creates a larger dataset that we can train on
![](../../attachments/Pasted%20image%2020250419161322.png)

Common data augmentation ideas include horizontal flips, random crops and scales, color jitter, etc
- **Main Idea:** images should still be recognizable to a human ⟶ generalize model more
![](../../attachments/Pasted%20image%2020250419161358.png)
![](../../attachments/Pasted%20image%2020250419161412.png)

## 5.3 Transfer Learning
**Key Idea:** if target domain has less data, use pre-trained models on similar tasks to start with
- For small datasets: reinitialize the final classification layer and fine tune it on the target task
- For large datasets: use pre-trained model as initialization, and fine tune entire model on target task
![](../../attachments/Pasted%20image%2020250419161750.png)
![](../../attachments/Pasted%20image%2020250419161854.png)

## 5.4 Overfitting and Underfitting
**Not Training Enough**: train and val accuracies are both low but there is still agap
- Solutions: Train for more epochs to allow the model to fully learn
![](../../attachments/Pasted%20image%2020250419162005.png)

**Overfitting**: large gap between high training accuracy and lower validation accuracy
- Solutions:
	- Add regularization (L1/L2 weight penalties, dropout, data augmentation)
	- Collect more training data to expose model to more examples
	- Use a simpler model with fewer parameters to reduce capacity for memorization
- Non-solutions:
	- Training longer: This will only widen the gap as the model further memorizes training data
	- Adding more complex features: This typically increases overfitting
![](../../attachments/Pasted%20image%2020250419162012.png)

**Underfitting**: train and val accuracies are both low and close to each other
- Solutions:
	- Train the model for more epochs if learning curves still show improvement
	- Use a more complex/powerful model architecture with more parameters
	- Reduce regularization strength if it's too restrictive
- Non-solutions:
	- Adding more data: The model is already struggling to learn patterns from existing data
	- Increasing regularization: This would further restrict the model's learning capacity
![](../../attachments/Pasted%20image%2020250419162544.png)

## 5.5 Hyperparameter Selection
**Step 1:** check initial loss
**Step 2:** overfit a small sample
**Step 3:** find LR that makes loss go down
- Use the architecture from step 2 with *all training data*.
- Turn on small weight decay and find a LR that drops loss within ~100 iterations
- Common LRs to try: 1e-1, 1e-2, 1e-3, 1e-4, 1e-5

**Step 4:** coarse *grid* of hyperparameters that you train for ~1-5 epochs
**Step 5:** refine grid and train longer
**Step 6:** look at loss and accuracy curves (from previous section) to diagnose
**Step 7:** GOTO step 5 and repeat

### 5.5.1 Random and Grid Search
**Grid Search**: Evaluates all combinations of hyperparameters from predefined sets
- **Pros**: Thorough exploration of parameter space, guaranteed to find best combination in the grid
- **Cons**: Computationally expensive
**Random Search**: Randomly samples hyperparameter combinations from defined ranges
- **Pros**: More efficient than grid search, better coverage of parameter space with fewer trials
- **Cons**: May miss optimal combinations by chance, less systematic
**Key Insight**: Random search often outperforms grid search because:
- Some hyperparameters matter more than others
- Random search tests more values for each important parameter
- Grid search wastes resources testing many combinations of less important parameters
![](../../attachments/Pasted%20image%2020250419163024.png)
