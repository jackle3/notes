
# 1 Image Classification
> [!NOTE] Given an image and a set of possible labels, assign one of those labels to that image.

**Challenges**
* An image is just a tensor of integers between (0, 255) ⟶ eg. a matrix shaped (800, 600, 3)
* **Viewpoint variation**: when the camera moves, the entire image tensor changes
* **Illumination**: images of the same object under different lighting looks significantly different
* **Background and Occlusion**: there may be objects in the background or covering the object
* **Deformation**: the same object (e.g. a cat) does not have a set shape, can be sitting, etc
* **Intraclass variation**: different breeds of cat looks different
* **Context**: objects of one class might look like another class under visual context

**Machine Learning: Data-Driven Approach**
1. Collect a dataset of images and labels (many examples of each class)
2. Use ML algorithms to train a classifier
3. Evaluate the classifer on new images, repeat

# 2 Nearest Neighbor Classifier
This is the easiest form of classification:
* **Training O(1):** memorize all the images and labels![](../../attachments/Pasted%20image%2020250405143314.png)
* **Predict O(n):** find closest train image and predict label of nearest image![](../../attachments/Pasted%20image%2020250405143321.png)
This is not ideal ⟶ we want classifiers that are fast in inference/prediction; slow training is fine because we do that once.

## 2.1 K-Nearest Neighbors
Instead of copying label from nearest neighbor, take majority vote from K closest points
* This makes the algorithm more robust to outliers (notice yellow region in $k=1$)
![](../../attachments/Pasted%20image%2020250405143524.png)

## 2.2 Hyperparameters
In nearest neighbors, the hyperparameters are:
1. What is the best value of $k$ to use?
2. What is the best distance to use?
The answer is problem/dataset dependent ⟶ must try and see what works best

### 2.2.1 Distance Metric
The performance of the algorithm depends on the distance metric used
![](../../attachments/Pasted%20image%2020250405143605.png)

### 2.2.2 Setting Hyperparameters
We generally split the data into train, test, and validation sets
* We can also perform K-fold cross validation
![](../../attachments/Pasted%20image%2020250405143931.png)
![](../../attachments/Pasted%20image%2020250405143954.png)

## 2.3 Issues with Nearest Neighbors
* Classification is done via pixel-wise differences ⟶ images that look like each other pixel-wise will be classified same, even if the objects are semanticaly different
	* Notice the fourth row with the frog and the cat
![](../../attachments/Pasted%20image%2020250405144625.png)

# 3 Linear Classifier
## 3.1 Parametric Approach
We learn **parameters** $W$ and $b$ to apply to the image ⟶ directly transform from image to scores
* The bias is input independent ⟶ it allows model to output non-zero scores even when features are zero or shift decision boundaries away from the origin
![](../../attachments/Pasted%20image%2020250405144819.png)

### 3.1.1 Algebraic Viewpoint
By multiplying the image with the weight matrix $W$, we transform the image into a vector of scores: likelihood of each class
![](../../attachments/Pasted%20image%2020250405144919.png)

### 3.1.2 Visual Viewpoint
The classifier attempts to learn one template per class
![](../../attachments/Pasted%20image%2020250414151435.png)

### 3.1.3 Geometric Viewpoint
The linear classifier is finding **linear hyperplanes** that separate classes
* You can see why the bias is useful here ⟶ without it, all hyperplane decision boundaries would have to go thru origin
![](../../attachments/Pasted%20image%2020250405145253.png)

## 3.2 Hard Cases for Linear Classifiers
By definition, linear classifiers can only separate classes using linear boundaries
![](../../attachments/Pasted%20image%2020250405145355.png)

## 3.3 Loss Functions
The loss quantifies our unhappiness with the scores across the training data ⟶ we want to minimize the loss
![](../../attachments/Pasted%20image%2020250405145535.png)

### 3.3.1 Softmax Classifier
Softmax allows us to interpret raw classifier scores as probabilities
* Softmax function: $P(y_i|x_i) = \frac{e^{s_{y_i}}}{\sum_j e^{s_j}}$ where $s$ are the raw scores

Using this probability distribution, we can compute the **cross-entropy loss**
* **Cross-entropy loss** measures how much predicted distribution differs from true distribution
	* In classification, the true distribution is one-hot encoded ⟶ only one class is 1, rest are 0
	* Loss is thus the negative log probability of the correct class: $L_i = -\log(P(y_i|x_i))$
![](../../attachments/Pasted%20image%2020250405145756.png)

This is mathematically equivalent to the **Kullback-Leibler divergence** between the predicted and true distribution: $D_{KL}(p_{true} || p_{pred})$
* KL divergence is defined as: $D_{KL}(p || q) = \sum_i p(i) \log\frac{p(i)}{q(i)}$
* For one-hot encoded labels where $p(y_i) = 1$ and $p(j) = 0$ for $j \neq y_i$:
$$
D_{KL}(p_{true} || p_{pred}) = 1 \cdot \log\frac{1}{p_{pred}(y_i)} + \sum_{j \neq y_i} 0 \cdot \log\frac{0}{p_{pred}(j)}
$$
* This simplifies to: $D_{KL}(p_{true} || p_{pred}) = -\log(p_{pred}(y_i))$
* Which is exactly the cross-entropy loss for classification

**Question:** What is the min/max possible softmax loss $L_i$?
* Recall that $L_i = -\log(P(y_i|x_i)) = -\log\frac{e^{s_{y_i}}}{\sum_j e^{s_j}}$
* Min: 0 ⟶ when correct class has infinitely higher score than others, $P(y_i|x_i) \approx 1$, so $L_i = -\log(1) = 0$
* Max: $\infty$ ⟶ when correct class has infinitely lower score than others, $P(y_i|x_i) \approx 0$, so $L_i = -\log(0) = \infty$

**Question:** At initialization, all $s_j$ will be approximately equal ⟶ what is the loss assuming $C$ classes?
* If all scores are equal, then $P(y_i|x_i) = \frac{e^{s_{y_i}}}{\sum_j e^{s_j}} = \frac{e^{s_{y_i}}}{C \cdot e^{s_{y_i}}} = \frac{1}{C}$
* Therefore, $L_i = -\log(\frac{1}{C}) = \log(C)$

## 3.4 SVM Classifier
![](../../attachments/Pasted%20image%2020250413141439.png)
![](../../attachments/Pasted%20image%2020250413141500.png)
![](../../attachments/Pasted%20image%2020250413141559.png)
![](../../attachments/Pasted%20image%2020250413141604.png)
![](../../attachments/Pasted%20image%2020250413141521.png)
![](../../attachments/Pasted%20image%2020250413141619.png)
![](../../attachments/Pasted%20image%2020250413141728.png)
