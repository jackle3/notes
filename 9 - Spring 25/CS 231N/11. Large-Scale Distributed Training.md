# 1 Open Source Models
OpenAI started the trend of not sharing model details
LLama is kind of like the SOTA in open source models right now
![](../../attachments/Pasted%20image%2020250514164422.png)

# 2 GPU Hardware
GPUs originally intended for graphics ⟶ now a general parallel processor used a lot in deep learning
* Has compute cores that have a lot of compute ⟶ each compute core has 132 SMs, which is like one core
* Also has a lot of HBM memory, but also a large memory hierarchy
![](../../attachments/Pasted%20image%2020250514164511.png)
![](../../attachments/Pasted%20image%2020250514164529.png)
## 2.1 Inside the SM
* There are specialized cores in each SM
	* FP32 core computes ax + b ⟶ 2 flops per cycle per core
	* Tensor cores computes matmul AX + B ⟶ 1024 flops per cycle per tensor core
		* Usually uses mixed precision ⟶ inputs and multiplication is done in 16 bit, accumulation and addition is in 32 bit
	* Note: **the main throughput is in the tensor cores**
![](../../attachments/Pasted%20image%2020250514164623.png)

## 2.2 GPUs Have Gotten Much Faster
This is the answer to the question: **why has AI gotten so good in the last 10 years?**
* We've gotten through a 1000 fold increase in computation over the last 12 years
* Researchers can now take advantage of this increase in compute to scale and improve much more
* ***This 1000x improvement in compute is the major driver of improvement in deep learning***
![](../../attachments/Pasted%20image%2020250514165412.png)

# 3 GPU Clusters
**In addition to single GPU improvement, we've also managed to spread training across thousands of GPUs at once ⟶ exponentially more compute**
![](../../attachments/Pasted%20image%2020250514165439.png)
![](../../attachments/Pasted%20image%2020250514165628.png)
![](../../attachments/Pasted%20image%2020250514165632.png)
![](../../attachments/Pasted%20image%2020250514165845.png)
![](../../attachments/Pasted%20image%2020250514165901.png)
![](../../attachments/Pasted%20image%2020250514171029.png)
* Even on these giant clusters, training runs take on the order of months to fully train a model like Llama3-405B
* Bigger models like GPT5 might train on the order of years for something like this

# 4 Other Training Architectures
![](../../attachments/Pasted%20image%2020250514171112.png)
![](../../attachments/Pasted%20image%2020250514171821.png)

# 5 How to Train on Lots of GPUs
To train on lots of GPUs, we want to split up the computation via different types of parallelism
* A transformer is just a stack of L layers, each of which operate on tensors shaped (B, S, D)
* Each type of parallelism splits across these dimensions
![](../../attachments/Pasted%20image%2020250514171849.png)

# 6 Data Parallelism
Recall: Loss is usually averaged over a minibatch of $N$ samples
Idea: Use a minibatch of $MN$ samples, split across $M$ GPUs
* This works because gradients are linear, meaning the gradient of a sum equals the sum of gradients:
$$
\nabla(\sum_i f_i) = \sum_i \nabla f_i
$$
* Each GPU computes gradients on its own data subset, then we can simply average these gradients to get the correct overall gradient
* This linearity property allows us to distribute the computation across GPUs without changing the mathematical result, only requiring communication during the gradient averaging step
* Each GPU processes $N$ samples independently, then results are averaged (all-reduce operation)
![|500](../../attachments/Pasted%20image%2020250514172542.png)
![](../../attachments/Pasted%20image%2020250516134835.png)

## 6.1 FSDP
Problem: Model size constrained by GPU memory.
* Each weight needs 4 numbers (weight, grad, Adam $\beta_1$, $\beta_2$). Each number needs 2 bytes.
* 1B params takes 8GB; 10B params fills up 80GB GPU.
Solution: Split model weights across GPUs
* In the example below, we're splitting the weights across two different GPUs
	* GPU1 stores $W_1$ and $W_2$, and GPU2 stores $W_3$ and $W_4$
* Before forwards for each layer, the weights are broadcasted
![](../../attachments/Pasted%20image%2020250516134942.png)
![](../../attachments/Pasted%20image%2020250516135002.png)
![](../../attachments/Pasted%20image%2020250516135025.png)
![](../../attachments/Pasted%20image%2020250516135039.png)
![](../../attachments/Pasted%20image%2020250516135132.png)
There are three sequential things happening during backward pass:
1. Communicate the weights $W_i$ to all GPUs
2. Once they get the weights, compute the gradients $\frac{\partial L_g}{\partial W_i}$ for that layer
3. They then communicate the partial gradients to the GPU that owns the weight, which computes the full gradient $\frac{\partial L}{\partial W_i}$. Then the owner of the weight updates its weights (no communication)

The steps can all happen in parallel in the steady state:
1. Some GPUs will be prefetching the weights for layer $i - 1$
2. Some GPUs will be computing the backward pass for layer $i$
3. Some GPUs will be aggregating the gradients and performing the weight update for layer $i+1$

## 6.2 HSDP
Going even further, we can have a hybrid sharding
![](../../attachments/Pasted%20image%2020250516135255.png)

## 6.3 Activation Checkpointing
![](../../attachments/Pasted%20image%2020250516135416.png)
**Problem:** Model activations can fill up memory.
* Llama3-405B Transformer has 126 layers, D=16,384, seq length 4096.
* Just FFN hidden activations need $2*126*(4*16384)*4096$ bytes = 63GB; plus need other activations.
**Solution:** Don’t keep all activations in memory; recompute them on the fly!
![](../../attachments/Pasted%20image%2020250516135508.png)
The forward and backward pass takes $O(N)$ compute and $O(N)$ memory
* To compute backward $G_4$ we need to keep $A_1$ thru $A_3$
* To compute backward $G_3$ we need to keep $A_1$ thru $A_2$
**Idea:** recompute activations during the backward pass
* Instead of keeping $A_1$ thru $A_3$ to compute $G_4$ we just recompute them
* **Problem:** this takes $O(N^2)$ compute but $O(1$) memory
**Idea:** don't recompute everything, save a checkpoint every $C$ layers
* If we have $C$ checkpoints, this takes $O(N^2 / C)$ comupte and $O(C)$ memory
* If we have $\sqrt{N}$ checkpoints, this takes $O(N\sqrt{N})$ compute and $O(\sqrt{N})$ memory

# 7 FLOPs Utilization
![](../../attachments/Pasted%20image%2020250516140039.png)
![](../../attachments/Pasted%20image%2020250516140124.png)

The Model FLOPs Utilization is a better metric ⟶ accounts for activation checkpointing, etc
![](../../attachments/Pasted%20image%2020250516140143.png)
![](../../attachments/Pasted%20image%2020250516140234.png)

# 8 Context Parallelism
This techniques splits parallelism on the sequence dimension!
* Often used for long-sequence finetuning
* Example: LLama3-405B training
	* Stage 1: seq_len=8192, no context-parallelism
	* Stage 2: seq_len=131,072, 16-way context-parallelism (8192 per GPU)
![](../../attachments/Pasted%20image%2020250516140323.png)
![](../../attachments/Pasted%20image%2020250516140412.png)
![](../../attachments/Pasted%20image%2020250516140423.png)
![](../../attachments/Pasted%20image%2020250516140436.png)

# 9 Pipeline Parallelism
In this technique, we split on the L dimension (layers of the model)
![](../../attachments/Pasted%20image%2020250516140549.png)
![](../../attachments/Pasted%20image%2020250516140703.png)
![](../../attachments/Pasted%20image%2020250516140742.png)

# 10 Tensor Parallelism
This splits on the hidden dimension of the model (recall it processes tensors shaped batchsz, seq_len, dim)
![](../../attachments/Pasted%20image%2020250516140826.png)
![](../../attachments/Pasted%20image%2020250516140846.png)
![](../../attachments/Pasted%20image%2020250516140921.png)
* With this, no need for communication after XW=Y!
* Each GPU computes one term of Z fully, then broadcasts to all other GPUs
	* They first compute $Y_i = XW_i$. Then they compute $Y_iU_i$ and broadcast that to all GPUs to compute the full $Z$

# 11 ND Parallelism
![](../../attachments/Pasted%20image%2020250516141102.png)
![](../../attachments/Pasted%20image%2020250516141108.png)

# 12 Summary
![](../../attachments/Pasted%20image%2020250516141130.png)
