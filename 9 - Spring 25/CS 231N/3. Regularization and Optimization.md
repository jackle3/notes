
# 1 Recap: Linear Classifiers
![](../../attachments/Pasted%20image%2020250413132355.png)
![](../../attachments/Pasted%20image%2020250413132412.png)

# 2 Regularization
Regularization pushes against fitting the data *too* well ⟶ stops us from fitting noise in the data
* We can express preferences over weights
* Make the model *simple* so it works on the test data
* Improve optimization by adding curvature

**High level goal:** do worse on training data to do better on unseen data
![](../../attachments/Pasted%20image%2020250413132621.png)

Simple examples of regularization include:
* L2 regularization: $R(W) = \sum_k \sum_l W_{k, l}^2$
* L1 regularization: $R(W) = \sum_k \sum_l |W_{k, l}|$
* Elastic net (L1 + L2): $R(W) = \sum_k \sum_l \beta W_{k, l}^2 + |W_{k, l}|$
More complex forms of regularization include:
* Dropout
* Batch normalization
* Stochastic depth, fractional pooling, etc

## 2.1 Expressing Preferences
Consider the following two weight matrices. Which would L1 and L2 regularization prefer?
$$
w_1 = [1, 0, 0, 0]
$$
$$
w_2 = [0.25, 0.25, 0.25, 0.25]
$$
* L2 would prefer $w_2$ ⟶ weights are more "spread out", squared sum is only $0.25^2 * 4 = 0.25$
* L1 has no preference ⟶ absolute sum is same in both weight arrays
	* In general, L1 prefers sparse weigth vectors (if it can be zero, it should be zero)

## 2.2 Summary so far
* We have some dataset of $(x, y)$
* We have a score function such as $s = f(x; W) = Wx$
* We have a loss function:
$$
L_i = -\log\frac{\exp(s_{y_i})}{\sum_j \exp(s_j)} \tag{Softmax}
$$
$$
L = \frac{1}{N} \sum_{i=1}^N L_i + R(W) \tag{Full loss}
$$
![](../../attachments/Pasted%20image%2020250413133711.png)

# 3 Optimization
In general, optimization is done by moving in the direction of the gradient!
* In multiple dimenisons, the **gradient** is the vector of partial derivatives along each dimension
* The *slope* in any direction is the dot product of the direction with the gradient
* The direction of steepest descent is the negative gradient

## 3.1 Numeric Gradient
One method to calculate the gradient is to do so numerically:
* For each dimension, apply the 1-dimensional derivative formula to calculate gradient wrt to that direction
$$
\frac{df(x)}{dx} = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
$$
* This is **slow** and you need to loop over all dimensions; it's also just an approximation (errors)

## 3.2 Analytic Gradient
Notice that the loss is simply a function of $W$. We can calculate $\nabla_W L$ directly.
* This is exact and fast, but harder to calculate. In practice, **always use analytic gradient** but check implementation using a numerical gradient.
![](../../attachments/Pasted%20image%2020250413134328.png)
![](../../attachments/Pasted%20image%2020250413134723.png)

## 3.3 Gradient Descent
![](../../attachments/Pasted%20image%2020250413191429.png)

# 4 Optimization Functions
## 4.1 Stochastic Gradient Descent
**Key Idea**: sample a minibatch, compute partial gradient, update parameters to move in direction of negative gradient (i.e. steepest descent)
![](../../attachments/Pasted%20image%2020250413134631.png)
![](../../attachments/Pasted%20image%2020250413134741.png)

### 4.1.1 Problems with SGD
1. **Poor conditioning**: When the loss function changes at different rates in different directions, gradient descent can struggle:![](../../attachments/Pasted%20image%2020250413134929.png)
2. **Local minima and saddle points**: gradient descent gets stuck because gradient is zero. This is very common in high-dimensional spaces
3. **Noisy**: Gradients come from minibatch, so they can be very noisy. Can lead to irregular paths.

## 4.2 SGD + Momentum
We can fix the above issues by including **momentum** in our SGD calculations
* **Key Idea**: continue moving in general direction as the previous iterations
![](../../attachments/Pasted%20image%2020250413135316.png)
![](../../attachments/Pasted%20image%2020250413135408.png)
![](../../attachments/Pasted%20image%2020250413135509.png)
![](../../attachments/Pasted%20image%2020250414162937.png)

## 4.3 RMSProp
RMSProp effectively adds a "per-parameter" or "adaptive" learning rates for each dimension, addresses the decaying learning rate issue of AdaGrad.
![](../../attachments/Pasted%20image%2020250413135532.png)

Because of the `/ (np.sqrt(grad_squared) + 1e-7)` term:
* Progress along "steep" directions is damped ⟶ prevent overshooting
* Progress along "flat" directions is accelerated ⟶ prevents getting stuck
![](../../attachments/Pasted%20image%2020250413140348.png)

## 4.4 Adam
![](../../attachments/Pasted%20image%2020250413140420.png)
The issue with this formulation is that in the initial step, the `second_moment` is close to zero. This leads to a *very large first step*. To fix, we add bias correction:
![](../../attachments/Pasted%20image%2020250413140603.png)
![](../../attachments/Pasted%20image%2020250413140644.png)

## 4.5 AdamW
In traditional Adam, the L2 regularization is calculated when you compute the gradient. In AdamW, we separate the regularization out and apply it at the end.
![](../../attachments/Pasted%20image%2020250413140810.png)

# 5 Learning Rate Decay
![](../../attachments/Pasted%20image%2020250413140916.png)
![](../../attachments/Pasted%20image%2020250413140944.png)
![](../../attachments/Pasted%20image%2020250413141003.png)
![](../../attachments/Pasted%20image%2020250413141036.png)
![](../../attachments/Pasted%20image%2020250413141040.png)
![](../../attachments/Pasted%20image%2020250413141047.png)

# 6 Second Order Optimization
Right now, we're using the gradient to form our approximation ⟶ this is a first-order optimization
![](../../attachments/Pasted%20image%2020250413141139.png)

Why not use a Hessian to form a second-order approximation?
![](../../attachments/Pasted%20image%2020250413141152.png)

**Second-order** is bad for deep learning because we need to compute a Hessian!
![](../../attachments/Pasted%20image%2020250413141213.png)

# 7 Summary
![](../../attachments/Pasted%20image%2020250413141238.png)
