
# 1 Recap
Recurrent neural networks allowed us to process **sequences of inputs**
![](../../attachments/Pasted%20image%2020250503152222.png)

# 2 Key Topics
![](../../attachments/Pasted%20image%2020250503152239.png)

# 3 Seq2Seq With RNNs
Example: **translating** from a sequence of English words to a sequence of Italian words!
Approach:
- Use an RNN to **encode** the input sequence into a context vector (e.g. last hidden state of encoder)
- Use a **decoder** RNN to autoregressively generate the output sequence given the context vector
	- Each RNN unit takes three inputs: prev hidden state, prev output as input, and context vector
![](../../attachments/Pasted%20image%2020250503152515.png)

## 3.1 Attention: Longer Contexts
**Problem:** the inputs sequence *bottlenecks* through fixed-length context vector $c$
- If $T = 1000$, we might not be able to summarize the entire sequence in the context $c$
**Solution:** allow context to look back at the input sequence on each step of output ⟶ **attention**

**Step 1:** compute *alignment scores* between decoder hidden state $s_0$ and encoder hidden states
![](../../attachments/Pasted%20image%2020250503153321.png)

**Step 2:** apply a softmax function to normalize scores into *attention weights*
- This outputs a predicted distribution over input tokens given the decoder hidden state
![](../../attachments/Pasted%20image%2020250503153330.png)

**Step 3:** compute the context vector as a weighted sum of the encoder hidden states
- **Inuition:** this context vector *attends* to more relevant parts of input depending on decoder RNN
![](../../attachments/Pasted%20image%2020250503153342.png)

**Step 4:** use context vector with autoregressive decoder input to get next hidden state $s_1$ and output $y_1$
![](../../attachments/Pasted%20image%2020250503153401.png)

**Step 5:** repeat ⟶ use $s_1$ to compute new context vector $c_2$ and use that to decode $y_2$
![](../../attachments/Pasted%20image%2020250503153702.png)
![](../../attachments/Pasted%20image%2020250503153711.png)

## 3.2 Attention Visualization
![](../../attachments/Pasted%20image%2020250503153823.png)
Each row of the visualization is the predicted distribution over input tokens given the current decoder state
- This lets us see which parts of input are most important for translation at each step
![](../../attachments/Pasted%20image%2020250503153959.png)

# 4 Attention
With the Seq2Seq RNN ⟶ the operations from one RNN unit to the next can become a **general operator**
![](../../attachments/Pasted%20image%2020250503154225.png)
The different parts of the encoder-decoder can be decomposed into vectors:
- Query vectors (decoder RNN states) ⟶ used to query encoder data vectors for output vector
- Data vectors (encoder RNN states) ⟶ used to attend over query to generate output
- Output vectors (context states)
![](../../attachments/Pasted%20image%2020250503154257.png)

## 4.1 Attention Layer
**Goal:** can we extract the attention layer from the RNN architecture?
![](../../attachments/Pasted%20image%2020250503154720.png)

**Step 1:** recall that first step is **computing *similarity*** between query $s_0$ and data vectors $\vec{h}$
- The simplest function for computing similarity is a **scaled dot product**
- Note: $X$ is of shape $N_q \times D_q$ due to the dot product
![](../../attachments/Pasted%20image%2020250503155309.png)

**Step 2:** we ideally want to have **multiple query vectors** to process many queries in parallel
- Idea: turn $q$ into a matrix $Q$ shaped $N_q \times D_q$ ⟶ use matmul instead of dot product
- To compute attention weights, compute softmax over the $N_x$ dimension (data vectors for each query)
![](../../attachments/Pasted%20image%2020250503155544.png)

**Step 3:** separate the key and value usages of the data vectors
- Currently, the same data vector computes both the attention scores and the output context vector
- These are two different roles ⟶ can we learn a projection of the data vectors to two different spaces to better handle these two roles?
	- Key vector: used to compute attention score, projection learned with $W_k$
	- Value vector: used to compute the context vector, project learned with $W_v$
- Note: now $X$ *no longer needs* to have the same shape as $Q$ ⟶ projection can change the shape
![](../../attachments/Pasted%20image%2020250503160123.png)

## 4.2 Cross-Attention Layer
What we've described so far is a cross-attention layer! It can be visualized like so:
- Softmax normalizes over the **columns**: each query predicts a distribution over the keys (i.e. data)
- Each output $Y_i$ is a linear combination of all values $V_i$ weighted by attention weights in column
**Cross-attention** produces **one output** for **each query**
- The inputs to the layer are both the queries and the data (i.e. why its called cross)
- The queries produce an output using a mix of information from the data
![](../../attachments/Pasted%20image%2020250503160544.png)

## 4.3 Self-Attention Layer
**Self-attention** produces **one output** for **each input**
- We **project each input vector** into three different spaces:
	- Query vectors: used to query other positions
	- Key vectors: used to be queried by other positions
	- Value vectors: used to create the output
$$
\underbrace{\begin{bmatrix} 
\mathbf{Q} & \mathbf{K} & \mathbf{V}
\end{bmatrix}}_{[\mathbf{N} \times 3\mathbf{D}_{out}]} = 
\underbrace{\mathbf{X}}_{[\mathbf{N} \times \mathbf{D}_{in}]} \underbrace{\begin{bmatrix} 
\mathbf{W}_Q & \mathbf{W}_K & \mathbf{W}_V
\end{bmatrix}}_{[\mathbf{D}_{in} \times 3\mathbf{D}_{out}]}
$$
- Each input position **attends to all other positions (including itself)** ⟶ allows information to flow directly between any positions in the sequence
![](../../attachments/Pasted%20image%2020250503160927.png)

## 4.4 Positional Encodings
Self-attention is **permutation equivariant** ⟶ $F(\sigma(X)) = \sigma(F(x))$
- When we permute the inputs, the outputs are the exact same but permuted the same way
- This implies that self-attention works on **unordered sets of vectors**
![](../../attachments/Pasted%20image%2020250503161712.png)

If the order of vectors are important, we can **add positional encodings** to the input
![](../../attachments/Pasted%20image%2020250503161739.png)

## 4.5 Masked Self-Attention Layer
**Key idea:** don't let vectors look ahead in the sequence
- Often used for language modeling where you want to predict the next word (no peeking in future)
![](../../attachments/Pasted%20image%2020250503161834.png)

## 4.6 Multiheaded Self-Attention Layer
This version is used everywhere in practice ⟶ compute $H$ heads of self-attention in parallel
![](../../attachments/Pasted%20image%2020250503162111.png)
![](../../attachments/Pasted%20image%2020250503162137.png)

## 4.7 Compute and Memory
> [!NOTE] Self-attention is just four matrix multiplies
> The computational steps for the four matrix multiplies in self-attention are:
> 1. QKV Projection: Project input X into query, key and value matrices
> 2. QK Similarity: Compute attention scores between positions
> 3. V-Weighting: Apply softmax and weight values
> 4. Output Projection: Project weighted values to output space

![](../../attachments/Pasted%20image%2020250503162312.png)

It requires $O(N^2)$ compute for $N$ inputs
- This is because the attention matrix computation ($Q \times K^T$) creates an $N \times N$ matrix

It requires $O(N^2)$ memory for $N$ inputs
- Between computing QK similarity and V-weighting, it needs to store the $H \times N \times N$ attention matrix
- Flash Attention reduces this to $O(N)$ memory by computing QK similarity and V-weighting simultaneously without storing the full attention matrix

# 5 Three Ways of Processing Sequences
![](../../attachments/Pasted%20image%2020250503163032.png)

# 6 Transformers
![](../../attachments/Pasted%20image%2020250503163327.png)
- Self-attention is the only interaction between vectors ⟶ *lets vectors talk and compare with each other*
- LayerNorm and MLP work independently on each vector ⟶ perform computation on each vector independently after they've talked to each other
- (+) Highly scalable and parallelizable ⟶ just 6 matmuls (4 from attention, 2 from MLPs)

## 6.1 Transformers for Language Modeling
![](../../attachments/Pasted%20image%2020250503163950.png)

## 6.2 Vision Transformers
Given an image, we:
1. Break the image into patches of 16x16x3
2. Flatten and project each batch independently into a vector
	- Another way to describe this is a 16x16 conv with stride 16, 3 input channels, D output chanenls
3. Give vectors as input to transformer without any masking
4. Transformer gives an output vector per patch
![](../../attachments/Pasted%20image%2020250503164151.png)

## 6.3 Tweaks to Transformers
A few tweaks since it was introduced include:
- **Pre-norm transformer**: move normalization inside residual so block can learn identity function
- **RMSNorm**: different normalization layer
- **SwiGLU MLP**: different MLP architecture
- **Mixture of Experts (MoE)**: learn $E$ different MLPs, use $A < E$ of them per token. This massively increases the number of learnable parameters, with only a modest increase in compute cost.
![](../../attachments/Pasted%20image%2020250503164305.png)
![](../../attachments/Pasted%20image%2020250503164324.png)
![](../../attachments/Pasted%20image%2020250503164347.png)
![](../../attachments/Pasted%20image%2020250503164402.png)
![](../../attachments/Pasted%20image%2020250503164451.png)
