\documentclass{article}
\usepackage[margin=0.2in]{geometry} % Super small margins
\usepackage{multicol}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{bbm}
\usepackage{xcolor}

\setlength{\parindent}{0pt} % No paragraph indentation
\setlength{\parskip}{1pt} % Minimal space between paragraphs
\setlength{\columnsep}{0.2in} % Space between columns

% Reduce spacing in lists
\setlist{noitemsep, topsep=0pt, parsep=0pt, partopsep=0pt, leftmargin=*}

% Reduce spacing in math environments
% \setlength{\abovedisplayskip}{3pt}
% \setlength{\belowdisplayskip}{3pt}
% \setlength{\abovedisplayshortskip}{1pt}
% \setlength{\belowdisplayshortskip}{1pt}

\expandafter\def\expandafter\normalsize\expandafter{%
    \normalsize%
    \setlength\abovedisplayskip{2pt}%
    \setlength\belowdisplayskip{2pt}%
    \setlength\abovedisplayshortskip{-8pt}%
    \setlength\belowdisplayshortskip{2pt}%
}

% Smaller section headers
\makeatletter
\renewcommand\section{\@startsection{section}{1}{\z@}%
                                  {-1.5ex \@plus -1ex \@minus -.2ex}%
                                  {0.8ex \@plus.2ex}%
                                  {\normalfont\small\bfseries}}
\makeatother

\begin{document}
\fontsize{6pt}{7pt}\selectfont % Size 6 font with 7pt line height

\begin{multicols}{3} % Three columns for maximum space utilization

\section*{\underline{Linear Classification}}
\textbf{Score function}: $s = f(x; W) = Wx + b$\\
\textbf{Softmax classifier}: $P(y_i|x_i) = \frac{e^{s_{y_i}}}{\sum_j e^{s_j}}$\\
\textbf{Cross-entropy loss}: $L_i = -\log(P(y_i|x_i))$
\begin{itemize}
\item Min loss: 0 (when $P(y_i|x_i)=1$)
\item Max loss: $\infty$ (when $P(y_i|x_i) \approx 0$)
\item Random initialization: $\log(C)$ for $C$ classes
\end{itemize}

\textbf{SVM loss}: $L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + \Delta)$
\begin{itemize}
\item $\Delta$ is margin parameter (typically $\Delta = 1$)
\item Wants correct class score higher than incorrect class scores by at least $\Delta$
\item Geometric interpretation: Linear hyperplanes separating classes
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item Any FC network can be expressed as a CNN (with 1×1 filters) and vice versa
\item Loss gradients flow from softmax loss to weight matrix proportional to input
\item Linear classifiers can't solve XOR problems (need nonlinearities)
\end{itemize}

\section*{\underline{Regularization}}
\textbf{Full loss}: $L = \frac{1}{N} \sum_{i=1}^N L_i + \lambda R(W)$\\
\textbf{Types}:
\begin{itemize}
\item L2: $R(W) = \sum_k \sum_l W_{k,l}^2$ (prefers diffuse weights)
\item L1: $R(W) = \sum_k \sum_l |W_{k,l}|$ (promotes sparsity)
\item Elastic Net: $R(W) = \sum_k \sum_l \beta W_{k,l}^2 + |W_{k,l}|$
\item Dropout: Randomly zero outputs during training (scale by p at test)
\item Normalization adds regularization effect due to noise in batch stats
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item Early stopping: end training when val error increases
\item Dropout forces redundant representations, acts like ensemble averaging
\item Regularizing bias terms is generally avoided (mainly regularize weights)
\item Data augmentation: Adding transformed training examples
\end{itemize}

\section*{\underline{Optimization Algorithms}}
\textbf{SGD}: $w_{t+1} = w_t - \alpha \nabla L(w_t)$\\
\textbf{SGD+Momentum}: 
\begin{align*}
v_{t+1} &= \rho v_t + \nabla L(w_t) \tag{typically $\rho = 0.9$ or $0.99$}\\
w_{t+1} &= w_t - \alpha v_{t+1}
\end{align*}

\textbf{RMSProp}:
\begin{align*}
\text{grad\_squared} &= \beta \cdot \text{grad\_squared} + (1-\beta) \cdot (\nabla L(w_t))^2\\
w_{t+1} &= w_t - \frac{\alpha \cdot \nabla L(w_t)}{\sqrt{\text{grad\_squared}} + \epsilon}
\end{align*}

\textbf{Adam}:
\begin{align*}
m_t &= \beta_1 m_{t-1} + (1-\beta_1)\nabla L(w_t) \tag{momentum}\\
v_t &= \beta_2 v_{t-1} + (1-\beta_2)(\nabla L(w_t))^2 \tag{RMSProp}\\
\hat{m}_t &= m_t / (1-\beta_1^t) \tag{bias correction}\\
\hat{v}_t &= v_t / (1-\beta_2^t) \tag{bias correction}\\
w_{t+1} &= w_t - \alpha \hat{m}_t / (\sqrt{\hat{v}_t} + \epsilon)
\end{align*}

\textbf{Learning Rate Decay}:
\begin{itemize}
\item Decrease LR over time (step, cosine, linear, etc.)
\item Linear warmup: increase LR from 0 over first few steps, prevent exploding loss
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item SGD issues: poor conditioning, getting stuck in local minima/saddle points, noisy
\item Momentum overcomes oscillations and escape poor local minima, continues moving in prev direction
\item RMSProp adds per-parameter learning rate, addresses AdaGrad's decaying learning rate issue
\item Adam combines momentum and RMSProp
\item AdamW separates weight decay from gradient update for better regularization
\item Second-order methods: $\theta^* = \theta_0 - \alpha H^{-1} \nabla L(\theta_0)$\\
Better updates, $O(N^2)$ mem and $O(N^3)$ time to invert
\end{itemize}

\section*{\underline{Neural Networks}}
\textbf{MLP}: $f = W_2 \text{max}(0, W_1 x + b_1) + b_2$\\
$x \in \mathbb{R}^D$, $W_1 \in \mathbb{R}^{H \times D}$, $b_1 \in \mathbb{R}^H$, $W_2 \in \mathbb{R}^{C \times H}$, $b_2 \in \mathbb{R}^C$\\
\textbf{Activation Functions}:
\begin{itemize}
\item ReLU: $f(x) = \max(0, x)$
\item Leaky ReLU: $f(x) = \max(\alpha x, x)$ with small $\alpha$
\item ELU: $f(x) = \begin{cases} x & \text{if } x \geq 0 \\ \alpha(e^x - 1) & \text{if } x < 0 \end{cases}$
\item GELU: $f(x) = x \cdot \Phi(x)$
\item Sigmoid: $\sigma(x) = \frac{1}{1+e^{-x}}$
\item Tanh: $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item ReLU has zero grad with negative inputs (dying ReLU)
\item Leaky ReLU (and variants) always has non-zero slope
\item Tanh has zero-centered outputs (sigmoid has $\mu = 0.5$)
\item Without nonlinear activations, deep networks reduce to linear models
\item Deeper networks can represent more complex functions with fewer parameters (more non-linearities)
\end{itemize}

\section*{\underline{Backpropagation}}
\textbf{Gradient flow}: Upstream $\times$ Local = Downstream\\
\textbf{Vector derivatives}:
\begin{itemize}
\item $d_x f(x)$ has same shape as $x$
\item Apply chain rule using matrix calculus
\item Matmul: $\frac{\partial}{\partial X}(XW) = W^T$ and $\frac{\partial}{\partial W}(XW) = X^T$
\item Each element $X_{n,d}$ affects the whole row $Y_{n}$
\item Backprop: X: [N, D], W: [D, M], Y: [N, M]
$$\frac{\partial L}{\partial X} = \left(\frac{\partial L}{\partial Y}\right)W^T \in [N, D]  \quad\frac{\partial L}{\partial W} = X^T\left(\frac{\partial L}{\partial Y}\right) \in [D, M]$$
\end{itemize}
\textbf{Special derivatives}:
\begin{itemize}
\item Sigmoid: $d_x \sigma(x) = \sigma(x)(1-\sigma(x))$
\item Tanh: $d_x \tanh(x) = 1 - \tanh^2(x)$
\item ReLU: $d_x \text{ReLU}(x) = \mathbbm{1}(x > 0)$
\item Max: $d_x \max(x, y) = \mathbbm{1}(x > y)$
\item Softmax: $\frac{d p_i}{d s_j} = p_i(\mathbbm{1}(i=j) - p_j)$ where $p = \frac{e^{s}}{\sum_k e^{s_k}}$
\item Cross-entropy: $d_{s_i} \left(-\sum_j y_j\log(p_j)\right) = p_i - y_i$
\item Huber Loss: $d_x L_{\delta}(x,y) = \begin{cases} x-y & \text{if } |x-y| < \delta \\ \delta \cdot \text{sign}(x-y) & \text{otherwise} \end{cases}$
\item L1 Loss: $d_x |x-y| = \text{sign}(x-y)$
\end{itemize}

\textbf{Key Backpropagation Concepts}:
\begin{itemize}
\item Vanishing gradients: gradients become too small in deep networks (esp. with sigmoid/tanh)
\item Exploding gradients: gradients become too large (common in RNNs)
\item Gradient clipping: Cap gradient magnitude to prevent explosion
\end{itemize}

\section*{\underline{Convolutional Neural Networks}}
\textbf{Conv Layer Summary}:
\begin{itemize}
\item \textbf{Hyperparameters}:
\begin{itemize}
\item Kernel size: $K_H \times K_W$
\item Number filters: $C_{\text{out}}$
\item Padding: $P = (K-1)/2$ (same padding)
\item Stride: $S$
\end{itemize}
\item \textbf{Weight matrix}: $C_{\text{out}} \times C_{\text{in}} \times K_H \times K_W$
\item \textbf{Bias vector}: $C_{\text{out}}$
\item \textbf{Input}: $C_{\text{in}} \times H \times W$
\item \textbf{Output activation}: $C_{\text{out}} \times H' \times W'$ where
$$
[H', W'] = \frac{[H, W] - K_{[H, W]} + 2P}{S} + 1
$$
\end{itemize}

\textbf{Advantages}:
\begin{itemize}
\item Parameter sharing: Same filter applied across image
\item Sparse connectivity: Each output depends on small local region
\item Translation equivariance: Shifting input shifts output
\end{itemize}

\textbf{Pooling layers}: 
\begin{itemize}
\item Given input C×H×W, downsample each 1×H×W plane
\item Max pooling: Take maximum value in window
\item Average pooling: Average values in window
\item Reduces spatial dimensions, increases receptive field
\end{itemize}

\textbf{Receptive field}: Region of input that affects output
\begin{itemize}
\item For K×K filters, RF grows by $(K-1)$ per layer
\item With $L$ layers and $S=1$, RF is $1 + L * (K - 1)$
\item In general, $R_0 = 1$ and $R_l = R_{l-1} + (K-1) \times S$
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item Multiple 3×3 filters better than single large filter: fewer parameters, more nonlinearities
\item 1×1 conv: same H/W, dim reduction across channels
\item Dilated convolutions: expand receptive field without increasing parameters
\end{itemize}

\section*{\underline{Normalization Techniques}}
\textbf{Batch Normalization}:
\begin{align*}
\mu_c &= \text{mean of feature values across batch for channel c}\\
\sigma_c &= \text{standard deviation across batch for channel c}\\
y &= \gamma_c \cdot \frac{x - \mu_c}{\sigma_c} + \beta_c \quad \text{(scale and shift)}
\end{align*}
Normalizes across batch dimension for each channel

\textbf{Layer Normalization}:
\begin{align*}
\mu_n &= \text{mean across all channels for sample n}\\
\sigma_n &= \text{standard deviation across all channels for sample n}\\
y &= \gamma_c \cdot \frac{x - \mu_n}{\sigma_n} + \beta_c \quad \text{(scale and shift)}
\end{align*}
Normalizes across channel dimension for each sample

\textbf{Instance Normalization}:
\begin{align*}
\mu_{n,c} &= \text{spatial mean for channel c of sample n}\\
\sigma_{n,c} &= \text{spatial standard deviation for channel c of sample n}\\
y &= \gamma_c \cdot \frac{x - \mu_{n,c}}{\sigma_{n,c}} + \beta_c \quad \text{(scale and shift)}
\end{align*}
Normalizes each channel independently for each sample

\textbf{Key concepts}:
\begin{itemize}
\item BatchNorm: Used in CNNs, must track running stats for inference
\item LayerNorm: Used in transformers, no dependence on batch statistics
\item Instance Norm: Used in style transfer, normalizes each feature map
\item Group Norm: Compromise between Layer and Instance Norm
\end{itemize}

\section*{\underline{CNN Architectures}}
\textbf{VGG}:
\begin{itemize}
\item Multiple 3×3 convs followed by max-pooling
\item Multiple 3×3 filters have same receptive field as larger filter with fewer parameters
\item Uniform design: doubles channels after each pooling
\end{itemize}

\textbf{ResNet}:
\begin{itemize}
\item Skip connections: $\text{output} = F(x) + x$
\item Allow deeper networks by learning residual mapping
\item Solves vanishing gradient problem in deep nets
\item Typical block: Conv → BN → ReLU → Conv → BN → Add → ReLU
\end{itemize}

\textbf{Bottleneck layer}:
\begin{itemize}
\item 1×1 conv to reduce channels, 3×3 conv, 1×1 conv to expand
\item Reduces computation by decreasing channels in 3×3 conv
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item Network design trade-offs: depth vs. width vs. resolution
\item Skip connections help gradient flow and enable deeper networks
\item Deeper networks generally need more regularization
\item ResNet skip connections: $O_l = I_l + F(I_l)$ not $O_l = I_l * F(I_l)$
\end{itemize}

\section*{\underline{Weight Initialization}}
\textbf{Xavier/Glorot}: $W \sim \mathcal{N}(0, \sqrt{\frac{2}{n_{in} + n_{out}}})$
\begin{itemize}
\item For tanh/sigmoid activations
\item Maintains variance across linear layers
\end{itemize}

\textbf{Kaiming initialization}: $W \sim \mathcal{N}(0, \sqrt{\frac{2}{D_{in}}})$ for ReLU
\begin{itemize}
\item For ReLU activations (accounts for half being zeroed)
\item For CNN: $D_{in} = \text{kernel\_size}^2 \times \text{input\_channels}$
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item Poor initialization can cause vanishing/exploding gradients
\item Initialization in deep nets is crucial for trainability
\item Even with good normalization, bad initialization slows training
\item Initialization should match the activation function
\end{itemize}

\section*{\underline{Training Techniques}}
\textbf{Data Augmentation}:
\begin{itemize}
\item Increases dataset size/diversity without new data
\item Robustness to variations (position, lighting)
\item Common techniques: flips, crops, color jitter, rotations
\end{itemize}

\textbf{Transfer Learning}:
\begin{itemize}
\item Small dataset: Freeze pretrained model, retrain final layers
\item Medium dataset: Freeze early layers, fine-tune later layers
\item Large dataset: Initialize with pretrained weights, fine-tune all layers
\end{itemize}

\textbf{Diagnostics}:
\begin{itemize}
\item Underfitting: Low train/val accuracy, small gap
\item Overfitting: High train accuracy, low val accuracy, large gap
\item Not training enough: Low train/val accuracy with gap
\end{itemize}

\textbf{Hyperparameter selection}:
\begin{itemize}
\item Random search usually better than grid search
\item Check initial loss, overfit small sample first
\item Find LR that makes loss decrease within ~100 iterations
\end{itemize}

\section*{\underline{Loss Functions}}
\textbf{Cross-entropy}: $L = -\sum_i y_i \log(\hat{y}_i)$
\begin{itemize}
\item For classification problems
\item Measures dissimilarity between two probability distributions
\end{itemize}

\textbf{KL Divergence}: $D_{KL}(p||q) = \sum_i p_i \log\frac{p_i}{q_i} = \sum_i p_i(\log p_i - \log q_i)$
\begin{itemize}
\item Rewrite as subtraction for numerical stability
\item Not symmetric: $D_{KL}(p||q) \neq D_{KL}(q||p)$
\end{itemize}

\textbf{Smooth L1/Huber Loss}:
\begin{align*}
L_{\delta}(x, y) = \begin{cases}
\frac{1}{2}(x-y)^2 & \text{if } |x-y| < \delta \\
\delta(|x-y| - \frac{1}{2}\delta) & \text{otherwise}
\end{cases}
\end{align*}
\begin{itemize}
\item Combines MSE (near zero) and L1 (for outliers)
\item Differentiable everywhere, robust to outliers
\end{itemize}

\textbf{Triplet margin loss}: $L(a,p,n) = \max\{d(a,p) - d(a,n) + \text{margin}, 0\}$
\begin{itemize}
\item Used in contrastive learning
\item Pushes anchor (a) closer to positive (p) than negative (n)
\item Margin controls separation between positive and negative pairs
\end{itemize}

\section*{\underline{Recurrent Neural Networks}}
\textbf{Vanilla RNN}:
\begin{align*}
h_t &= \tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)\\
y_t &= W_{hy}h_t + b_y
\end{align*}

\textbf{LSTM}:
\begin{align*}
f_t &= \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)\\
i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)\\
\tilde{C}_t &= \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)\\
C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t\\
o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)\\
h_t &= o_t \odot \tanh(C_t)
\end{align*}

\textbf{RNN Applications}:
\begin{itemize}
\item Language modeling: Predict next token in sequence
\item Image captioning: CNN feature extractor + RNN decoder
\item Sequence-to-sequence: Translation, summarization
\end{itemize}

\textbf{Training RNNs}:
\begin{itemize}
\item Backpropagation through time (BPTT)
\item Truncated BPTT for long sequences
\item Gradient clipping to prevent explosion
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item RNNs can process variable-length sequences
\item Vanishing/exploding gradients limit learning long-term dependencies
\item LSTM addresses vanishing gradients via cell state pathway
\item RNNs sequential processing limits parallelization
\end{itemize}

\section*{\underline{Attention Mechanism}}
\textbf{Inputs}:
\begin{itemize}
\item Input sequence: $X \in \mathbb{R}^{N \times D_{in}}$
\item Query matrix: $W_Q \in \mathbb{R}^{D_{in} \times D_{out}}$
\item Key matrix: $W_K \in \mathbb{R}^{D_{in} \times D_{out}}$
\item Value matrix: $W_V \in \mathbb{R}^{D_{in} \times D_{out}}$
\end{itemize}

\textbf{Computation}:
\begin{align*}
\text{Queries: } Q &= XW_Q \in \mathbb{R}^{N \times D_{out}}\\
\text{Keys: } K &= XW_K \in \mathbb{R}^{N \times D_{out}}\\
\text{Values: } V &= XW_V \in \mathbb{R}^{N \times D_{out}}\\
\text{Similarities: } E &= \frac{QK^T}{\sqrt{D_{out}}} \in \mathbb{R}^{N \times N}\\
\text{Weights: } A &= \text{softmax}(E, \text{dim=1}) \in \mathbb{R}^{N \times N}\\
\text{Output: } Y &= AV \in \mathbb{R}^{N \times D_{out}}
\end{align*}

\textbf{Multi-Head Attention}:

\textbf{Inputs}:
\begin{itemize}
\item Input vectors: $X \in \mathbb{R}^{N \times D}$
\item For each head $h \in \{1,...,H\}$:
  \begin{itemize}
  \item Key matrix: $W_K^h \in \mathbb{R}^{D \times D_H}$
  \item Value matrix: $W_V^h \in \mathbb{R}^{D \times D_H}$
  \item Query matrix: $W_Q^h \in \mathbb{R}^{D \times D_H}$
  \end{itemize}
\item Output matrix: $W_O \in \mathbb{R}^{HD_H \times D}$
\end{itemize}

\textbf{Computation for each head $h$}:
\begin{align*}
\text{Queries: } Q^h &= XW_Q^h \in \mathbb{R}^{N \times D_H}\\
\text{Keys: } K^h &= XW_K^h \in \mathbb{R}^{N \times D_H}\\
\text{Values: } V^h &= XW_V^h \in \mathbb{R}^{N \times D_H}\\
\text{Similarities: } E^h &= \frac{Q^h(K^h)^T}{\sqrt{D_H}} \in \mathbb{R}^{N \times N}\\
\text{Attention weights: } A^h &= \text{softmax}(E^h, \text{dim=1}) \in \mathbb{R}^{N \times N}\\
\text{Head output: } Y^h &= A^hV^h \in \mathbb{R}^{N \times D_H}
\end{align*}

\textbf{Combining heads}:
\begin{align*}
\text{Concatenated output: } Y &= [Y^1; Y^2; ...; Y^H] \in \mathbb{R}^{N \times HD_H}\\
\text{Final output: } O &= YW_O \in \mathbb{R}^{N \times D}
\end{align*}

\textbf{Types of Attention}:
\begin{itemize}
\item Self-attention: Q, K, V from same sequence
\item Cross-attention: Q from one sequence, K, V from another
\item Masked attention: Future positions masked (decoder)
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item Time complexity: $O(n^2d)$ for sequence length $n$ and dimension $d$
\item Memory complexity: $O(n^2)$ for attention weights
\item Attention weights computed from Q and K (not V)
\item Scaling factor $\sqrt{d_k}$ prevents vanishing gradients with large dimensions
\item Self-attention is permutation equivariant without positional encoding
\end{itemize}

\section*{\underline{Transformers}}
\textbf{Transformer block}:
\begin{enumerate}
\item Layer normalization
\item Multi-head self-attention
\item Residual connection
\item Layer normalization
\item Feed-forward network (MLP)
\item Residual connection
\end{enumerate}

\textbf{Parameters in transformer block}:
\begin{itemize}
\item Self-attention: $4d^2$ (Q, K, V projections + output)
\item Feed-forward: $2df$ (where $f$ is FF dimension, typically $4d$)
\end{itemize}

\textbf{Vision Transformer (ViT)}:
\begin{itemize}
\item Split image into patches (16×16)
\item Linear projection + position embeddings
\item Standard transformer encoder architecture
\item [CLS] token or pooling for classification
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item Transformers use LayerNorm, NOT BatchNorm
\item Pre-norm vs. post-norm: affects training stability
\item Transformers parallelize better than RNNs for sequences
\item Positional encodings enable model to learn position information
\end{itemize}

\section*{\underline{Semantic Segmentation}}
\textbf{Task}: Classify each pixel in an image\\
\textbf{Architectures}:
\begin{itemize}
\item Fully Convolutional Networks (FCN)
\item U-Net: Encoder-decoder with skip connections
\item DeepLab: Atrous convolutions for dense predictions
\end{itemize}

\textbf{Upsampling techniques}:
\begin{itemize}
\item Unpooling: Reverse pooling operation
\item Transposed convolution: Learnable upsampling
\item Bilinear interpolation + 1×1 convs: Smoother results
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item Semantic segmentation: One label per pixel, no instance separation
\item Downsampling followed by upsampling preserves context while maintaining resolution
\item Skip connections help preserve spatial detail
\item Dilated/atrous convolutions expand receptive field without losing resolution
\end{itemize}

\section*{\underline{Object Detection}}
\textbf{Key architectures}:
\begin{itemize}
\item R-CNN family: Region proposals + classification
\item YOLO: Single-pass detection with grid cells
\item DETR: Transformers with object queries
\end{itemize}

\textbf{Region Proposal Network}:
\begin{itemize}
\item Generate candidate boxes
\item Binary classification (object vs. background)
\item Bounding box regression
\end{itemize}

\textbf{Evaluation metrics}:
\begin{itemize}
\item IoU (Intersection over Union): $\frac{\text{area of intersection}}{\text{area of union}}$
\item Precision: $\frac{\text{TP}}{\text{TP + FP}}$, Recall: $\frac{\text{TP}}{\text{TP + FN}}$
\item AP: Area under PR curve for each class
\item mAP: Mean AP across all classes
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item Two-stage detectors (R-CNN family): region proposal + classification
\item One-stage detectors (YOLO, SSD): directly predict boxes from grid cells
\item NMS (Non-Maximum Suppression): Remove duplicate detections
\item Anchor boxes: Pre-defined box shapes to match during training
\end{itemize}

\section*{\underline{Instance Segmentation}}
\textbf{Mask R-CNN}:
\begin{itemize}
\item Extends Faster R-CNN with mask branch
\item RoIAlign for accurate feature extraction
\item Parallel heads for classification, box regression, mask prediction
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item RoIAlign: Keeps spatial information intact (avoids quantization)
\item Instance segmentation separates individual instances of same class
\item Panoptic segmentation: Combines semantic and instance segmentation
\end{itemize}

\section*{\underline{Video Understanding}}
\textbf{Architectures}:
\begin{itemize}
\item Single-frame CNN + temporal pooling
\item Early fusion: Treat time as channels
\item 3D CNN: 3D convolutions (C3D, I3D)
\item CNN + RNN: CNN features fed to RNN
\item Transformer: Space-time attention
\end{itemize}

\textbf{3D convolution}: 
\begin{align*}
\text{Output}: F \times T' \times H' \times W'\\
\text{Filter size}: C \times k_t \times k \times k
\end{align*}

\textbf{Two-stream networks}:
\begin{itemize}
\item Spatial stream: RGB frames
\item Temporal stream: Optical flow
\item Late fusion of predictions
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item 3D CNN receptive fields span space and time dimensions
\item Early fusion builds temporal receptive field all at once
\item Slow fusion gradually builds temporal receptive field
\item 3D CNNs have temporal-shift invariance (early fusion doesn't)
\end{itemize}

\section*{\underline{Neural Network Visualization}}
\textbf{Saliency maps}:
\begin{itemize}
\item Compute gradient of class score w.r.t input pixels
\item Highlights regions important for classification
\end{itemize}

\textbf{Class Activation Mapping (CAM)}:
\begin{align*}
M_c(x,y) = \sum_k w_k^c \cdot f_k(x,y)
\end{align*}

\textbf{Grad-CAM}:
\begin{itemize}
\item Generalizes CAM to any CNN architecture
\item Global-average-pools gradients for importance weights
\item Weighted combination of feature maps
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item Visualizations help debug network decisions
\item CNN filters often detect edges, textures, patterns, and semantic concepts
\item Attention maps in transformers provide built-in visualization
\end{itemize}

\section*{\underline{Evaluation Metrics}}
\textbf{Classification}:
\begin{itemize}
\item Accuracy: $\frac{\text{correct predictions}}{\text{total predictions}}$
\item Precision: $\frac{\text{TP}}{\text{TP + FP}}$
\item Recall: $\frac{\text{TP}}{\text{TP + FN}}$
\item F1 Score: $\frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$
\end{itemize}

\textbf{Segmentation}:
\begin{itemize}
\item Pixel accuracy: $\frac{\text{correctly classified pixels}}{\text{total pixels}}$
\item Mean IoU: Average IoU across all classes
\item Dice coefficient: $\frac{2 \times \text{intersection}}{\text{sum of areas}}$
\end{itemize}

\textbf{Point Cloud Processing}:
\begin{itemize}
\item Translation equivariance: Output shifts when input shifts
\item Rotation equivariance: Output rotates when input rotates 
\item Convs on grid structure not rotation-equivariant
\item Continuous point convs use weight functions of relative positions
\end{itemize}

\end{multicols}
\end{document}
