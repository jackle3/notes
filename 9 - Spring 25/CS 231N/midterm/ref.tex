\documentclass{article}
\usepackage[margin=0.2in]{geometry} % Super small margins
\usepackage{multicol}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{graphicx}

\setlength{\parindent}{0pt} % No paragraph indentation
\setlength{\parskip}{1pt} % Minimal space between paragraphs
\setlength{\columnsep}{0.2in} % Space between columns

% Reduce spacing in lists
\setlist{noitemsep, topsep=0pt, parsep=0pt, partopsep=0pt, leftmargin=*}

% Reduce spacing in math environments
% \setlength{\abovedisplayskip}{3pt}
% \setlength{\belowdisplayskip}{3pt}
% \setlength{\abovedisplayshortskip}{1pt}
% \setlength{\belowdisplayshortskip}{1pt}

\expandafter\def\expandafter\normalsize\expandafter{%
    \normalsize%
    \setlength\abovedisplayskip{2pt}%
    \setlength\belowdisplayskip{2pt}%
    \setlength\abovedisplayshortskip{-8pt}%
    \setlength\belowdisplayshortskip{2pt}%
}

% Smaller section headers
\makeatletter
\renewcommand\section{\@startsection{section}{1}{\z@}%
                                  {-1.5ex \@plus -1ex \@minus -.2ex}%
                                  {0.8ex \@plus.2ex}%
                                  {\normalfont\small\bfseries}}
\makeatother

\begin{document}
\fontsize{6pt}{7pt}\selectfont % Size 6 font with 7pt line height

% \raggedcolumns
\begin{multicols}{3} % Three columns for maximum space utilization

\section*{\underline{Linear Classification}}
\textbf{Score function}: $s = f(x; W) = Wx + b$\\
\textbf{Softmax classifier}: $P(y_i|x_i) = \frac{e^{s_{y_i}}}{\sum_j e^{s_j}}$\\
\textbf{Cross-entropy loss}: $L_i = -\log(P(y_i|x_i))$
\begin{itemize}
\item Min loss: 0 (when $P(y_i|x_i)=1$)
\item Max loss: $\infty$ (when $P(y_i|x_i) \approx 0$)
\item Random initialization: $\log(C)$ for $C$ classes
\end{itemize}

\textbf{SVM loss}: $L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + \Delta)$
\begin{itemize}
\item $\Delta$ is margin parameter (typically $\Delta = 1$)
\item Wants correct class score higher than incorrect class scores by at least $\Delta$
\item Geometric interpretation: Linear hyperplanes separating classes
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item Any FC network can be expressed as a CNN (with 1×1 filters) and vice versa
\item Loss gradients flow from softmax loss to weight matrix proportional to input
\item Linear classifiers can't solve XOR problems (need nonlinearities)
\end{itemize}

\section*{\underline{Regularization}}
\textbf{Full loss}: $L = \frac{1}{N} \sum_{i=1}^N L_i + \lambda R(W)$\\
\textbf{Types}:
\begin{itemize}
\item L2: $R(W) = \sum_k \sum_l W_{k,l}^2$ (prefers diffuse weights)
\item L1: $R(W) = \sum_k \sum_l |W_{k,l}|$ (promotes sparsity)
\item Elastic Net: $R(W) = \sum_k \sum_l \beta W_{k,l}^2 + |W_{k,l}|$
\item Dropout: Randomly zero outputs during training (scale by p at test)
\item Normalization adds regularization effect due to noise in batch stats
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item Early stopping: end training when val error increases
\item Dropout forces redundant representations, acts like ensemble that shares parameters
\item Regularizing bias terms is generally avoided (mainly regularize weights)
\item Data augmentation: Adding transformed training examples
\end{itemize}

\section*{\underline{Optimization Algorithms}}
\textbf{SGD}: $w_{t+1} = w_t - \alpha \nabla L(w_t)$\\
\textbf{SGD+Momentum}: 
\begin{align*}
v_{t+1} &= \rho v_t + \nabla L(w_t) \tag{typically $\rho = 0.9$ or $0.99$}\\
w_{t+1} &= w_t - \alpha v_{t+1}
\end{align*}

\textbf{RMSProp}:
\begin{align*}
\text{grad\_squared} &= \beta \cdot \text{grad\_squared} + (1-\beta) \cdot (\nabla L(w_t))^2\\
w_{t+1} &= w_t - \frac{\alpha \cdot \nabla L(w_t)}{\sqrt{\text{grad\_squared}} + \epsilon}
\end{align*}

\textbf{Adam}:
\begin{align*}
m_t &= \beta_1 m_{t-1} + (1-\beta_1)\nabla L(w_t) \tag{momentum}\\
v_t &= \beta_2 v_{t-1} + (1-\beta_2)(\nabla L(w_t))^2 \tag{RMSProp}\\
\hat{m}_t &= m_t / (1-\beta_1^t) \tag{bias correction}\\
\hat{v}_t &= v_t / (1-\beta_2^t) \tag{bias correction}\\
w_{t+1} &= w_t - \alpha \hat{m}_t / (\sqrt{\hat{v}_t} + \epsilon)
\end{align*}

\textbf{Learning Rate Decay}:
\begin{itemize}
\item Decrease LR over time (step, cosine, linear, etc.)
\item Linear warmup: increase LR from 0 over first few steps, prevent exploding loss
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item SGD issues: poor conditioning, getting stuck in local minima/saddle points, noisy
\item Momentum overcomes oscillations and escape poor local minima, continues moving in prev direction
\item RMSProp adds per-parameter learning rate, addresses AdaGrad's decaying learning rate issue
\item Adam combines momentum and RMSProp
\item AdamW separates weight decay from gradient update for better regularization
\item Second-order methods: $\theta^* = \theta_0 - \alpha H^{-1} \nabla L(\theta_0)$\\
Better updates, $O(N^2)$ mem and $O(N^3)$ time to invert
\end{itemize}

\section*{\underline{Neural Networks}}
\textbf{MLP}: $f = W_2 \text{max}(0, W_1 x + b_1) + b_2$\\
$x \in \mathbb{R}^D$, $W_1 \in \mathbb{R}^{H \times D}$, $b_1 \in \mathbb{R}^H$, $W_2 \in \mathbb{R}^{C \times H}$, $b_2 \in \mathbb{R}^C$\\
\textbf{Activation Functions}:
\begin{itemize}
\item ReLU: $f(x) = \max(0, x)$
\item Leaky ReLU: $f(x) = \max(\alpha x, x)$ with small $\alpha$
\item ELU: $f(x) = \begin{cases} x & \text{if } x \geq 0 \\ \alpha(e^x - 1) & \text{if } x < 0 \end{cases}$
\item GELU: $f(x) = x \cdot \Phi(x)$
\item Sigmoid: $\sigma(x) = \frac{1}{1+e^{-x}}$
\item Tanh: $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item ReLU has zero grad with negative inputs (dying ReLU)
\item Leaky ReLU (and variants) always has non-zero slope
\item Tanh has zero-centered outputs (sigmoid has $\mu = 0.5$)
\item Without nonlinear activations, deep networks reduce to linear models
\item Deeper networks can represent more complex functions with fewer parameters (more non-linearities)
\end{itemize}

\section*{\underline{Backpropagation}}
\textbf{Gradient flow}: Upstream $\times$ Local = Downstream\\
\textbf{Vector derivatives}:
\begin{itemize}
\item $d_x f(x)$ has same shape as $x$
\item Apply chain rule using matrix calculus
\item Matmul: $\frac{\partial}{\partial X}(XW) = W^T$ and $\frac{\partial}{\partial W}(XW) = X^T$
\item Each element $X_{n,d}$ affects the whole row $Y_{n}$
\item Backprop: X: [N, D], W: [D, M], Y: [N, M]
$$\frac{\partial L}{\partial X} = \left(\frac{\partial L}{\partial Y}\right)W^T \in [N, D]  \quad\frac{\partial L}{\partial W} = X^T\left(\frac{\partial L}{\partial Y}\right) \in [D, M]$$
\end{itemize}
\textbf{Special derivatives}:
\begin{itemize}
\item Sigmoid: $d_x \sigma(x) = \sigma(x)(1-\sigma(x))$
\item Tanh: $d_x \tanh(x) = 1 - \tanh^2(x)$
\item ReLU: $d_x \text{ReLU}(x) = \mathbbm{1}(x > 0)$
\item Max: $d_x \max(x, y) = \mathbbm{1}(x > y)$
\item Softmax: $\frac{d p_i}{d s_j} = p_i(\mathbbm{1}(i=j) - p_j)$ where $p = \frac{e^{s}}{\sum_k e^{s_k}}$
\item Cross-entropy: $d_{s_i} \left(-\sum_j y_j\log(p_j)\right) = p_i - y_i$
\item Huber Loss: $d_x L_{\delta}(x,y) = \begin{cases} x-y & \text{if } |x-y| < \delta \\ \delta \cdot \text{sign}(x-y) & \text{otherwise} \end{cases}$
\item L1 Loss: $d_x |x-y| = \text{sign}(x-y)$
\end{itemize}

\textbf{Key Backpropagation Concepts}:
\begin{itemize}
\item Vanishing gradients: gradients become too small in deep networks (esp. with sigmoid/tanh)
\item Exploding gradients: gradients become too large (common in RNNs)
\item Gradient clipping: Cap gradient magnitude to prevent explosion
\end{itemize}

\section*{\underline{Convolutional Neural Networks}}
\textbf{Conv Layer Summary}:
\begin{itemize}
\item \textbf{Hyperparameters}:
\begin{itemize}
\item Kernel size: $K_H \times K_W$
\item Number filters: $C_{\text{out}}$
\item Padding: $P = (K-1)/2$ (same padding)
\item Stride: $S$
\end{itemize}
\item \textbf{Weight matrix}: $C_{\text{out}} \times C_{\text{in}} \times K_H \times K_W$
\item \textbf{Bias vector}: $C_{\text{out}}$
\item \textbf{Input}: $C_{\text{in}} \times H \times W$
\item \textbf{Output activation}: $C_{\text{out}} \times H' \times W'$ where
$$
[H', W'] = \frac{[H, W] - K_{[H, W]} + 2P}{S} + 1
$$
\end{itemize}

\textbf{Advantages}:
\begin{itemize}
\item Parameter sharing: Same filter applied across image
\item Sparse connectivity: Each output depends on small local region
\item Translation equivariance: Shifting input shifts output
\end{itemize}

\textbf{Pooling layers}: 
\begin{itemize}
\item Given input C×H×W, downsample each 1×H×W plane
\item Max pooling: Take maximum value in window
\item Average pooling: Average values in window
\item Reduces spatial dimensions, increases receptive field
\end{itemize}

\textbf{Receptive field}: Region of input that affects output
\begin{itemize}
\item For K×K filters, RF grows by $(K-1)$ per layer
\item With $L$ layers and $S=1$, RF is $1 + L * (K - 1)$
\item In general, $R_0 = 1$ and $R_l = R_{l-1} + (K-1) \times S$
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item Multiple 3×3 filters better than single large filter: fewer parameters, more nonlinearities
\item 1×1 conv: same H/W, dim reduction across channels
\item Dilated convolutions: expand receptive field without increasing parameters
\end{itemize}

\section*{\underline{CNN Architectures}}
\textbf{VGG}:
\begin{itemize}
\item Multiple 3×3 convs followed by max-pooling
\item Stack of three 3×3 convs has same receptive field as 7×7 conv, but deeper with less params ($3\times 9C^2$ vs $49C^2$)
\item Uniform design: doubles channels after each pooling
\end{itemize}

\textbf{ResNet}:
\begin{itemize}
\item Skip connections: $\text{output} = F(x) + x$
\item Allow deeper networks by learning residual mapping
\item Solves vanishing gradient problem in deep nets
\item Conv → BN → ReLU → Conv → BN → Add → ReLU
\end{itemize}

\section*{\underline{Equivariance and Invariance}}
\textbf{Definitions:}
\begin{itemize}
  \item \textbf{Equivariant:} $f(Tx) = T f(x)$\hspace{0.5em} (output transforms in the same way as input)
  \item \textbf{Invariant:} $f(Tx) = f(x)$\hspace{0.5em} (output does not change under transformation)
\end{itemize}

\textbf{Key Types:}
\begin{itemize}
  \item \textbf{Translation equivariant:} Shifting the input causes the output to shift in the same way.
    \begin{itemize}
      \item \textit{Example:} \textbf{Convolution:} If you shift an image, the output activation map shifts the same amount.
      \item \textit{Non-example:} Fully connected (MLP) layers are not translation equivariant.
    \end{itemize}
  \item \textbf{Translation invariant:} Shifting the input does not change the output.
    \begin{itemize}
      \item \textit{Example:} \textbf{Global pooling:} $\max(x)$ or $\sum x$ over all positions.
    \end{itemize}
    \item \textbf{Rotation equivariant:} Rotating the input causes the output to rotate in the same way.
    \begin{itemize}
      \item \textit{Non-example:} Standard CNNs with fixed kernels are not rotation equivariant (rotating the image does not rotate the output activation map).
    \end{itemize}
  \item \textbf{Rotation invariant:} Rotating the input does not change the output.
    \begin{itemize}
      \item \textit{Example:} $\|x\|_2$ (L2 vector norm)
    \end{itemize}
  \item \textbf{Permutation equivariant (Self-attention):}
    \begin{itemize}
      \item \textit{Definition:} Reordering (permuting) the input sequence reorders the output in the same way.
      \item \textit{Example:} \textbf{Self-attention without positional encoding:} If you swap two tokens in the input, the outputs for those tokens are swapped.
      \item \textit{Non-example:} Self-attention with positional encoding is not permutation equivariant.
    \end{itemize}
\end{itemize}

\section*{\underline{Normalization Techniques}}
\includegraphics[width=\columnwidth]{imgs/norm.png}
\textbf{Batch Normalization}:
\begin{align*}
\mu_c &= \text{mean of feature values across batch for channel c}\\
\sigma_c &= \text{standard deviation across batch for channel c}\\
y &= \gamma_c \cdot (x - \mu_c)/ \sigma_c + \beta_c
\end{align*}
\begin{itemize}
\item Normalizes across batch dimension for each channel
\item Used in CNNs, must track running stats for inference
\end{itemize}

\textbf{Layer Normalization}:
\begin{align*}
\mu_n &= \text{mean across all channels for sample n}\\
\sigma_n &= \text{standard deviation across all channels for sample n}\\
y &= \gamma_c \cdot (x - \mu_n)/ \sigma_n + \beta_c
\end{align*}
\begin{itemize}
\item Normalizes across channel dimension for each sample
\item Used in transformers, no dependence on batch statistics, good for sequence models
\end{itemize}

\textbf{Group Normalization}:
\begin{itemize}
\item Group channels into groups, normalize each group independently for each sample
\item Used in CNNs, good for parallelization
\end{itemize}

\section*{\underline{Weight Initialization}}
\textbf{Kaiming initialization}: $W \sim \mathcal{N}(0, \sqrt{\frac{2}{D_{in}}})$ for ReLU
\begin{itemize}
\item For ReLU activations (accounts for half being zeroed)
\item For CNN: $D_{in} = C_{\text{in}} \times K_H \times K_W$
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item Too small or too large initializations can cause vanishing/exploding gradients
\item Initialization in deep nets is crucial for trainability
\item Normalization mitigates bad initialization (not solve)
\item Initialization should match the activation function
\end{itemize}

\section*{\underline{Training Techniques}}
\textbf{Data Normalization}: subtract per-channel mean, divide by per-channel std, better convergence and generalization

\textbf{Data Augmentation}:
\begin{itemize}
\item Increases dataset size/diversity without new data
\item Improves robustness to image variations
\item Common techniques: flips, crops, color jitter, rotations
\end{itemize}

\textbf{Transfer Learning}: use pre-trained models
\begin{itemize}
\item Small dataset + similar: retrain final layer
\item Small dataset + different: another model or more data
\item Large dataset + similar: finetune all model layers
\item Large dataset + different: either finetune all layers or train from scratch
\end{itemize}

\textbf{Diagnostics}:
\begin{itemize}
\item Underfitting: Low train/val accuracy, small or no gap
\item Overfitting: High train, low val accuracy, large gap
\item Not training enough: Low train/val accuracy with gap
\end{itemize}

\textbf{Hyperparameter selection}:
\begin{itemize}
\item Random search usually better than grid search
\item Check initial loss, overfit small sample first
\item Find LR that makes loss decrease quickly
\item Split data into train/val/test; tune on validation set
\item K-fold cross-validation useful for small datasets
\end{itemize}

\section*{\underline{Loss Functions}}
\textbf{Cross-entropy}: $L = -\sum_i y_i \log(\hat{y}_i)$
\begin{itemize}
\item For classification problems, measures how well predictions match true labels
\end{itemize}

\textbf{KL}: $D_{KL}(p||q) = \sum_i p_i \log\frac{p_i}{q_i} = \sum_i p_i(\log p_i - \log q_i)$
\begin{itemize}
\item $D_{KL}(p||q) = \text{CrossEntropy}(p, q) - H(p)$
\item In one-hot classification, KL is same as CE because $H(\text{one hot true labels})$ is zero
\item Measures dissimilarity between probability dist.
\item Not symmetric: $D_{KL}(p||q) \neq D_{KL}(q||p)$
\end{itemize}

\textbf{Smooth L1/Huber Loss}:
\begin{align*}
L_{\delta}(x, y) = \begin{cases}
\frac{1}{2}(x-y)^2 & \text{if } |x-y| < \delta \\
\delta(|x-y| - \frac{1}{2}\delta) & \text{otherwise}
\end{cases}
\end{align*}
\begin{itemize}
\item Combines MSE (near zero) and L1 (for outliers)
\item Differentiable everywhere, robust to outliers
\end{itemize}

\textbf{Triplet margin}: L(a,p,n) = $\max\{d(a,p) - d(a,n) + \Delta, 0\}$
\begin{itemize}
\item Used in contrastive learning, pushes anchor (a) closer to positive (p) than negative (n)
\item Margin controls separation between positive and negative pairs
\item Small margin $\rightarrow$ harder to separate, larger margin $\rightarrow$ too much separation, difficult to learn
\end{itemize}

\section*{\underline{Recurrent Neural Networks}}
\textbf{Vanilla RNN}:
\begin{align*}
h_t &= \tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)\\
y_t &= W_{hy}h_t + b_y
\end{align*}

\textbf{LSTM}:
\begin{itemize}
\item Solves vanilla RNN's vanishing gradient problem
\item Cell state ($C_t$) maintains long-term memory
\item Three gates control information flow:
  \begin{itemize}
  \item Forget gate: decides what to discard from cell state
  \item Input gate: decides what new information to store
  \item Output gate: controls what parts of cell state affect output
  \end{itemize}
\item Gradient can flow unchanged through cell state
\item More complex but better at capturing long sequences
\end{itemize}

\textbf{RNN Applications}:
\begin{itemize}
\item Language modeling: Predict next token in sequence
\item Captioning: CNN feature extractor + RNN decoder
\item Sequence-to-sequence: encoder-decoder for translation
\end{itemize}

\textbf{Training RNNs}:
\begin{itemize}
\item Backpropagation through time (BPTT)
\item Truncated BPTT for long sequences
\item Gradient clipping to prevent explosion
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item RNNs can process variable-length sequences
\item Vanishing gradients limit long-term learning
\item RNNs sequential processing limits parallelization
\end{itemize}

\section*{\underline{Attention}}
\includegraphics[width=\columnwidth]{imgs/cross.png}
\includegraphics[width=\columnwidth]{imgs/self.png}
\includegraphics[width=\columnwidth]{imgs/masked.png}
\includegraphics[width=\columnwidth]{imgs/multihead.png}
\textbf{Types of Attention}:
\begin{itemize}
\item Self-attention: Q, K, V from same sequence
\item Cross-attention: Q from one, KV from another
\item Masked attention: Future positions masked (decoder)
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item Time complexity: $O(n^2d)$ for sequence length $n$ and dimension $d$
\item Memory complexity: $O(n^2)$ for attention weights
\item Attention weights computed from Q and K (not V)
\item Scaling factor $\sqrt{d_k}$ prevents vanishing gradients with large dimensions
\item Self-attention is permutation equivariant without positional encoding
\end{itemize}

\includegraphics[width=\columnwidth]{imgs/seqs.png}

\section*{\underline{Transformers}}
\includegraphics[width=\columnwidth]{imgs/transformer.png}

\textbf{Language Modeling}:
\begin{itemize}
\item Learn an embedding and projection matrix to convert tokens to vectors and background
\item Use masked attention to prevent future peeking
\item Train to predict next token with softmax + CE loss
\end{itemize}

\textbf{Vision Transformer (ViT)}:
\begin{itemize}
\item Split image into patches (16×16)
\item Linear projection + position embeddings (same as 16×16 conv with stride 16, $C_{\text{in}} = 3$ and $C_{\text{out}} = D$)
\item Standard transformer encoder architecture
\item CLS token or pooling for classification
\end{itemize}

\textbf{More Details}:
\begin{itemize}
\item Positional encodings to learn position information
\item Pre-norm transformer: Normalization inside residual block allows learning identity function
\item RMSNorm: Alternative normalization layer used in modern transformers
\item SwiGLU MLP: Improved MLP architecture for transformer blocks
\item Mixture of Experts (MoE): Uses $E$ different MLPs but only activates $A < E$ per token, increasing parameters with modest compute cost
\end{itemize}

\section*{\underline{Semantic Segmentation}}
\textbf{Task}: Classify each pixel in an image\\
\textbf{Architectures}:
\begin{itemize}
\item Fully Convolutional Networks (FCN)
\item U-Net: Downsample with conv (increases receptive field, loses spatial information) then upsample with transposed conv (high resolution mapping), includes skip connections
\end{itemize}

\textbf{Upsampling techniques}:
\begin{itemize}
\item Unpooling: reverse pooling operation
\item Transposed convolution: learnable upsampling
\end{itemize}
\includegraphics[width=\columnwidth]{imgs/upsampl.png}

\textbf{Key concepts}:
\begin{itemize}
\item Semantic segmentation: One label per pixel, no instance separation
\item Downsampling followed by upsampling preserves context while maintaining resolution
\item Skip connections help preserve spatial detail
\end{itemize}

\section*{\underline{Object Detection}}
\textbf{R-CNN (multi-pass detection)}:
\begin{itemize}
\item \textbf{R-CNN}: Extract region proposals → CNN classifies each region independently as object or background
\item \textbf{Fast R-CNN}: CNN first, then extract regions from feature maps → more efficient
\item \textbf{Faster R-CNN}: Region Proposal Network (RPN) to generate proposals from feature maps
\item \textbf{RPN}: Predicts objectness score and box coordinates for anchor boxes at each location
  \begin{itemize}
    \item K bounding boxes of different size/aspect ratio at each location
    \item For each anchor: binary classification (object/not) and regress box to ground truth
    \item Outputs region proposals, picks top ones
  \end{itemize}
\end{itemize}

\textbf{YOLO (single-pass detection)}:
\begin{itemize}
\item Divides image into grid of cells → each cell predicts: prob of object, potential bounding boxes, class scores
\item Each bounding box includes: (x, y, w, h, confidence)
\item Single forward pass → much faster than R-CNN family
\end{itemize}

\textbf{DETR (transformer-based)}:
\begin{itemize}
\item CNN backbone extracts image features → transformer encoder processes them
\item Transformer decoder with object queries attends to encoded image
\item Directly outputs fixed set of bounding boxes and class predictions
\end{itemize}

\section*{\underline{Instance Segmentation}}
\textbf{Mask R-CNN}:
\begin{itemize}
\item Extends Faster R-CNN with mask branch for segmentation, operates on each ROI and predicts binary mask
\item CNN backbone → RPN → RoIAlign → mask prediction
\item RoIAlign preserves spatial precision (outputs classification scores and box coordinates)
\end{itemize}

\section*{\underline{Neural Network Visualization}}
\textbf{Saliency maps}:
\begin{itemize}
\item Compute gradient of class score w.r.t input pixels
\item Highlights regions important for classification
\item Simple technique to visualize what the network looks at
\end{itemize}

\textbf{Class Activation Mapping (CAM)}:
\begin{itemize}
\item Extract feature maps from the final convolutional layer
\item Weight these maps using the classification layer weights
\item Requires specific network architecture with global average pooling
\end{itemize}

\textbf{Grad-CAM}:
\begin{itemize}
\item Works with any CNN architecture (more flexible)
\item Computes importance of feature map for target class
\item Combines feature maps weighted by their importance
\item Creates heatmap highlighting discriminative regions
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item Visualization reveal network's attention regions
\item Early layers detect low-level features (edges, textures)
\item Deeper layers detect high-level concepts (objects, parts)
\item Helps identify dataset bias and explain model decisions
\item Can verify if model focuses on relevant image regions
\end{itemize}

\section*{\underline{Video Understanding}}
\textbf{Architectures for Video Classification}:
\begin{itemize}
\item Single-frame CNN: Process each frame, average preds
\item Late fusion: Process frames independently with CNN, combine features with MLP or pooling
\item Early fusion: Treat time as channels (reshape to $3T \times H \times W$), apply 2D CNN to get class scores
\item 3D CNN: 3D convolutions across space-time dimensions
\item CNN + RNN: Extract CNN features from frames, feed sequence to RNN for long-term temporal modeling
\item Recurrent Convolutional Network: Replace RNN matrix multiplications with convolutions
\item Transformer: Space-time self-attention on video tokens
\end{itemize}

\textbf{Receptive Fields}:

\includegraphics[width=\columnwidth]{imgs/video.png}

\textbf{RCN Layer}:
\begin{align*}
H_t^l &= \tanh(W_{xh}^l * X_t^l + W_{hh}^l * H_{t-1}^l)\\
X_t^{l+1} &= H_t^l
\end{align*}
\begin{itemize}
\item Replaces matmuls in RNNs with convolutions
\item Each layer is a convolution, each column is a time step
\item Captures both spatial and temporal patterns simultaneously, though its slow
\end{itemize}

\textbf{Key concepts}:
\begin{itemize}
\item Early Fusion has no temporal shift invariance, needs to learn filters for same motion at different times.
\item 3D CNN has temporal shift invariance, each filter slides over time.
\item I3D: Inflated 2D CNN, expands 2D filters to 3D by repeating weights temporally
\item Long-term temporal structure: CNN+RNN or space-time self-attention
\item Video understanding benefits from multi-modal inputs (RGB, optical flow, audio)
\end{itemize}

\end{multicols}
\end{document}
