
# 1 Recap: Self-Supervised Learning
Learning structure from data **without any labels** ⟶ use encoder to extract feature representations
![](../../attachments/Pasted%20image%2020250524144512.png)
![](../../attachments/Pasted%20image%2020250524144517.png)
## 1.1 Pretext Tasks
Task that we can use to train the encoder-decoder without needing to have explicit labels
* During training, we can train on huge amounts of low quality unlabeled data
* For downstream tasks, throw away decoder and swap in a FC network to predict labels for task
![](../../attachments/Pasted%20image%2020250524144620.png)

## 1.2 Contrastive Representation Learning
We have pairs that are similar and dissimliar ⟶ pull similar pairs together and push dissimilar pairs apart
* For each image, apply two random perturbations to input image and train their embeddings to be similar
* We create a $2N \times 2N$ similarity matrix for our perturbed images ⟶ bring them close together
![](../../attachments/Pasted%20image%2020250524144638.png)
![](../../attachments/Pasted%20image%2020250524144643%201.png)

**Problem:** need large batch sizes with lots of negatives in order for this to work
* It's too easy for the network if there is not enough negative examples ⟶ large batch sizes make it harder which leads to better learned features
* **Solution:** MoCo
	* MoCo is very similar to SimCLR ⟶ the difference is keeping a queue of negative samples from previous iterations, such that we can include the negatives into the training (with no grads) so that we can artifically create a large batch size
	* Momentum Encoder is exponential moving average of the actual encoder weights (decayed)
	* We now no longer need a huge batch of negatives
![](../../attachments/Pasted%20image%2020250524144919.png)

**DINO:** similar to MoCo but uses a different KL Divergence loss
* DINOv2 is a really strong model for self-supervised features ⟶ used a lot today
* DINOv2 uses a huge amount of images ⟶ scaled up to very strong features
![](../../attachments/Pasted%20image%2020250524145110.png)
![](../../attachments/Pasted%20image%2020250524145130.png)

# 2 Generative Models
## 2.1 Supervised Vs Unsupervised Learning
![](../../attachments/Pasted%20image%2020250524145244.png)
![](../../attachments/Pasted%20image%2020250524145308.png)

## 2.2 Generative Vs Discriminative Models
**Density Function**: $p(x)$ assigns a positive number to each $x$ ⟶ higher numbers mean $x$ is more likely
* These functions are normalized such that $\int_x p(x) dx = 1$
* Different values of $x$ **compete** for density

### 2.2.1 **Discriminative Models**
Predicts a label given an image
* Labels for each image compete, no competition between images
* **Problem:** no way to handle unreasonable inputs; must give a label distribution for all possible inputs
![](../../attachments/Pasted%20image%2020250524150209.png)
![](../../attachments/Pasted%20image%2020250524150307.png)

### 2.2.2 Generative Models
Learns a probability distribution over images
* Requires deep understanding:
	* Is a dog more likely to sit or stand?
	* Is a 3-legged dog more likely than a 3-armed monkey? Probably a 3-legged dog more common
* Models can "reject" unreasonable inputs by giving them small probability mass
	* If we wanted a generative model of zoo animals, we can assign very low probability mass to abstract art to reject that image from the generative model
![](../../attachments/Pasted%20image%2020250524150338.png)

### 2.2.3 Conditional Generative Models
Learns a probability distribution over images given the label
* Each possible label induces competition across all possible images
* We can sample from this distribution to generate an image of a cat given the label cat
![](../../attachments/Pasted%20image%2020250524150430.png)

### 2.2.4 Summary
These model paradigms are connected via Bayes' Rule
![](../../attachments/Pasted%20image%2020250524150458.png)

When do we use each model?
* Discriminative model ⟶ assigns labels to data, classification, etc
* Generative models ⟶ kind of useless in practice, can be used to detect outliers via probability mass
	* We can sample to get new data, but it's not useful because we can't control what is generated
* Conditional generative model ⟶ most common in practice, can generate new data and control via prompts
![](../../attachments/Pasted%20image%2020250524150503.png)

## 2.3 Why Generative Models?
**Modeling ambiguity:** If there are many possible outputs x for an input y, we want to model $P(x \mid y)$
* When there is ambiguity in the outputs, generative models are useful ⟶ we can then sample from this distribution to get reasonable outputs

![](../../attachments/Pasted%20image%2020250524150731.png)
![](../../attachments/Pasted%20image%2020250524150737.png)
![](../../attachments/Pasted%20image%2020250524150747.png)

## 2.4 Taxonomy of Generative Models
There are two main categories of generative models:
* **Explicit density models:** These models directly learn and represent the probability distribution $P(x \mid y)$. They can compute exact probabilities for any input.
* **Implicit density models:** These models cannot compute exact probabilities $p(x \mid y)$, but they can generate samples from the distribution $P(x \mid y)$ through a learned sampling process.
**Note:** for the rest of lecture, when we say $P(x)$ we mean $P(x \mid y)$
![](../../attachments/Pasted%20image%2020250524151436.png)

# 3 Autoregressive Models
## 3.1 Maximum Likelihood Estimation
In MLE, we want to find the weights to maximizes the likelihood of our training data
* Varying the distribution that the network is modeling, to maximize the likelihood of the fixed training data
* **Key assumption:** there is a true distribution $P_\text{data}$ from the universe that generated our training data
![](../../attachments/Pasted%20image%2020250524152032.png)
## 3.2 Autoregressive Methods
This method is very similar to MLE ⟶ condition next token based on previous tokens in sequence
* For a sample $x$, we break it up into subparts $x_1, \dots, x_T$
![](../../attachments/Pasted%20image%2020250524152133.png)

We've already seen this! This is exactly what an RNN and transformers are doing ⟶ hidden states
![|350](../../attachments/Pasted%20image%2020250524152418.png)
![|350](../../attachments/Pasted%20image%2020250524152436.png)

## 3.3 Autoregressive Models of Images
Autogressive models requires us to break our data up into sequence
* This is very natural with text data bc text is a 1D sequence of discrete characters/tokens
* Images are **more tricky** because they are continous (real-valued) and not 1D

**Idea:** treat the image as a sequence of pixels (three discrete RGB numbers)
![](../../attachments/Pasted%20image%2020250524152624.png)

**Problem:** this is really expensive. A 1024x1024 image (with 3 channels) is a sequence of 3M pixels
**Solution:** model the image as a **sequence of tiles**, not sequence of subpixels ⟶ patches in the image

# 4 Variational Autoencoders
With autoregressive models, we broke our data into discrete parts and **explicitly parameterize a density function** to maximize likelihood of the data.
![](../../attachments/Pasted%20image%2020250524152739.png)

Variational Autoencoders (VAE) define an **intractable density** that we cannot explicitly compute or optimize
* But we will be able to directly optimize a **lower bound** on the density
* Why? We give the ability to explicitly compute exact densities in order to compute **reasonable latent vectors** over our data ⟶ vectors that represent our data

## 4.1 Non-Variational Autoencoders
This approach is similar to self-supervised learning, where we learn representations by predicting parts of the input from other parts, without requiring explicit labels.

**Note:** if $z$ had no constraints, we can learn $z$ as an identity and nail this problem ⟶ in autoencoders, we constraint $z$ to be **much smaller** in latent space
* Network now has to learn identity function in a much smaller latent space ⟶ model learns structure abt data

![](../../attachments/Pasted%20image%2020250524152936.png)
![](../../attachments/Pasted%20image%2020250524152952.png)
![](../../attachments/Pasted%20image%2020250524153005.png)
![|650](../../attachments/Pasted%20image%2020250524153018.png)

## 4.2 VAEs for Generating
If we could generate/sample new latent features $z$, we can use the **decoder to generate new images**
![](../../attachments/Pasted%20image%2020250524153209.png)

**Problem:** Generating new z is not any easier than generating new x
**Solution:** force all $z$ to come from a known distribution so that we can easily sample from it!

**Key idea:** Learn latent features $z$ from data, then sample $z$ to generate new data $x$
* After training, we can generate new $x$ by first sampling $z$ from a prior (e.g., Gaussian), then using the decoder to get $x$
* This gives us a way to generate new, realistic samples
![](../../attachments/Pasted%20image%2020250524153430.png)

 **Training goal:** maximize the likelihood of the data under the model
* If we had both $x$ and $z$, we could directly train a conditional generative model $p_\theta(x|z)$
* In practice, we sample $z$ from the prior and use the decoder to generate $x$
![](../../attachments/Pasted%20image%2020250524153456.png)

**But:** we don't observe $z$ in real data, so we have to marginalize it out: $p_\theta(x) = \int p_\theta(x|z)p(z)dz$
* We assumed a Gaussian prior for $z$ so $p_\theta(z)$ is a Gaussian
* We can compute $p_\theta(x \mid z)$ using our decoder model
* **Problem:** we can't integrate over all of $z$ to compute this
	* This marginalization is intractable, but we can approximate it using the decoder
	* **Note:** This is the core challenge in training VAEs!
![](../../attachments/Pasted%20image%2020250524153740.png)

## 4.3 Bayes' Rule
![](../../attachments/Pasted%20image%2020250524153834.png)
![](../../attachments/Pasted%20image%2020250524153838.png)

## 4.4 VAEs Structure
![](../../attachments/Pasted%20image%2020250524153952.png)
![](../../attachments/Pasted%20image%2020250524154007.png)

## 4.5 ELBO Training Objective
What is our training objective? Answer: **ELBO**
![](../../attachments/Pasted%20image%2020250524154110.png)
![](../../attachments/Pasted%20image%2020250524154115.png)
![](../../attachments/Pasted%20image%2020250524154119.png)
Since $p_\theta(x)$ does not depend on $z$, we can just wrap it around an expectation (constant rule)
![](../../attachments/Pasted%20image%2020250524154220.png)
The first two terms below we can compute
![](../../attachments/Pasted%20image%2020250524154320.png)
![](../../attachments/Pasted%20image%2020250524154328.png)
We can't compute the last term because we can't compute $p_\theta(z \mid x)$ ⟶ drop it to get lower bound
![](../../attachments/Pasted%20image%2020250524154334.png)
![](../../attachments/Pasted%20image%2020250524154357.png)
![](../../attachments/Pasted%20image%2020250524154402.png)

**Final:** we jointly train the encoder $q$ and decoder $p$ to maximize the ELBo
![](../../attachments/Pasted%20image%2020250524154431.png)

## 4.6 Training a VAE
* We have our neural network encoder that takes $x$ and outputs a distribution $q(z \mid x)$
![](../../attachments/Pasted%20image%2020250524154503.png)
![](../../attachments/Pasted%20image%2020250524154607.png)
![](../../attachments/Pasted%20image%2020250524154614.png)
![](../../attachments/Pasted%20image%2020250524154620.png)
![](../../attachments/Pasted%20image%2020250524154720.png)

## 4.7 Sampling from a VAE
![](../../attachments/Pasted%20image%2020250524154749.png)

## 4.8 VAE: Disentangling
Since the latent space is a diagonal gaussian (meaning the covariance matrix is a diagonal matrix), there is a notion of independence between dimensions of latent space $z$
* Notice how below, varying different dimensions allows the digits to smoothly morph
![](../../attachments/Pasted%20image%2020250524154855.png)

# 5 Summary
![](../../attachments/Pasted%20image%2020250524154902.png)
![](../../attachments/Pasted%20image%2020250524154910.png)
![](../../attachments/Pasted%20image%2020250524154915.png)
