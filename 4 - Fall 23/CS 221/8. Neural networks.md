---
Date: 2023-10-04
---
# Non-linear predictors

![Untitled 107.png](attachments/Untitled%20107.png)

- Linear predictors were linear in two ways. The feature vector $\phi(x)$﻿ was linear, and the way it interacted with the weight $w$﻿ was also linear.
- The quadratic predictor left the linearity alone of the above the same. We just added terms to the feature vector $\phi(x)$﻿.
- Finally, for neural networks, we leave $\phi(x)$﻿ alone but play with the way it interacts with our prediction.
    - Leave the feature extractor alone, but increase the complexity of predictor, which can also produce non-linear (though not necessarily quadratic) predictors

# Motivating example

![Untitled 1 73.png](attachments/Untitled%201%2073.png)

- Any example in between the lines is unsafe, which means they collide. The two cars $x_1$﻿ and $x_2$﻿ are too close.

# Decomposing the problem

![Untitled 2 73.png](attachments/Untitled%202%2073.png)

- Just define each decision boundary as it’s own. Then, we check if the set of cars are outside the boundary, for each.

## Rewriting with vector notation

![Untitled 3 73.png](attachments/Untitled%203%2073.png)

- To get $\mathbf h(x)$﻿, we just stack our two vectors $h_i(x)$﻿.

# Avoiding zero gradients

![Untitled 4 70.png](attachments/Untitled%204%2070.png)

- The gradient of the red line is zero everywhere. We replace this function with activation functions to avoid non-zero gradients.

# Two-layer neural networks

![Untitled 5 70.png](attachments/Untitled%205%2070.png)

- We start with a feature vector $\phi(x)$﻿. We then multiply it by a weight matrix $V$﻿. We then put it through an activation function $\sigma$﻿. This generates a vector $\mathbf h(x)$﻿.
    - The rows of the weight matrix $V$﻿ can be interpreted as the weight vectors of the $k$﻿ intermediate subproblems.
    - It’s like the $[-1, +1, -1]$﻿ from $h_1$﻿ above.
- Now with the $\mathbf h(x)$﻿, I can easily do prediction to classify it. $\mathbf h(x)$﻿ is simply a smarter and leanred feature representation, as compared to the original $\phi(x)$﻿.
- The neural network decomposes a problem into a bunch of smaller subproblems, which is captured by $\mathbf h(x)$﻿.

# Deep neural networks

![Untitled 6 69.png](attachments/Untitled%206%2069.png)

- For more layers, basically just rinse and repeat with more weight matrices $V_i$﻿ and repeated applications of the activation layer.

## Layers in neural networks

- Each layer represents another level of abstraction.

![Untitled 7 67.png](attachments/Untitled%207%2067.png)

# Summary

![Untitled 8 63.png](attachments/Untitled%208%2063.png)