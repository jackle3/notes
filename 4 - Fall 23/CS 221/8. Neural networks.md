---
Date: 2023-10-04
---
# Non-linear predictors

![[attachments/Untitled 107.png|Untitled 107.png]]

- Linear predictors were linear in two ways. The feature vector $\phi(x)$﻿ was linear, and the way it interacted with the weight $w$﻿ was also linear.
- The quadratic predictor left the linearity alone of the above the same. We just added terms to the feature vector $\phi(x)$﻿.
- Finally, for neural networks, we leave $\phi(x)$﻿ alone but play with the way it interacts with our prediction.
    - Leave the feature extractor alone, but increase the complexity of predictor, which can also produce non-linear (though not necessarily quadratic) predictors

# Motivating example

![[attachments/Untitled 1 73.png|Untitled 1 73.png]]

- Any example in between the lines is unsafe, which means they collide. The two cars $x_1$﻿ and $x_2$﻿ are too close.

# Decomposing the problem

![[attachments/Untitled 2 73.png|Untitled 2 73.png]]

- Just define each decision boundary as it’s own. Then, we check if the set of cars are outside the boundary, for each.

## Rewriting with vector notation

![[attachments/Untitled 3 73.png|Untitled 3 73.png]]

- To get $\mathbf h(x)$﻿, we just stack our two vectors $h_i(x)$﻿.

# Avoiding zero gradients

![[attachments/Untitled 4 70.png|Untitled 4 70.png]]

- The gradient of the red line is zero everywhere. We replace this function with activation functions to avoid non-zero gradients.

# Two-layer neural networks

![[attachments/Untitled 5 70.png|Untitled 5 70.png]]

- We start with a feature vector $\phi(x)$﻿. We then multiply it by a weight matrix $V$﻿. We then put it through an activation function $\sigma$﻿. This generates a vector $\mathbf h(x)$﻿.
    - The rows of the weight matrix $V$﻿ can be interpreted as the weight vectors of the $k$﻿ intermediate subproblems.
    - It’s like the $[-1, +1, -1]$﻿ from $h_1$﻿ above.
- Now with the $\mathbf h(x)$﻿, I can easily do prediction to classify it. $\mathbf h(x)$﻿ is simply a smarter and leanred feature representation, as compared to the original $\phi(x)$﻿.
- The neural network decomposes a problem into a bunch of smaller subproblems, which is captured by $\mathbf h(x)$﻿.

# Deep neural networks

![[attachments/Untitled 6 69.png|Untitled 6 69.png]]

- For more layers, basically just rinse and repeat with more weight matrices $V_i$﻿ and repeated applications of the activation layer.

## Layers in neural networks

- Each layer represents another level of abstraction.

![[attachments/Untitled 7 67.png|Untitled 7 67.png]]

# Summary

![[attachments/Untitled 8 63.png|Untitled 8 63.png]]