---
Date: 2023-10-23
---
# Reinforcement Learning

- In reinforcement learning, we are given an MDP where we don’t know the transitions or rewards. We still want to maximize expected utility (sum of discounted rewards)

![Untitled 104.png](../../attachments/Untitled%20104.png)

- In reinforcement learning, we should take good actions to get rewards, but in order to know which actions are good, we need to explore and try different actions

## MDP vs RL

- In MDPs, you have a **mental model of how the world works**. You go lock yourself in a room, think really hard, and **come up with a policy**. Then you come out and use it to act in the real world.
    - The mental model basically means you know the transition functions and the rewards. You know the rules of the world.
- In RL, you **don’t know how the world works**, but you only have **one life**, so you just have to go out into the real world and learn how it works from experiencing it and trying to **take actions that yield high rewards**.

## RL Framework

- We can think of an agent (the reinforcement learning algorithm) that repeatedly chooses an action $a_t$﻿ to perform in the environment, and receives some reward $r_t$﻿, and information about the new state $s_t$﻿.
- There are two questions here:
    - How I’m acting right now and how to choose actions (what is $\pi_{\text{act}}$﻿)
        - This is known as the acting policy, data collection policy, and exploration policy.
    - How to update the parameters of the policy once you take an action
        - Updating things you don’t know, like transition functions, rewards, or Q-values

![Untitled 1 70.png](../../attachments/Untitled%201%2070.png)

# Updating parameters

## Model-based Monte Carlo

- In model-based methods, we try to figure out the transition probabilities and the rewards (finding the model), then using that to find the solution.
- We start by just running an **arbitrary policy** to collect some data. Then, we estimate
    
    ![Untitled 2 70.png](../../attachments/Untitled%202%2070.png)
    
    - For transitions, we estimate by count the number of times we see a particular state $s$﻿ and action $a$﻿, and the number of times we see that state and action leading to us to another state $s'$﻿
    - The reward is just the reward that the transition had.

### Value Iteration

![Untitled 3 70.png](../../attachments/Untitled%203%2070.png)

- Above is one run of this policy. There are four copies of `in; stay, 4, in` and one time where we say `in; stay 4, end`
    - With this, T(in, stay, in) = 3/4 and T(in, stay, end) = 1/4
    - As for the rewards, R(in, stay, in) = 4, and R(in, stay, end) = 4
- We can do multiple runs and keep a running count.

![Untitled 4 67.png](../../attachments/Untitled%204%2067.png)

- Estimates converge to true values (under certain conditions)
    - With estimated MDP $(\hat{T} , \hat{Reward})$﻿, compute policy using value iteration

### Problem

![Untitled 5 67.png](../../attachments/Untitled%205%2067.png)

- If the acting policy does not explore a certain action, we will never find out what the probabilities and the rewards of that edge is.

### Model-based to model-free

- Notice that we are just trying to find $\hat{T}$﻿ and $\hat{Reward}$﻿ so that we can calculate the estimate for $\hat{Q}_\text{opt}$﻿ because we need that for value iteration.
    
    ![Untitled 6 66.png](../../attachments/Untitled%206%2066.png)
    
    - Instead, we can just estimate $\hat{Q}_\text{opt}$﻿ direction, instead of estimating the transitions and rewards.

## Model-free Monte Carlo

- We gather data, and then we estimate the value of $Q_\pi$﻿

![Untitled 7 64.png](../../attachments/Untitled%207%2064.png)

### Average of Utilities

- Similar to before, we collect some data following some policy $\pi$﻿
    
    ![Untitled 8 61.png](../../attachments/Untitled%208%2061.png)
    
- Suppose we run multiple episodes of our reinforcement learning algorithm and we got various occurences of state and action pairs $(s, a)$﻿.
    - We want to find estimate $\hat{Q}_\pi (s, a)$﻿. Note that $Q_\pi(s, a)$﻿ is the true value.
        
        ![Untitled 9 59.png](../../attachments/Untitled%209%2059.png)
        
- With that in mind, we can basically find the expected utility by taking an average of the utilities of each state and action pair $(s, a)$﻿ that we have in our episodes.
    
    ![Untitled 10 57.png](../../attachments/Untitled%2010%2057.png)
    
    - Notice that we only consider the utility of the first occurence of $(s, a)$﻿ in that episode.
    
    ![Untitled 11 56.png](../../attachments/Untitled%2011%2056.png)
    
- Observe that $Q_π(s_{t−1}, a_t) = E[u_t]$﻿; that is, if we’re at state $s_{t−1}$﻿ and take action $a_t$﻿, the average value of $u_t$﻿ is $Q_π(s_{t−1}, a_t)$﻿

### Example

- In this episode, we get an estimate for our $Q_\pi(in, stay)$﻿ to be 4.
    
    ![Untitled 12 55.png](../../attachments/Untitled%2012%2055.png)
    
- In this episode, we update our policy for $Q_\pi(in, stay)$﻿
    
    ![Untitled 13 53.png](../../attachments/Untitled%2013%2053.png)
    
- In this episode, we run it again, and update our $Q_{\pi}$﻿ again.
    
    ![Untitled 14 51.png](../../attachments/Untitled%2014%2051.png)
    

![Untitled 15 49.png](../../attachments/Untitled%2015%2049.png)

- Note that we are estimating $Q_\pi$﻿. We are finding the utilities by finding a specific policy. Therefore, the estimate is based on that specific policy.

### Convex combination

![Untitled 16 47.png](../../attachments/Untitled%2016%2047.png)

![Untitled 17 44.png](../../attachments/Untitled%2017%2044.png)

- This is basically the average of the utilities.

### Stochastic gradient descent

![Untitled 18 42.png](../../attachments/Untitled%2018%2042.png)

- We want our prediction to be as close as possible to our target. This is the same as the average of utilities.

## SARSA

- Model-free Monte Carlo’s target was u, the discounted sum of rewards after taking an action. However, u itself is just an estimate of Qπ(s, a).
    - If the episode is long, u will be a bad estimate. This is because u only corresponds to one episode out of an exponential (in the episode length) number of possible episodes, so as the epsiode lengthens, it becomes an increasingly less representative sample of what could happen.
- SARSA’s target is a combination of the data (the first step) and the estimate (for the rest of the steps). In contrast, model-free Monte Carlo’s u is taken purely from the data
    
    ![Untitled 19 39.png](../../attachments/Untitled%2019%2039.png)
    
    - Instead of looking at the full utility, it looks at the rewards and connects it with the previous estimate.

![Untitled 20 38.png](../../attachments/Untitled%2020%2038.png)

## Q-learning

![Untitled 21 35.png](../../attachments/Untitled%2021%2035.png)

- Value iteration is the model-based method of getting $Q_{opt}$﻿.
    - Q-learning is the model-free method.

![Untitled 22 31.png](../../attachments/Untitled%2022%2031.png)

- For each state, action, reward, and successor state
    - We estimate Q_opt using a convex combination of what I already know (the prediction is the current estimate) and the new data.
    - For the target, instead of putting utility, we put the reward plus the discounted estimate of $\hat{V}$﻿.
        - This estimate is the max $Q$﻿ over all polices. Basically, we don’t commit outself to just one policy, so we don’t have a single utility value of $u$﻿.

### SARSA vs Q-learning

- In SARSA, we were actually sampling an action that we should take. Here, we are not sampling an action, so we put a max over all actions we can take.

![Untitled 23 28.png](../../attachments/Untitled%2023%2028.png)

## Summary

- By running the algorithm, though our average lifetime utility won’t change much, we will figure out the Q_opt of the optimal policy, allowing us to backtrack and find the optimal policy.

![Untitled 24 27.png](../../attachments/Untitled%2024%2027.png)

## On and Off Policy

![Untitled 25 24.png](../../attachments/Untitled%2025%2024.png)

- On-policy means the thing we are estimating is the same as the policy that is giving us our data.
- Model-free Monte Carlo is an on-policy algorithm. The acting policy is $\pi$﻿, and the thing we are estimating is $\hat{Q}_\pi(s, a)$﻿
- Q-learning is an off-policy algorithm. The acting policy is $\pi$﻿, and the thing we are estimating is $\hat{Q}_\text{opt}(s, a)$﻿
- Monte Carlo and SARSA were kind of evaluating the policy by finding $Q_\pi$﻿, while this is estimating the optimal policy by finding $Q_{opt}$﻿

# Choosing the exploration policy

![Untitled 26 21.png](../../attachments/Untitled%2026%2021.png)

- We want to make sure that $\pi_{act}$﻿ is exploring enough so that we see enough states.
- The two extremes are exploration and exploitation:
- In exploitation, we greedily explore until we find a good state, then we just stay there and exploit it.
    
    ![Untitled 27 19.png](../../attachments/Untitled%2027%2019.png)
    
- In exploration, we just explore all the time. We don’t stay and exploit any positions. This essentially says that $\pi_{act}$﻿ is the policy that randomly moves around.
    
    ![Untitled 28 17.png](../../attachments/Untitled%2028%2017.png)
    
- We need to balance **exploration** and **exploitation**.
    - This is basically like choosing to either continue doing what you enjoy or is good for you, or explore new things to try.

## Epsilon-greedy

![Untitled 29 16.png](../../attachments/Untitled%2029%2016.png)

- In the example above, we start with an epsilon of 1 so that we are very open to exploration. Then, we switch to 0.5 to balance it. Then, we switch to 0 to exploit the estimate of $\hat{Q}_{opt}$﻿ that we have so far.

# Generalization

- With our exploration algorithm so far, we basically have to explore all the states and figure out their Q_opt.

![Untitled 30 16.png](../../attachments/Untitled%2030%2016.png)

- WIth how we are learning so far with Q-learning, it does not scale very well.
    
    ![Untitled 31 14.png](../../attachments/Untitled%2031%2014.png)
    
    - We’ve just been memorizing Q-values for each (s, a), treating each pair independently.
    - In other words, we haven’t been generalizing, which is actually one of the most important aspects of learnin

## Function approximation

![Untitled 32 14.png](../../attachments/Untitled%2032%2014.png)

- Function approximation fixes this by parameterizing Q_opt by a weight vector and a feature vector, as we did in linear regression.
- Recall that features are supposed to be properties of the state-action (s, a) pair that are indicative of the quality of taking action a in state s.
- The effect is that all the states that have similar features will have similar Q-values.
    - For example, suppose φ included the feature 1[s = (∗, 4)].
    - If we were in state (1, 4), took action E, and managed to get high rewards, then Q-learning with function approximation will propagate this positive signal to all positions in column 4 taking any action.
    - In our example, we defined features on actions (to capture that moving east is generally good) and features on states (to capture the fact that the 6th column is best avoided, and the 5th row is generally a good place to travel to
- Now, instead of updating Q_opt, we update w to parameterize it

![Untitled 33 14.png](../../attachments/Untitled%2033%2014.png)