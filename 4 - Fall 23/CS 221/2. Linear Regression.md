---
Date: 2023-10-02
---
# Reflex-based models

![Untitled 113.png](../../attachments/Untitled%20113.png)

- It’s a predictor $f$﻿ that takes some input $x$﻿ and returns some output $y$﻿.

## Binary classification

- This is an example of a reflex-based model

![Untitled 1 79.png](../../attachments/Untitled%201%2079.png)

- $f$﻿ is the classifier, it takes some input $x$﻿ and produces a label $y$﻿

## Regression

- This is the second major type of prediction task we’ll cover

![Untitled 2 79.png](../../attachments/Untitled%202%2079.png)

- It predicts a real number, called the response

## Structured prediction

- This is the third type of prediction task

![Untitled 3 79.png](../../attachments/Untitled%203%2079.png)

- This predicts complex objects, instead of just numbers like the previous types.

# Linear Regression

![Untitled 4 75.png](../../attachments/Untitled%204%2075.png)

- The learning algorithm takes in the training data that we have, and produces a predictor.
    - In linear regression, the predictor is a line. It takes in input and gives output, which is the y position on the line.
- For each of the design decisions, the red text corresponds to the terminology for each of the questions.
    - For any learning problem, just go through these three steps and specify them.

## Hypothesis class

- Which predictors should we consider?

![Untitled 5 75.png](../../attachments/Untitled%205%2075.png)

- For linear regression, the predictors are linear lines, of the form $f(x) = w_1 + w_2x$﻿.
- The feature extractor for linear regression is $\phi(x) = [1, x]$﻿.
    - $\phi$﻿ is the **translation** between the **underlying raw object** (string, integer, etc) into a **feature vector** that the machine learning algorithm can understand
- The hypothesis class is a set of functions that output scores. They are the predictors that we are considering, or the possible functions we want our algorithm to consider.

## Loss function

- How do we measure how good a predictor is?

![Untitled 6 74.png](../../attachments/Untitled%206%2074.png)

- One of the loss functions used for linear regression is the **squared** **loss** function
    - It takes the squared difference between the actual data and the predicted data
        - The difference between the actual and prediction is called the **residual**
- The **TrainLoss** function is the average over all the loss functions for every pair.
    - Loss function depends on a particular $(x, y)$﻿ pair, and train loss doesn’t (it’s the overall average of all the pairs).

### Visualization

![Untitled 7 72.png](../../attachments/Untitled%207%2072.png)

- For every value of $w_1$﻿ and $w_2$﻿, we have a train loss. We can define the optimization algorithm as finding the $\mathbf w$﻿ that minimizes the train loss.
    - The best predictor is the one with the lowest training loss.

## Optimization

![Untitled 8 67.png](../../attachments/Untitled%208%2067.png)

- The gradient tells us which way not to move. You go in the opposite direction.
- Squared loss functions are convex, so if you find a critical point, then it is the minimum.
- **Iterative optimization:** we start with some $\mathbf w$﻿ and keep on tweaking it to make the objective function go down.
    - Epochs and the step size are hyperparameters.
    - $T$﻿ is the number of epochs → the number of iterations where we perform the gradient descent update.
    - The **step size** specifies how aggressively we want to pursue a direction.

## Computing Gradients

![Untitled 9 64.png](../../attachments/Untitled%209%2064.png)

- The gradient of a sum is the sum of the gradient.
- The gradient of an expression squared is twice that expression times the gradient of the expression.
- The gradient of a dot product $\mathbf w \cdot \phi(x)$﻿ is just $\phi(x)$﻿.
- For the squared loss function, the gradient is the residual (prediction - target) times the feature vector.
    - If we’re overshooting, so prediction is larger than target, we have a positive $\phi(x)$﻿. Since we go in the opposite direction, we basically subtract $\phi(x)$﻿.
    - If we’re undershooting, so prediction is less than target, it’s the same as above but reversed.
    - If the prediction equals the target, we don’t move from there.
- **Important note:** no matter what the loss function is, the gradient wrt $\mathbf w$﻿ is always something times $\phi(x)$﻿ because $\mathbf w$﻿ only affects the loss through $\mathbf w \cdot \phi(x)$﻿

### Gradient Descent Example

![Untitled 10 62.png](../../attachments/Untitled%2010%2062.png)

- If you go for a long time, eventually the gradient will settle at zero, which shows the algorithm has converged.

### Gradient Descent in Python

- Here, Percy just simply implements the equations that we have previously into code.

![Untitled 11 61.png](../../attachments/Untitled%2011%2061.png)

- Here, Percy implements the gradient descent algorithm

![Untitled 12 60.png](../../attachments/Untitled%2012%2060.png)

- Here, `F` is the value that we want to minimize, and `gradientF` gives us the gradient of `F`.

## Summary

- For linear regression, the feature extractor is usually always $\phi(x)$﻿.

![Untitled 13 58.png](../../attachments/Untitled%2013%2058.png)