---
Date: 2023-10-04
---
# 1 Motivating Example

- Suppose we are doing regression with a four-layer neural network.
- We have each $V_i$﻿ for each hidden layer, as well as the weights vector $w$﻿.

![Untitled 114.png](../../attachments/Untitled%20114.png)

- In order to train the network using SGD, we would need to compute the gradient of the loss function with respect to each of the parameters, for each data point.
    - That’s a lot of manual math work.

# 2 Computation graphs

![Untitled 1 80.png](../../attachments/Untitled%201%2080.png)

- In the example above, the root node is the full loss function.
- This computation graph allows us to apply backpropagation to the computation graph and automatically compute the gradients.

# 3 Functions as boxes

![Untitled 2 80.png](../../attachments/Untitled%202%2080.png)

- Each of the graphs above are simple computation graphs.
    - $a$﻿ and $b$﻿ point into the box labeled $+$﻿, and that result is labeled $c$﻿.
- The connection of the graph is the gradient / partial derivative wrt to the variable.
    - The meaning of the gradient is basically just how much does $c$﻿ change if $a$﻿ or $b$﻿ changes a little bit?
    - For the left example, a small change $\epsilon$﻿ in $a$﻿ changes the result by $1 \epsilon$﻿. As such, the multiplicative factor of the change is $1$﻿.

## 3.1 Basic building blocks

![Untitled 3 80.png](../../attachments/Untitled%203%2080.png)

- From left to right, top to bottom, the functions are:
    - $a + b$﻿
    - $a - b$﻿
    - $a * b$﻿
    - $a^2$﻿
    - $\max(a, b)$﻿
        - If $a > b$﻿, then changing $a$﻿ a little bit will produce a change of $1\epsilon$﻿ in the result. Otherwise, it has no change. As such, the gradient is the indicator function.
    - $\sigma(a)$﻿
        - This is the logistic activation function.

## 3.2 Function composition

![Untitled 4 76.png](../../attachments/Untitled%204%2076.png)

- You can just follow the paths to get the chain rule result. This function is $a^4 = (a^2)^2$﻿.
    - Note that $b = a^2$﻿, and $c = b^2$﻿.

# 4 Linear classification with hinge loss

- We build the computation graph of the loss function.

![Untitled 5 76.png](../../attachments/Untitled%205%2076.png)

- To get the gradient, we basically just traverse through the graph from loss until we get to $\mathbf w$﻿.
    - It’s just the product of all the edges in the path.

# 5 Two-layer neural networks

![Untitled 6 75.png](../../attachments/Untitled%206%2075.png)

# 6 Backpropagation

![Untitled 7 73.png](../../attachments/Untitled%207%2073.png)

- Backpropagation is the general procedure for computing gradients.

## 6.1 Forward step

- The forward step walks through each $f_i$﻿ from the leaves back to the root.
    - It’s purpose is to evaluate the loss at our current stage.
- In the example above, our leaves are $w$﻿ and $\phi(x)$﻿. We do the dot product to get our score, and repeat that until we get to the loss for our root.

## 6.2 Backward step

- We compute the backward value $g_i$﻿ for every node, showing how $f_i$﻿ influences the loss.
    - We start from the root, which is the loss, and traverse back to $w$﻿.
- In the above example:
    - The gradient of the loss with respect to the loss is $1$﻿.
    - The gradient of the loss with respect to the residual is $1 * 2(3) = 6$﻿.
        - The $3$﻿ came from the value of the residual that we found on the forward pass.
- The backprop algorithm gets us the value of the gradient of the loss with respect to $w$﻿.

# 7 Optimization

![Untitled 8 68.png](../../attachments/Untitled%208%2068.png)

- For non-convex loss functions, there is no guarantee that we will converge to the global minimum, let alone the local minimum.

# 8 Training neural networks

![Untitled 9 65.png](../../attachments/Untitled%209%2065.png)

- Initialization (where you start the weights) matters for non-convex optimization. Unlike for linear models, you can’t start at zero or else all the subproblems will be the same (all rows of V will be the same). Instead, you want to initialize with a small amount of random noise.
- It is common to use overparameterized neural networks, ones with more hidden units (k) than is needed, because then there are more ”chances” that some of them will pick out on the right signal, and it is okay if some of the hidden units become ”dead”.
- There are small but important extensions of stochastic gradient descent that allow the step size to be tuned per weight.
- It is important to monitor the gradients while training.
    - If they vanish (get too small), then training won’t make progress.
    - If they explode (get too big), then training will be unstable.

# 9 Summary

![Untitled 10 63.png](../../attachments/Untitled%2010%2063.png)

- **Computation graph** allows us to represent arbitrary mathematical expressions, which can just be built out of simple building blocks.
    - They hopefully have given you a more visual and better understanding of what gradients  
        are about.  
        
- The backpropagation algorithm allows us to simply write down an expression, and never have to take a gradient manually again.
    - However, it is still important to understand how the gradient arises, so that when you try to train a deep neural network and your gradients vanish, you know how to think about debugging your network.
- The generality of computation graphs and backpropagation makes it possible to iterate very quickly on new types of models and loss functions.