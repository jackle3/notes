---
Date: 2023-10-04
---
# Feature extraction & learning

![[attachments/Untitled 96.png|Untitled 96.png]]

- Given a set of all predictors, the feature extractor constrains us to a subset of those predictors.
    - The learning algorithm tells us to choose a particular $f_{\mathbf w} \in F$﻿ to be the final predictor.

## Feature extraction with feature names

![[attachments/Untitled 1 63.png|Untitled 1 63.png]]

## Prediction with feature names

![[attachments/Untitled 2 63.png|Untitled 2 63.png]]

- Think of like each weight vector having a vote, and the final score is our result.
    - If $w_j$﻿ is positive, it is voting in favor, and vice versa.
    - The magnitude of $w_j$﻿ is the strength of the vote.

# Feature template

![[attachments/Untitled 3 63.png|Untitled 3 63.png]]

- Instead of defining individual features like `endsWith_com`, we can define a single template which expands into all features that checks whether the input ends with any three letters.
- Maybe we don’t know what the last three characters can be. If we define the entire pattern of all of them, and let the algorithm learn to find which feature is relevant.
    - We don’t need to know which particular patterns (e.g., three-character suffixes) are useful, but only that existence of certain patterns (e.g., three-character suffixes) are useful cue to look at.

## Example 1

![[attachments/Untitled 4 62.png|Untitled 4 62.png]]

## Example 2

![[attachments/Untitled 5 62.png|Untitled 5 62.png]]

# Feature vector implementations

![[attachments/Untitled 6 61.png|Untitled 6 61.png]]

# Summary

![[attachments/Untitled 7 60.png|Untitled 7 60.png]]

- The primary question of this was how to define the hypothesis class $F$﻿, which is sometimes a question of what the feature extractor $\phi$﻿ is.