---
Date: 2023-10-02
---
# Linear Classification

- Classification is when you want to take an input and classify it into buckets

![[attachments/Untitled 98.png|Untitled 98.png]]

- This graph is now just input space. It plots the dimensions of our inputs, and the color gives us the label.
- We visualize the classifier by the decision boundary, which in this case cuts the input space into two halves, for each of our labels.

## Example Linear Classifier

- Our classifier now looks a the sign of the dot product of a weight vector and a feature vector.

![[attachments/Untitled 1 65.png|Untitled 1 65.png]]

- The dot product tells us the cosine of the angle.
- $\mathbf w$﻿ is the small red arrow that you see on the graph. If the angle between that and the data point is acute (e.g. cosine is positive) then it labels it 1, otherwise it labels is -1.
    - The dot product reasons it geometrically. Acute angles go on on side, obtuse on the other.
- The weight vector is orthogonal to the decision boundary.
- If a point is on the decision boundary, it basically means “I don’t know”

## Hypothesis Class

- What classifiers should we consider?

![[attachments/Untitled 2 65.png|Untitled 2 65.png]]

- Notice that the dot product is the same as regression. Now, we just look at the sign of the score instead of the score itself.

## Loss function

- How do we measure how good a classifier is?
- One example loss function is the zero-one loss function.

![[attachments/Untitled 3 65.png|Untitled 3 65.png]]

- If the model predicts correctly, then the loss for that point is $0$﻿. If it is incorrect, the loss is $1$﻿.
- The $\mathbf 1$﻿ is an indicator function that returns one if the boolean is true, and zero if not.

### Notation: Score and margin

![[attachments/Untitled 4 63.png|Untitled 4 63.png]]

- For the scores, the ones in the yellow section (+1) have very high scores. The ones near the decision boundary have near zero scores. The ones in the blue section (-1) have negative scores.
- You can be confident but incorrect. The margin measures how correct we are.
    - If the score and the actual target $y$﻿ have opposite signs, then the margin is negative and you are incorrect. If they are both positive, then you are correct.

### Notation with Zero One Loss

![[attachments/Untitled 5 63.png|Untitled 5 63.png]]

- When the margin is negative, we are incorrect.

## Optimization

![[attachments/Untitled 6 62.png|Untitled 6 62.png]]

- Since the gradient is zero, the algorithm is not moving. Basically it thinks that if it moves, it is still wrong, so it doesn’t know where to move.

### Hinge Loss

- We want a loss function where when we are wrong, there is some pressure to get it right.

![[attachments/Untitled 7 61.png|Untitled 7 61.png]]

- The 1 is there to provide some buffer: we ask the classifier to predict not only correctly, but by a (positive) margin of safety.

### Logistic Regression

![[attachments/Untitled 8 58.png|Untitled 8 58.png]]

- The main property of the logistic loss is no matter how correct your prediction is, you will have non-zero loss, and so there is still an incentive (although a diminishing one) to push the loss down by increasing the margin

### Gradient of hinge loss

![[attachments/Untitled 9 57.png|Untitled 9 57.png]]

- To take the gradient with the max, just do piecewise with whichever function is active at that point in time.

## Code

- There is very little change needed in the code between linear regression and linear classification

![[attachments/Untitled 10 55.png|Untitled 10 55.png]]

- The optimization algorithm does not change at all, because we still do gradient descent.

# Summary

![[attachments/Untitled 11 55.png|Untitled 11 55.png]]