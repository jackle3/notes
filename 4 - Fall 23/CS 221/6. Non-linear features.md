---
Date: 2023-10-04
---
# Linear regression

![[attachments/Untitled 101.png|Untitled 101.png]]

- The hypothesis class in the picture above is a linear predictor. The feature extractor gives us a linear function.
    - Remember that the feature extractor for linear regression is $\phi(x) = [1, x]$﻿

## Non-linear predictors

- The **key idea** is that the feature predictor can be arbitrary.

![[attachments/Untitled 1 68.png|Untitled 1 68.png]]

- Note that here, $x \in \R$﻿, so getting $x^2$﻿ is easy. For larger dimensions, this can be an issue.

## Piecewise constant predictors

![[attachments/Untitled 2 68.png|Untitled 2 68.png]]

## Predictors with periodicity structure

![[attachments/Untitled 3 68.png|Untitled 3 68.png]]

- Features represent what properties **might** be useful for prediction.
    - If a feature ends up not being useful, it’s weight will just be close to zero.
    - **Disadvantage:** the more features one has, the harder learning becomes

# Linearity

- Remember that we are still using the optimization algorithms and things used for linear predictors. However, this works because we are still linear wrt $\mathbf w$﻿.

![[attachments/Untitled 4 65.png|Untitled 4 65.png]]

- The efficiency of the score makes it easy to do gradient descent.

# Linear classification

![[attachments/Untitled 5 65.png|Untitled 5 65.png]]

- Linearity with classification refers to the linearity of the decision boundary line.

## Quadratic classifier

- If needed, we can make our decision boundary a circle.

![[attachments/Untitled 6 64.png|Untitled 6 64.png]]

- The piecewise function basically means the label is $1$﻿ if you are in the circle, and $-1$﻿ otherwise.

# Visualization in feature space

![[attachments/Untitled 7 63.png|Untitled 7 63.png]]

- The purple is the slice of the linear predictor, inducing a circle in the original 2D space.

# Summary

![[attachments/Untitled 8 60.png|Untitled 8 60.png]]