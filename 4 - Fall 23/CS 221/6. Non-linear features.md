---
Date: 2023-10-04
---
# Linear regression

![Untitled 101.png](attachments/Untitled%20101.png)

- The hypothesis class in the picture above is a linear predictor. The feature extractor gives us a linear function.
    - Remember that the feature extractor for linear regression is $\phi(x) = [1, x]$﻿

## Non-linear predictors

- The **key idea** is that the feature predictor can be arbitrary.

![Untitled 1 68.png](attachments/Untitled%201%2068.png)

- Note that here, $x \in \R$﻿, so getting $x^2$﻿ is easy. For larger dimensions, this can be an issue.

## Piecewise constant predictors

![Untitled 2 68.png](attachments/Untitled%202%2068.png)

## Predictors with periodicity structure

![Untitled 3 68.png](attachments/Untitled%203%2068.png)

- Features represent what properties **might** be useful for prediction.
    - If a feature ends up not being useful, it’s weight will just be close to zero.
    - **Disadvantage:** the more features one has, the harder learning becomes

# Linearity

- Remember that we are still using the optimization algorithms and things used for linear predictors. However, this works because we are still linear wrt $\mathbf w$﻿.

![Untitled 4 65.png](attachments/Untitled%204%2065.png)

- The efficiency of the score makes it easy to do gradient descent.

# Linear classification

![Untitled 5 65.png](attachments/Untitled%205%2065.png)

- Linearity with classification refers to the linearity of the decision boundary line.

## Quadratic classifier

- If needed, we can make our decision boundary a circle.

![Untitled 6 64.png](attachments/Untitled%206%2064.png)

- The piecewise function basically means the label is $1$﻿ if you are in the circle, and $-1$﻿ otherwise.

# Visualization in feature space

![Untitled 7 63.png](attachments/Untitled%207%2063.png)

- The purple is the slice of the linear predictor, inducing a circle in the original 2D space.

# Summary

![Untitled 8 60.png](attachments/Untitled%208%2060.png)