---
Date: 2023-10-09
---
# Supervised learning - classification

![Untitled 105.png](../../attachments/Untitled%20105.png)

- We’re given a set of labeled training examples.
- A learning algorithm produces a classifier that can classify new points.
    - Note that we’re now plotting the (two-dimensional) feature vector rather than the raw input, since the learning algorithms only depend on the feature vectors.
- The **main challenge** with supervised learning is that it can be expensive to collect the labels.

# Unsupervised learning - clustering

![Untitled 1 71.png](../../attachments/Untitled%201%2071.png)

- In clustering, you are only given unlabeled training examples.
- Our goal is to assign each point to a cluster. In this case, there are two clusters, 1 (blue) and 2 (orange).
    - Intuitively, nearby points should be assigned to the same cluster.
    - The advantage of unsupervised learning is that unlabeled data is often very cheap.

# Clustering task

![Untitled 2 71.png](../../attachments/Untitled%202%2071.png)

- Take a set of points as input and return a partitioning of the points into K clusters.
    - We will represent the partitioning using an assignment vector $z = [z_1, \dots, z_n]$﻿.
    - For each point i, $z_i \in \{1, . . . , K\}$﻿ specifies which of the K clusters that i is assigned to

## Centroids

![Untitled 3 71.png](../../attachments/Untitled%203%2071.png)

- We want to place each point (its feature vector representation) to be close to its assigned centroid.
    - Point $x_i$﻿ is assigned to cluster $z_i$﻿, and $\mu_{z_i}$﻿ is the centroid of that cluster.

# K-means objective function

![Untitled 4 68.png](../../attachments/Untitled%204%2068.png)

- The **k-means Objective** or loss, $\text{Loss}_{\text{kmeans}}(z, \mu)$﻿, is defined as the sum of squared distances from each point $\phi(x_i)$﻿ to its assigned centroid $\mu_{z_i}$﻿.
    - Takes in the assignments of each point and the centroids of each cluster.
    - This loss can be interpreted as a reconstruction loss: imagine replacing each data point by its assigned centroid. Then the objective captures how lossy this compression was.
- The goal is to minimize the k-means loss with respect to both the cluster assignments $z$﻿ and the centroids $\mu$﻿.
    - We can decide different assignments, and we can also decide different centroids.

### k-means Algorithm

![Untitled 5 68.png](../../attachments/Untitled%205%2068.png)

- **Initialization**: Centroids $\mu = [\mu_1, ..., \mu_K]$﻿ are initialized randomly.
- **Iteration**: The algorithm iteratively alternates between two steps:
    - **Step 1**: Set assignments $z$﻿ given centroid $\mu$﻿.
        - For each point $i = 1, ..., n$﻿:
            - $z_i$﻿ is assigned to the cluster $k$﻿ that minimizes the distance between the point $\phi(x_i)$﻿ to the centroid $\mu_k$﻿.
    - **Step 2**: Set centroids $\mu$﻿ given assignments $z$﻿.
        - For each cluster $k = 1, ..., K$﻿:
            - $\mu_k$﻿ is set to the average of all the points assigned to cluster $k$﻿.
- In the first iteration, we first lock the centroids and try to assign each point to the closest centroid.
    
    ![Untitled 6 67.png](../../attachments/Untitled%206%2067.png)
    
- Then, also in the first iteration, we use the assignments to adjust the centroid.
    
    ![Untitled 7 65.png](../../attachments/Untitled%207%2065.png)
    

### Local Minima and Solutions

- k-means is guaranteed to converge to a local minimum but not guaranteed to find the global minimum.
- Solutions include running multiple times from different random initializations or using a heuristic for initialization (e.g., k-means++).

### Summary

- k-means is a widely-used method for discovering cluster structure in data, and it can refer to both the objective and the algorithm.
- The k-means objective is defined as the sum of squared differences between a point and its assigned centroid.
- The k-means algorithm performs alternating optimization on the k-means objective.
- Clustering, an instance of unsupervised learning, can be used for exploring unlabeled datasets and learning representations useful for downstream supervised applications