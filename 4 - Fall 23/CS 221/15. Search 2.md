---
Date: 2023-10-16
---
# Dynamic programming

- DP is backtracking search with **memoization**. It only works for acyclic graphs.

![[attachments/Untitled 106.png|Untitled 106.png]]

- Starting from a starting state `s`, we can get to various states `s'`. Each of these have a `FutureCost` to go from that state to the end state, so we can just add it.
    - In this case, `s'` is equal to `Succ(s, a)`
    - `FutureCost(s)` is the cost of the minimum cost path from s to some end state.

## Motivating task

![[attachments/Untitled 1 72.png|Untitled 1 72.png]]

- Notice that all the subtrees under city `5` are identical. Traversing this is a lot of repeated work.
- We can cache the solution for the `FutureCost` from city `5`.
    - The `FutureCost` is a function of the state that we are in.
        - If we use dynamic programming, the state is just the city.
        - If we didn’t, the state would be the entire history — all the past sequences of actions that we took

![[attachments/Untitled 2 72.png|Untitled 2 72.png]]

- Using DP allows us to condense the original tree into this simplified graph.
    - We no longer have a tree, but a directed acyclic graph with only $n$﻿ nodes.

## State

![[attachments/Untitled 3 72.png|Untitled 3 72.png]]

## Basic DP algorithm

![[attachments/Untitled 4 69.png|Untitled 4 69.png]]

![[attachments/Untitled 5 69.png|Untitled 5 69.png]]

# Practice with making states

## Can’t go three odd in a row

![[attachments/Untitled 6 68.png|Untitled 6 68.png]]

- Starting from city `3`, we know that the previous city was odd, so we can’t go to another odd city without violating our constraint.
- The choice of state matters a lot, because it can significantly change the size of our state space.
    
    ![[attachments/Untitled 7 66.png|Untitled 7 66.png]]
    

## Must visit at least three odd

![[attachments/Untitled 8 62.png|Untitled 8 62.png]]

![[attachments/Untitled 9 60.png|Untitled 9 60.png]]

---

# Uniform cost search

![[attachments/Untitled 10 58.png|Untitled 10 58.png]]

![[attachments/Untitled 11 57.png|Untitled 11 57.png]]

- Uniform cost search allows us to handle cycles, assuming costs are non-negative.
- Called uniform cost search because for every “iteration”, it uniformly expands its frontier until we reach the target.

## High-level strategy

- There are three sets of nodes that we keep track of in uniform cost search
    
    - We move states from unexplored to frontier, then from frontier to explored.
    
    ![[attachments/Untitled 12 56.png|Untitled 12 56.png]]
    
- The key idea is that we have computed the minimum cost paths to all the nodes in the explored set.
    - When an end state moves into the explored set, then we are done (we know the answer)
- The frontier is implemented with a priority queue
- This algorithm is basically Dijkstra’s
    - The difference is that this search problem goes from node A to B, and end once we reach the best path to state B
    - With Djikstra, we find paths from node A to every other node

## Example

- **Step 1 - original state**
    
    ![[attachments/Untitled 13 54.png|Untitled 13 54.png]]
    
- **Step 2 - we start with A, our initial state**
    
    ![[attachments/Untitled 14 52.png|Untitled 14 52.png]]
    
    - We first add A to the frontier. Then, since A is the only element, we pop off A and add it to the explored state.
- **Step 3 - we explore all the children of A and add it to the frontier**
    
    ![[attachments/Untitled 15 50.png|Untitled 15 50.png]]
    
- **Step 4**
    
    - After exploring B, we see that a new path to C is found. We can update our frontier.
    
    ![[attachments/Untitled 16 48.png|Untitled 16 48.png]]
    
- **Step 5 - repeat until finish**
    
    ![[attachments/Untitled 17 45.png|Untitled 17 45.png]]
    
    - We stop once our target node gets into the explored set.

## Pseudocode

![[attachments/Untitled 18 43.png|Untitled 18 43.png]]

![[attachments/Untitled 19 40.png|Untitled 19 40.png]]

## Correctness

![[attachments/Untitled 20 39.png|Untitled 20 39.png]]

![[attachments/Untitled 21 36.png|Untitled 21 36.png]]

- Let $p_s$﻿ be the priority of $s$﻿ when $s$﻿ is popped off the frontier. Since all costs are non-negative, $p_s$﻿ increases over the course of the algorithm.
    - Note that $p_s = \text{PastCost(s)}$﻿ is the cost of the blue path
- Suppose we pop $s$﻿ off the frontier. Let the blue path denote the path with cost $p_s$﻿.
- Consider any alternative red path from the start state to $s$﻿. The red path must leave the explored region at some point.
    - Let $t$﻿ and $u = \text{Succ}(t, a)$﻿ be the first pair of states straddling the boundary.
- We want to show that the red path cannot be cheaper than the blue path via a string of inequalities.
    - First, by definition of $\text{PastCost}(t)$﻿ and non-negativity of edge costs, the cost of the red path is at least the cost of the part leading to $u$﻿, which is $\text{PastCost}(t) + \text{Cost}(t, u) = p_t + \text{Cost}(t, u)$﻿, where the last equality is by the inductive hypothesis.
    - Second, we have $p_t + \text{Cost}(t, u) \geq p_u$﻿ since we updated the frontier based on $(t, u)$﻿.
        - $u$﻿ is already a state on the frontier. Therefore, the final value of $p_u$﻿ is either just equal to $p_t + \text{Cost}(t, u)$﻿ or it’s equal to something smaller.
    - Third, we have that $p_u \geq p_s$﻿ because both $s$﻿ and $u$﻿ were on the frontier, and we chose to pop off $s$﻿ first, so it has the minimum cost.

## DP vs UCS

![[attachments/Untitled 22 32.png|Untitled 22 32.png]]

- The $\log(n)$﻿ comes from the use of the priority queue.
    - $n$﻿ is the number of states you’ve explored, and $N$﻿ is all states.

# A*

- A* is like uniform cost search, but it uses a heuristic to explore in a more directed way towards the end state.
    
    ![[attachments/Untitled 23 29.png|Untitled 23 29.png]]
    
- UCS orders states purely based on the PastCost. A* improves this by kind of also considering FutureCost with a heuristic estimate.
    
    ![[attachments/Untitled 24 28.png|Untitled 24 28.png]]
    
    - We don’t really know FutureCost(s). If we did, our problem would basically already be solved.
- **Distortion:** A* distorts edge costs to favor end states.

## Algorithm

- A* is just uniform cost search, with modified edge costs

![[attachments/Untitled 25 25.png|Untitled 25 25.png]]

- If we choose a successor that gets us closer to $s_{end}$﻿, $h(\text{Succ(s, a)}) - h(s)$﻿ will become negative, which reduces our cost and encourages us to take that path more.

![[attachments/Untitled 26 22.png|Untitled 26 22.png]]

- In this example, $h(s_1') < h(s) < h(s_2')$﻿. Therefore, we are encouraging moving to $s_1'$﻿, and we are penalizing moving to $s_2'$﻿
- Make sure you choose heuristics that don’t lead to negative edge weights.

## Consistent heuristics

![[attachments/Untitled 27 20.png|Untitled 27 20.png]]

![[attachments/Untitled 28 18.png|Untitled 28 18.png]]

## Correctness of A*

- If $h$﻿ is consistent, then we know that A* is correct.

![[attachments/Untitled 29 17.png|Untitled 29 17.png]]

![[attachments/Untitled 30 17.png|Untitled 30 17.png]]

- The new cost of this path is the same thing as the old cold, minus some constant (the heuristic at $s_0$﻿.
    - As such, optimizing the new cost is the same as optimizing the old cost.

## Efficiency of A*

- A* is more efficient than UCS
    
    - UCS explores all states $s$﻿ satisfying $\text{PastCost}(s) \leq \text{PastCost}(s_{\text{end}})$﻿
    - A* explores all states $s$﻿ satisfying $\text{PastCost}(s) \leq \text{PastCost}(s_{\text{end}}) - h(s)$﻿
    
    ![[attachments/Untitled 31 15.png|Untitled 31 15.png]]
    
    - The larget the heuristic, the smaller the area we are exploring.
    
    ![[attachments/Untitled 32 15.png|Untitled 32 15.png]]
    

## Admissibility

![[attachments/Untitled 33 15.png|Untitled 33 15.png]]

---

# A* relaxations

![[attachments/Untitled 34 12.png|Untitled 34 12.png]]

- By removing constraints on the problem, we can either get a **closed-form solution**, get an **easier search** problem, or we can get **independent subproblems** that are easier to solve.

## Closed form solution

![[attachments/Untitled 35 12.png|Untitled 35 12.png]]

- Suppose our original problem was the one on the left. Finding the heuristic for that is hard, so we should remove the constraints.
    - With arbitrary walls, we can’t compute FutureCost(s) except by doing search.
    - However, if relaxed the original problem by removing the walls, then we can compute FutureCost(s) in closed form with the Manhattan distance

## Easier search

![[attachments/Untitled 36 11.png|Untitled 36 11.png]]

- In the original state, the number of states is $O(n^2)$﻿. In the relaxed state, the number of states is just $O(n)$﻿.
- If we drop the constraint, we can use that relaxed state for our heuristic, which can be computed much faster.

![[attachments/Untitled 37 11.png|Untitled 37 11.png]]

- To compute this, we reverse the search problem to basically get our future costs. We search from the end state, and end at the start state.
    - This gives us a good estimate of the future cost from each location.

## Independent subproblems

![[attachments/Untitled 38 11.png|Untitled 38 11.png]]

- In the 8-puzzle, the goal is to slide the tiles around to produce the desired configuration, but with the constraint that **no two tiles can occupy the same position**.
- We can remove that constraint to get a relaxed problem. Now, the new problem is really easy, because the tiles can now move independently.
- So we’ve taken one giant problem and turned it into 8 smaller problems. Each of the smaller problems can now be solved separately (in this case, in closed form, but in other cases, we can also just do search).
    - Each of the smaller problems is the path that each tile takes to get to their final solution. The closed form solution is just the Manhattan distance.

## General framework

- Relaxing basically reduces the edge costs for unreachable edges from infinity to some finite cost.
    
    ![[attachments/Untitled 39 10.png|Untitled 39 10.png]]
    

![[attachments/Untitled 40 10.png|Untitled 40 10.png]]

- The future costs of the relaxed problem should be efficiently computable.

## Consistency of relaxed heuristics

![[attachments/Untitled 41 8.png|Untitled 41 8.png]]

## Tradeoff

![[attachments/Untitled 42 8.png|Untitled 42 8.png]]

# Terminology

- PastCost(s): The minimum cost from the start state to state 's'.
    - This is the cost that has been incurred to arrive at the current state from the start state.
    - For example, if you are finding the shortest path in a graph, the past cost at a certain node would be the shortest distance from the start node to the current node.
- FutureCost(s): The minimum cost from state 's' to an end state.
    - This is an estimate of the remaining cost to reach a goal state from the current state.
    - However, this cost can be difficult to compute exactly since it would require solving the original problem. Hence, we often use a heuristic function 'h(s)', which is an estimate of the FutureCost(s).
- These terms are often used in the context of the A* algorithm, which guides the search for the shortest path by examining states in the order of the sum of their past cost and an estimate of their future cost, i.e., PastCost(s) + h(s).