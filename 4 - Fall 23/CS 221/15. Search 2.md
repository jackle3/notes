---
Date: 2023-10-16
---
# Dynamic programming

- DP is backtracking search with **memoization**. It only works for acyclic graphs.

![Untitled 106.png](../../attachments/Untitled%20106.png)

- Starting from a starting state `s`, we can get to various states `s'`. Each of these have a `FutureCost` to go from that state to the end state, so we can just add it.
    - In this case, `s'` is equal to `Succ(s, a)`
    - `FutureCost(s)` is the cost of the minimum cost path from s to some end state.

## Motivating task

![Untitled 1 72.png](../../attachments/Untitled%201%2072.png)

- Notice that all the subtrees under city `5` are identical. Traversing this is a lot of repeated work.
- We can cache the solution for the `FutureCost` from city `5`.
    - The `FutureCost` is a function of the state that we are in.
        - If we use dynamic programming, the state is just the city.
        - If we didn’t, the state would be the entire history — all the past sequences of actions that we took

![Untitled 2 72.png](../../attachments/Untitled%202%2072.png)

- Using DP allows us to condense the original tree into this simplified graph.
    - We no longer have a tree, but a directed acyclic graph with only $n$﻿ nodes.

## State

![Untitled 3 72.png](../../attachments/Untitled%203%2072.png)

## Basic DP algorithm

![Untitled 4 69.png](../../attachments/Untitled%204%2069.png)

![Untitled 5 69.png](../../attachments/Untitled%205%2069.png)

# Practice with making states

## Can’t go three odd in a row

![Untitled 6 68.png](../../attachments/Untitled%206%2068.png)

- Starting from city `3`, we know that the previous city was odd, so we can’t go to another odd city without violating our constraint.
- The choice of state matters a lot, because it can significantly change the size of our state space.
    
    ![Untitled 7 66.png](../../attachments/Untitled%207%2066.png)
    

## Must visit at least three odd

![Untitled 8 62.png](../../attachments/Untitled%208%2062.png)

![Untitled 9 60.png](../../attachments/Untitled%209%2060.png)

---

# Uniform cost search

![Untitled 10 58.png](../../attachments/Untitled%2010%2058.png)

![Untitled 11 57.png](../../attachments/Untitled%2011%2057.png)

- Uniform cost search allows us to handle cycles, assuming costs are non-negative.
- Called uniform cost search because for every “iteration”, it uniformly expands its frontier until we reach the target.

## High-level strategy

- There are three sets of nodes that we keep track of in uniform cost search
    
    - We move states from unexplored to frontier, then from frontier to explored.
    
    ![Untitled 12 56.png](../../attachments/Untitled%2012%2056.png)
    
- The key idea is that we have computed the minimum cost paths to all the nodes in the explored set.
    - When an end state moves into the explored set, then we are done (we know the answer)
- The frontier is implemented with a priority queue
- This algorithm is basically Dijkstra’s
    - The difference is that this search problem goes from node A to B, and end once we reach the best path to state B
    - With Djikstra, we find paths from node A to every other node

## Example

- **Step 1 - original state**
    
    ![Untitled 13 54.png](../../attachments/Untitled%2013%2054.png)
    
- **Step 2 - we start with A, our initial state**
    
    ![Untitled 14 52.png](../../attachments/Untitled%2014%2052.png)
    
    - We first add A to the frontier. Then, since A is the only element, we pop off A and add it to the explored state.
- **Step 3 - we explore all the children of A and add it to the frontier**
    
    ![Untitled 15 50.png](../../attachments/Untitled%2015%2050.png)
    
- **Step 4**
    
    - After exploring B, we see that a new path to C is found. We can update our frontier.
    
    ![Untitled 16 48.png](../../attachments/Untitled%2016%2048.png)
    
- **Step 5 - repeat until finish**
    
    ![Untitled 17 45.png](../../attachments/Untitled%2017%2045.png)
    
    - We stop once our target node gets into the explored set.

## Pseudocode

![Untitled 18 43.png](../../attachments/Untitled%2018%2043.png)

![Untitled 19 40.png](../../attachments/Untitled%2019%2040.png)

## Correctness

![Untitled 20 39.png](../../attachments/Untitled%2020%2039.png)

![Untitled 21 36.png](../../attachments/Untitled%2021%2036.png)

- Let $p_s$﻿ be the priority of $s$﻿ when $s$﻿ is popped off the frontier. Since all costs are non-negative, $p_s$﻿ increases over the course of the algorithm.
    - Note that $p_s = \text{PastCost(s)}$﻿ is the cost of the blue path
- Suppose we pop $s$﻿ off the frontier. Let the blue path denote the path with cost $p_s$﻿.
- Consider any alternative red path from the start state to $s$﻿. The red path must leave the explored region at some point.
    - Let $t$﻿ and $u = \text{Succ}(t, a)$﻿ be the first pair of states straddling the boundary.
- We want to show that the red path cannot be cheaper than the blue path via a string of inequalities.
    - First, by definition of $\text{PastCost}(t)$﻿ and non-negativity of edge costs, the cost of the red path is at least the cost of the part leading to $u$﻿, which is $\text{PastCost}(t) + \text{Cost}(t, u) = p_t + \text{Cost}(t, u)$﻿, where the last equality is by the inductive hypothesis.
    - Second, we have $p_t + \text{Cost}(t, u) \geq p_u$﻿ since we updated the frontier based on $(t, u)$﻿.
        - $u$﻿ is already a state on the frontier. Therefore, the final value of $p_u$﻿ is either just equal to $p_t + \text{Cost}(t, u)$﻿ or it’s equal to something smaller.
    - Third, we have that $p_u \geq p_s$﻿ because both $s$﻿ and $u$﻿ were on the frontier, and we chose to pop off $s$﻿ first, so it has the minimum cost.

## DP vs UCS

![Untitled 22 32.png](../../attachments/Untitled%2022%2032.png)

- The $\log(n)$﻿ comes from the use of the priority queue.
    - $n$﻿ is the number of states you’ve explored, and $N$﻿ is all states.

# A*

- A* is like uniform cost search, but it uses a heuristic to explore in a more directed way towards the end state.
    
    ![Untitled 23 29.png](../../attachments/Untitled%2023%2029.png)
    
- UCS orders states purely based on the PastCost. A* improves this by kind of also considering FutureCost with a heuristic estimate.
    
    ![Untitled 24 28.png](../../attachments/Untitled%2024%2028.png)
    
    - We don’t really know FutureCost(s). If we did, our problem would basically already be solved.
- **Distortion:** A* distorts edge costs to favor end states.

## Algorithm

- A* is just uniform cost search, with modified edge costs

![Untitled 25 25.png](../../attachments/Untitled%2025%2025.png)

- If we choose a successor that gets us closer to $s_{end}$﻿, $h(\text{Succ(s, a)}) - h(s)$﻿ will become negative, which reduces our cost and encourages us to take that path more.

![Untitled 26 22.png](../../attachments/Untitled%2026%2022.png)

- In this example, $h(s_1') < h(s) < h(s_2')$﻿. Therefore, we are encouraging moving to $s_1'$﻿, and we are penalizing moving to $s_2'$﻿
- Make sure you choose heuristics that don’t lead to negative edge weights.

## Consistent heuristics

![Untitled 27 20.png](../../attachments/Untitled%2027%2020.png)

![Untitled 28 18.png](../../attachments/Untitled%2028%2018.png)

## Correctness of A*

- If $h$﻿ is consistent, then we know that A* is correct.

![Untitled 29 17.png](../../attachments/Untitled%2029%2017.png)

![Untitled 30 17.png](../../attachments/Untitled%2030%2017.png)

- The new cost of this path is the same thing as the old cold, minus some constant (the heuristic at $s_0$﻿.
    - As such, optimizing the new cost is the same as optimizing the old cost.

## Efficiency of A*

- A* is more efficient than UCS
    
    - UCS explores all states $s$﻿ satisfying $\text{PastCost}(s) \leq \text{PastCost}(s_{\text{end}})$﻿
    - A* explores all states $s$﻿ satisfying $\text{PastCost}(s) \leq \text{PastCost}(s_{\text{end}}) - h(s)$﻿
    
    ![Untitled 31 15.png](../../attachments/Untitled%2031%2015.png)
    
    - The larget the heuristic, the smaller the area we are exploring.
    
    ![Untitled 32 15.png](../../attachments/Untitled%2032%2015.png)
    

## Admissibility

![Untitled 33 15.png](../../attachments/Untitled%2033%2015.png)

---

# A* relaxations

![Untitled 34 12.png](../../attachments/Untitled%2034%2012.png)

- By removing constraints on the problem, we can either get a **closed-form solution**, get an **easier search** problem, or we can get **independent subproblems** that are easier to solve.

## Closed form solution

![Untitled 35 12.png](../../attachments/Untitled%2035%2012.png)

- Suppose our original problem was the one on the left. Finding the heuristic for that is hard, so we should remove the constraints.
    - With arbitrary walls, we can’t compute FutureCost(s) except by doing search.
    - However, if relaxed the original problem by removing the walls, then we can compute FutureCost(s) in closed form with the Manhattan distance

## Easier search

![Untitled 36 11.png](../../attachments/Untitled%2036%2011.png)

- In the original state, the number of states is $O(n^2)$﻿. In the relaxed state, the number of states is just $O(n)$﻿.
- If we drop the constraint, we can use that relaxed state for our heuristic, which can be computed much faster.

![Untitled 37 11.png](../../attachments/Untitled%2037%2011.png)

- To compute this, we reverse the search problem to basically get our future costs. We search from the end state, and end at the start state.
    - This gives us a good estimate of the future cost from each location.

## Independent subproblems

![Untitled 38 11.png](../../attachments/Untitled%2038%2011.png)

- In the 8-puzzle, the goal is to slide the tiles around to produce the desired configuration, but with the constraint that **no two tiles can occupy the same position**.
- We can remove that constraint to get a relaxed problem. Now, the new problem is really easy, because the tiles can now move independently.
- So we’ve taken one giant problem and turned it into 8 smaller problems. Each of the smaller problems can now be solved separately (in this case, in closed form, but in other cases, we can also just do search).
    - Each of the smaller problems is the path that each tile takes to get to their final solution. The closed form solution is just the Manhattan distance.

## General framework

- Relaxing basically reduces the edge costs for unreachable edges from infinity to some finite cost.
    
    ![Untitled 39 10.png](../../attachments/Untitled%2039%2010.png)
    

![Untitled 40 10.png](../../attachments/Untitled%2040%2010.png)

- The future costs of the relaxed problem should be efficiently computable.

## Consistency of relaxed heuristics

![Untitled 41 8.png](../../attachments/Untitled%2041%208.png)

## Tradeoff

![Untitled 42 8.png](../../attachments/Untitled%2042%208.png)

# Terminology

- PastCost(s): The minimum cost from the start state to state 's'.
    - This is the cost that has been incurred to arrive at the current state from the start state.
    - For example, if you are finding the shortest path in a graph, the past cost at a certain node would be the shortest distance from the start node to the current node.
- FutureCost(s): The minimum cost from state 's' to an end state.
    - This is an estimate of the remaining cost to reach a goal state from the current state.
    - However, this cost can be difficult to compute exactly since it would require solving the original problem. Hence, we often use a heuristic function 'h(s)', which is an estimate of the FutureCost(s).
- These terms are often used in the context of the A* algorithm, which guides the search for the shortest path by examining states in the order of the sum of their past cost and an estimate of their future cost, i.e., PastCost(s) + h(s).