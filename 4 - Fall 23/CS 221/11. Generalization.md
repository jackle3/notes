---
Date: 2023-10-09
---
# Summary of recipe so far

![Untitled 94.png](attachments/Untitled%2094.png)

# Generalization

- We generalize to avoid overfitting — when the model fits too much into the training data, and it fails to perform well on the validation data.
- The real objective is **not training loss** — it’s loss on unseen future examples.
    - The semi-real objective is the test loss.
    - Try to minimize the training error, but also keep the hypothesis class small.

## Evaluation

- To measure how good a predictor is, we want to minimize **error on unseen future examples**

![Untitled 1 61.png](attachments/Untitled%201%2061.png)

- The test set is a surrogate of the unseen future examples.

## Approximation and estimation error

![Untitled 2 61.png](attachments/Untitled%202%2061.png)

- Consider the space above.
    - The **outer light red circle** is the space of all predictors.
    - The **inner blue circle** is the hypothesis class.
        - This usually doesn’t contain the ground truth predictor
- $f^*$﻿ is the ground truth predictor — it returns the actual ground truth.
- $\hat{f}$﻿ is the predictor within the hypothesis class that we get after learning.
- $g$﻿ is the predictor we can find in the hypothesis class.
    - g ∈ F is the best possible predictor in the hypothesis class in the sense of minimizing test error g = arg min f∈F Err(f).
- **Approximation error** is how far the hypothesis class is from the target predictor $f^*$﻿.
    - Larger hypothesis classes have lower approximation error.
    - Here, distance is just the differences in test error: **Err(g) − Err(****$f^*$**﻿**).**
- **Estimation error** is how good the predictor $\hat{f}$﻿ returned by the learning algorithm is with respect to the best in the hypothesis class:
    - **Err(ˆf) − Err(g)**.
    - Larger hypothesis classes have higher estimation error because it’s harder to find a good predictor based on limited data.

## Effect of hypothesis class size on error

- As the hypothesis class size increases
    - Approximation error decreases because we are taking min over larger set
    - Estimation error increases because it is harder to estimate something more complex

# Controlling Hypothesis Class Size

## Strategy 1: Dimensionality

- We can **reduce the dimensionality** by removing features. This can also mean one of the features must have zero weight.

![Untitled 3 61.png](attachments/Untitled%203%2061.png)

### Controlling dimensinoality

![Untitled 4 60.png](attachments/Untitled%204%2060.png)

- No matter how complex the features are, it’s the number of features that matter the most in terms of controlling dimensionality.

# Strategy 2: Norm

- We can also **reduce the norm** (length) of the weight vector → $\|w\|$﻿

![Untitled 5 60.png](attachments/Untitled%205%2060.png)

## Controlling the norm

- There are two ways to do this:

### Regularization

- We can create a regularized objective by adding a penalty term $\frac{\lambda}{2} \|w\|^2$﻿
    - This penalty basically creates of dual objective of minimizing the training loss while also keeping the norm of the vector small.

![Untitled 6 59.png](attachments/Untitled%206%2059.png)

- We now change the optimization step in GD to include the gradient of the penalty term.

### Early stopping

- The only difference from normal GD is to just remove the number of epochs we go for.

![Untitled 7 59.png](attachments/Untitled%207%2059.png)