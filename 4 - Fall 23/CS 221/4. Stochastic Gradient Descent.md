---
Date: 2023-10-02
---
# Issues with Gradient Descent

![Untitled 117.png](../../attachments/Untitled%20117.png)

- Gradient descent is really slow.
    - Recall that training loss is the sum over the training data.
    - For large data, this means each gradient computation requires going through every single data point.
        - Very expensive to do this multiple times.

# Stochastic Gradient Descent

![Untitled 1 83.png](../../attachments/Untitled%201%2083.png)

- On each epoch, we loop over the training examples and update $\mathbf w$﻿ on the individual losses for each example.
- **Advantage:** much faster in terms of getting more updates to the weight.
- **Disadvantage:** the updates won’t be as high quality since it’s per data point.

# Step size

![Untitled 2 83.png](../../attachments/Untitled%202%2083.png)

- Step size is usually tuned by trial and error.
- For setting $\eta$﻿, we can either keep it constant or have it be variable and decreasing.
    - In the beginning, we are far away so we want to move quickly. As we get closer, we want to start slowing down.

# SGD in Python

## Generating Random Examples

![Untitled 3 83.png](../../attachments/Untitled%203%2083.png)

- We create a `trueW` for the true weight vector of the answer. Then, we randomly generate our data points with noise added.

## Regular GD

- Gradient descent is extremely slow with this example because there are 1,000,000 examples.

![Untitled 4 79.png](../../attachments/Untitled%204%2079.png)

- Notice also that after the first epoch, the answer is still far from our truth.

## Setup

![Untitled 5 79.png](../../attachments/Untitled%205%2079.png)

## Optimization Function

![Untitled 6 78.png](../../attachments/Untitled%206%2078.png)

## Result

- Now, SGD goes through each epoch much faster.

![Untitled 7 76.png](../../attachments/Untitled%207%2076.png)

- Furthermore, after just one epoch, it gets much closer to the true vector.

# Key Idea of SGD

![Untitled 8 71.png](../../attachments/Untitled%208%2071.png)

- The **quality** solution would be to optimize the TrainLoss. The **quantity** solution is to optimize based on the Loss.