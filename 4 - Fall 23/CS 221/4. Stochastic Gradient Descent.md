---
Date: 2023-10-02
---
# Issues with Gradient Descent

![[attachments/Untitled 117.png|Untitled 117.png]]

- Gradient descent is really slow.
    - Recall that training loss is the sum over the training data.
    - For large data, this means each gradient computation requires going through every single data point.
        - Very expensive to do this multiple times.

# Stochastic Gradient Descent

![[attachments/Untitled 1 83.png|Untitled 1 83.png]]

- On each epoch, we loop over the training examples and update $\mathbf w$﻿ on the individual losses for each example.
- **Advantage:** much faster in terms of getting more updates to the weight.
- **Disadvantage:** the updates won’t be as high quality since it’s per data point.

# Step size

![[attachments/Untitled 2 83.png|Untitled 2 83.png]]

- Step size is usually tuned by trial and error.
- For setting $\eta$﻿, we can either keep it constant or have it be variable and decreasing.
    - In the beginning, we are far away so we want to move quickly. As we get closer, we want to start slowing down.

# SGD in Python

## Generating Random Examples

![[attachments/Untitled 3 83.png|Untitled 3 83.png]]

- We create a `trueW` for the true weight vector of the answer. Then, we randomly generate our data points with noise added.

## Regular GD

- Gradient descent is extremely slow with this example because there are 1,000,000 examples.

![[attachments/Untitled 4 79.png|Untitled 4 79.png]]

- Notice also that after the first epoch, the answer is still far from our truth.

## Setup

![[attachments/Untitled 5 79.png|Untitled 5 79.png]]

## Optimization Function

![[attachments/Untitled 6 78.png|Untitled 6 78.png]]

## Result

- Now, SGD goes through each epoch much faster.

![[attachments/Untitled 7 76.png|Untitled 7 76.png]]

- Furthermore, after just one epoch, it gets much closer to the true vector.

# Key Idea of SGD

![[attachments/Untitled 8 71.png|Untitled 8 71.png]]

- The **quality** solution would be to optimize the TrainLoss. The **quantity** solution is to optimize based on the Loss.