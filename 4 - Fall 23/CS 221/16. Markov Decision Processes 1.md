---
Date: 2023-10-18
---
- This deals with state based problems that deal with some sort of uncertainty.
    - Problems where there is some chance about where we end up at.
- Starting from state $s$﻿, taking action $a$﻿ does not deterministically mean we end up at state $s'$﻿.
- The decisions are Markov, so we don’t care about the history. It only concerns the question: at a particular state $s$﻿, what is the best thing we can do.

## Example

- This is an example of an MDP. We have two islands, with a volcano in between. However, we have a probability of slipping into the volcano. Is it worth it to cross the island?
    
    ![Untitled 116.png](../../attachments/Untitled%20116.png)
    
    - The arrows indicate the most optimal path.
    - The numbers are sort of like the expected value at that state. As long as it’s greater than 2, then moving there to cross the island might be a better idea.

# Modeling MDPs

- Suppose we have the following game:
    
    ![Untitled 1 82.png](../../attachments/Untitled%201%2082.png)
    
    - If we choose to quit, we deterministically get $10.
    - If we stay, there is an element of chance.

## Rewards

- From the example above, here is the graph of the probabilities and expected total rewards.
    
    ![Untitled 2 82.png](../../attachments/Untitled%202%2082.png)
    
- The calculate of the expected utility is probability of losing first round, plus probability of winning first round then losing second round, etc.

## MDP for the dice game

- An MDP can be represented as a graph.
    
    - The nodes in this graph include both states and chance nodes.
    - Edges coming out of **states** are the possible actions from that state, which lead to chance nodes. Edges coming out of a **chance** nodes are the possible random outcomes of that action, which end up back in states.
    - Our convention is to label these chance-to-state edges with the probability of a particular transition and the associated reward for traversing that edge
    
    ![Untitled 3 82.png](../../attachments/Untitled%203%2082.png)
    
    - The blue nodes are the state nodes. We are either in the game or we are out of the game.
        - The blue edges are the actions that we have from each state.
    - The red nodes are the chance nodes. It’s a state + action node, denoting our previous state and the action we took. From there, there is probability of going to other nodes.
    - From the `in, stay` state, there is a $2/3$﻿ probability that we win $4 and we can go back to being `in` (winning the round). There is a $1/3$﻿ probability that we get $4 but we lose.

# Definition of MDP

![Untitled 4 78.png](../../attachments/Untitled%204%2078.png)

- From the previous example of the dice game, each of these variables are:
    
    ![Untitled 5 78.png](../../attachments/Untitled%205%2078.png)
    
- Compared to **Search problems**, the successor function is now the transition probabilities, and the cost function is now the rewards.
    - Instead of minimizing costs, we now maximize rewards.

## Transitions

![Untitled 6 77.png](../../attachments/Untitled%206%2077.png)

- If we sum over all $s'$﻿, the transition probabilities should add up to 1.
- The successors of a certain state is all the $s'$﻿ such that $T(s, a, s') > 0$﻿

# Transportation example

- Consider the transportation search problem from before. Now, there is uncertainty.
    
    ![Untitled 7 75.png](../../attachments/Untitled%207%2075.png)
    
    - If the tram fails, we have to wait 2 minutes, but we don’t move.

## Code

![Untitled 8 70.png](../../attachments/Untitled%208%2070.png)

![Untitled 9 67.png](../../attachments/Untitled%209%2067.png)

- Note that we want to maximize rewards. This is basically like minimizing the negative reward.
- Walking is determinisitic. We have a probability of 1 of getting to `state + 1`, and our reward is `-1` since it takes one minute.
- Tram is uncertainy. We have a 0.5 probability of staying at `state`, and a 0.5 prob of getting to `state * 2`. Each of these takes two minutes.

# Policy

- A solution to an MDP is called a policy. It’s a function from states to actions. It takes in a state, and tells us what action we should take.

![Untitled 10 65.png](../../attachments/Untitled%2010%2065.png)

- Our goal with MDP problems is to find the optimal policy.

# Utility and value

![Untitled 11 63.png](../../attachments/Untitled%2011%2063.png)

- Since it’s hard to maximize the utility since it’s a rnadom variable, we will instead maximize the expected utility, which we will refer to as the **value** of a policy.
- For example, with the volcano example from before, the utility changes everytime we run the program, since there is uncertainty in the actions.
    
    - However, the value does not change, since it’s the expected utility given the uncertainty.
    
    ![Untitled 12 62.png](../../attachments/Untitled%2012%2062.png)
    

# Discounting

- Discount basically tells us how much we should care about the future.
    
    ![Untitled 13 60.png](../../attachments/Untitled%2013%2060.png)
    
- With $\gamma = 1$﻿, it says getting 4 dollars now is the same as getting 4 dollars in the future.
- With $\gamma = 0$﻿, it says that getting 4 dollars now is worth, while getting 4 dollars in the future is worthless.

# Policy evaluation

![Untitled 14 56.png](../../attachments/Untitled%2014%2056.png)

- Value is the expected utility from a state node.
- Q value is the expected utility from a chance node.

![Untitled 15 54.png](../../attachments/Untitled%2015%2054.png)

- The expression $V_\pi(s)$﻿ tells us how good state $s$﻿ is. What is it equal to?
    - If $s$﻿ is an end node, then the value is zero, because I’m done.
    - If I’m not, it’s the value of taking action $a$﻿ from state $s$﻿, or $Q_\pi(s, a)$﻿.
- The expected utility from the chance node is found by consider all successor states $s'$﻿ and taking the expectation (multiplying by the **transition probability**) over the **immediate reward** plus the **discounted future reward**.
    - Notice that this is a recurrence.

## Dice example

![Untitled 16 52.png](../../attachments/Untitled%2016%2052.png)

- By observation, $V_\pi(\text{end}) = 0$﻿ because we the expected utility of being at the end is 0.
- Now, we see that $V_\pi(\text{in}) = Q_\pi(\text{in}, \text{stay})$﻿

![Untitled 17 49.png](../../attachments/Untitled%2017%2049.png)

## Algorithm

![Untitled 18 47.png](../../attachments/Untitled%2018%2047.png)

- We iterative update $V_\pi(s)$﻿ for all states using the recurrence relation that we defined previously. We use the previous value of $V_\pi(s')$﻿ for our calculations.

![Untitled 19 43.png](../../attachments/Untitled%2019%2043.png)

## Complexity

![Untitled 20 42.png](../../attachments/Untitled%2020%2042.png)

- The time complexity of this algorith is $O(t_\text{PE} * S * S')$﻿
- This algorithm converges exponentially. The error decreases exponentially as we increase the number of iterations.
- For acyclic graphs (for example, the MDP for Blackjack), we just need to do one iteration (not tPE) provided that we process the nodes in reverse topological order of the graph.
    - This is the same setup as we had for dynamic programming in search problems, only the equations are different

# Terminology summary

- **MDP**: graph with states, chance nodes, transition probabilities, rewards
- **Policy**: mapping from state to action (solution to MDP)
- **Value of policy**: expected utility over random paths
- **Policy evaluation**: iterative algorithm to compute value of policy

# Value iteration

- Policy evaluation just told us how good a policy was. Now, we use value iteration to actually find what the optimal policy is.

![Untitled 21 39.png](../../attachments/Untitled%2021%2039.png)

## Finding the optimal policy

- Suppose we are starting at some state $s$﻿
    
    ![Untitled 22 35.png](../../attachments/Untitled%2022%2035.png)
    
    - To find $V_\text{opt}(s)$﻿, we have to find good actions to take, which takes us to a chance node. From there, we have to find $Q_\text{opt}$﻿ of the chance node, which depends on $V_\text{opt}(S')$﻿
    
    ![Untitled 23 32.png](../../attachments/Untitled%2023%2032.png)
    
- Before, someone told us what policy we should take, so find $V_\pi$﻿ just had a single $Q_\pi$﻿ to consider. Now, since we have many possible actions and we want the optimal one, we want the chance node that gives us the maximum value, which is the action we should take.
    
    ![Untitled 24 30.png](../../attachments/Untitled%2024%2030.png)
    
- Using what we have, the optimal policy is just the action that brought us to the best chance node.
    
    ![Untitled 25 27.png](../../attachments/Untitled%2025%2027.png)
    

## Bellman Algorithm

![Untitled 26 24.png](../../attachments/Untitled%2026%2024.png)

### Code solution

![Untitled 27 22.png](../../attachments/Untitled%2027%2022.png)

- Notice that in `def Q`, the reference to `V[newState]` is the old optimal value from the previous iteration.

![Untitled 28 20.png](../../attachments/Untitled%2028%2020.png)

## Convergence

![Untitled 29 19.png](../../attachments/Untitled%2029%2019.png)

- If we had a discount value of $1$﻿ with cycles, we would never be able to get any rewards.
- We can reinterpret the discount γ < 1 condition as introducing a new transition from each state to a special end state with probability (1−γ), multiplying all the other transition probabilities by γ, and setting the discount to 1.
- The interpretation is that with probability 1 − γ, the MDP terminates at any state.
- In this view, we just need that a sampled path be finite with probability 1.

# Summary

![Untitled 30 19.png](../../attachments/Untitled%2030%2019.png)