---
Date: 2023-10-25
---
# Baby delivery example

- This is a probability distribution of when a baby shows up, where zero is the due date, given that the baby hasn’t been born so far.

![[attachments/Untitled 168.png|Untitled 168.png]]

- The P(no child so far | D = d) is gonna be either one or zero, depending on the actual date that the baby would be born at.
- To find out what P(no child so far) is, observe that the area under the sum of the probabilities have to be equal to one. This probability would be whatever it needs to be such that the sum is one.

# Assumption

![[attachments/Untitled 1 131.png|Untitled 1 131.png]]

![[attachments/Untitled 2 130.png|Untitled 2 130.png]]

- Assume that our prior belief that a baby can hear is 1/2

![[attachments/Untitled 3 127.png|Untitled 3 127.png]]

- The term $f(X = 14 | H = 1)$﻿ is just the normal distribution defined above.
    - It’s equivalent to `scipy.norm.pdf(x = 14, mu = 15, std = sqrt(50))`
    - It’s also equivalent to the actual PDF equation of the normal.
- To expand the bottom, you can use the law of total probability and do the same thing.

# Stanford Eye Test

- The ability to see in the natural world is continous. However, to make the problem easier, we can discretize it.
    
    ![[attachments/Untitled 4 122.png|Untitled 4 122.png]]
    
    - Note that this is a discretized version of the PDF. As such, it also tells us that it is more likely that a person has standard vision than can’t see.
- Suppose you show a test to a user with a certain font size $s_i$﻿, and they get it wrong
    
    ![[attachments/Untitled 5 120.png|Untitled 5 120.png]]
    
    - This is a new observation for $Y$﻿, where $Y = 1$﻿ means they got it right, and $Y = 0$﻿ means they got it wrong.
- With this new observation, the whole distribution changes.
    
    ![[attachments/Untitled 6 118.png|Untitled 6 118.png]]
    
    ![[attachments/Untitled 7 115.png|Untitled 7 115.png]]
    
    - We already knew $P(A = a)$﻿ from the prior belief. $P(Y = 0)$﻿ is just the normalization constant, so it’s just any number that makes you the sum of all the probabilities equal 1.

## Inference on a non-bernoulli RV

![[attachments/Untitled 8 107.png|Untitled 8 107.png]]

- To calculate this updated probability after the observation, simply run Bayes and calculate all the numerators.
- Then, normalize it to get the denominator.
    
    ![[attachments/Untitled 9 103.png|Untitled 9 103.png]]
    

## Causal Model

![[attachments/Untitled 10 98.png|Untitled 10 98.png]]

## Finding Likelihood

- The likelihood is P(Y = 0 | A = a), and it’s a function that’s given to us.
    - Formally, it’s a model which gives the probability that the student will answer a question correctly given their ability.
- Suppose we get told that someone’s ability to see is 0.8. What is the probability that they get it right?
    
    ![[attachments/Untitled 11 94.png|Untitled 11 94.png]]
    
- There’s also the possibility of the student guessing or circling wrong.
    
    ![[attachments/Untitled 12 91.png|Untitled 12 91.png]]
    
    - There’s a 1/4 chance of them getting it right given that they don’t know the answer
        - This is through randomly guessing
    - There’s a (1 - s) chance of them getting it right given that they do know the answer
        - Because there’s a chance of them slipping (circling wrong)
- With this, we can find the probability of them getting wrong
    - $P(Y = 0 | A = a) = 1 - P(Y = 1 | A = a)$﻿
- Here it is implemented in code for this problem:
    
    ![[attachments/Untitled 13 85.png|Untitled 13 85.png]]
    
    ![[attachments/Untitled 14 74.png|Untitled 14 74.png]]
    

## Multiple observations

- Every time you have an observation, just update the distribution based on what you have after the previous observation.
    
    - Because the observations are independent, the posterior of the first observation is the prior of the second.
    
    ![[attachments/Untitled 15 71.png|Untitled 15 71.png]]
    
    ![[attachments/Untitled 16 67.png|Untitled 16 67.png]]
    
- Notice in the code below, that the result from the updated belief just becomes the belief for the next observation.
    
    ![[attachments/Untitled 17 62.png|Untitled 17 62.png]]
    
    ![[attachments/Untitled 18 56.png|Untitled 18 56.png]]