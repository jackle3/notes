---
Date: 2023-10-09
---
- When you condition probability of something, all the rules of probability still hold.

$P(A|BE) = \frac{P(B|AE)P(A|E)}{P(B|E)}$

- Since we consistently condition on event $E$﻿, the Bayes’ theorem still exists. Notice that if we just delete the $E$﻿ from the conditioning, the formulas are exactly Bayes’ theorem.

# Independence and Causality

- Independence rules out causality structures

![Untitled 171.png](attachments/Untitled%20171.png)

## Conditional Independence

- We can also look for independence relationships in the case when we condition on some other event.
    
    ![Untitled 1 134.png](attachments/Untitled%201%20134.png)
    
    - In this case, $T$﻿ is independent of $G5$﻿ when we condition upon $G2$﻿
    
    ![Untitled 2 133.png](attachments/Untitled%202%20133.png)
    
    - $G5$﻿ does not directly influence $T$﻿ → it influences $G2$﻿.

![Untitled 3 130.png](attachments/Untitled%203%20130.png)

- Independent things can become dependent when you condition it upon something. Similarly, dependent things can become independent.
    
    ![Untitled 4 125.png](attachments/Untitled%204%20125.png)
    
- **Conditional independence:** If E and F are dependent, that does not mean E and F will be dependent when another event is observed.
- **Conditional dependence:** If E and F are independent, that does not mean E and F will be independent when another event is observed.

## Conditioning on a set of events

- Suppose we want to find the probability that they wanted a certain movie, given that they watched these three other movies.
    
    ![Untitled 5 123.png](attachments/Untitled%205%20123.png)
    
- The probability becomes easier when we make this conditionally independent assumption.
    
    ![Untitled 6 121.png](attachments/Untitled%206%20121.png)
    

# Random variables

![Untitled 7 118.png](attachments/Untitled%207%20118.png)

- There are many possibilities of what $Y$﻿ can be, with a probability of each occurring.
- Note that **random variables** are not the same as **events**

## Example of RV

![Untitled 8 110.png](attachments/Untitled%208%20110.png)

## Properties of random variables

- Probability mass function: $P(X = a)$﻿
- Expectation: $E[X]$﻿
- Variance: $\text{Var}(X)$﻿

# Probability mass unfction

- The relationship between values a random variable can take on, and he corresponding probability.
- Let $Y$﻿ be a random variable.
    - E.g. $Y$﻿ is the number of heads in 5 coin flips.
- It is an event when $Y$﻿ takes on a value
    - For example $Y$﻿ = 2, the event that $Y$﻿ is two heads.
    - Any boolean operation you do with a random variable is an event.
- This is the probability of an event happening
    
    ![Untitled 9 106.png](attachments/Untitled%209%20106.png)
    
- This is the **probability mass function**
    
    ![Untitled 10 101.png](attachments/Untitled%2010%20101.png)
    
    ![Untitled 11 97.png](attachments/Untitled%2011%2097.png)
    
- **Key idea**: If a random variable is _discrete_, we call this function the **probability mass function**

## Notation

![Untitled 12 94.png](attachments/Untitled%2012%2094.png)

## Examples of PMFs

![Untitled 13 88.png](attachments/Untitled%2013%2088.png)

![Untitled 14 77.png](attachments/Untitled%2014%2077.png)

![Untitled 15 74.png](attachments/Untitled%2015%2074.png)

- You can define the PMF as a graph, a function, code, etc.

## Sum over a PMF

- The sum over the probabilities for all the possibilities of a random variable is $1$﻿.

![Untitled 16 70.png](attachments/Untitled%2016%2070.png)

- This is basically the probability of getting any outcome in the sample space.

# Expectation

- If $X$﻿ is your random variable, what is your most central outcome?

![Untitled 17 65.png](attachments/Untitled%2017%2065.png)

- It’s basically just a weighted average of all the outcomes.

## Examples

![Untitled 18 59.png](attachments/Untitled%2018%2059.png)

![Untitled 19 52.png](attachments/Untitled%2019%2052.png)

![Untitled 20 51.png](attachments/Untitled%2020%2051.png)

![Untitled 21 46.png](attachments/Untitled%2021%2046.png)

## Properties of expectations

![Untitled 22 42.png](attachments/Untitled%2022%2042.png)

- For linearity, $a$﻿ and $b$﻿ are scalars.
- For the expectation of a sum, $X$﻿ and $Y$﻿ are both random variables.

![Untitled 23 39.png](attachments/Untitled%2023%2039.png)

## Example with expectations

![Untitled 24 35.png](attachments/Untitled%2024%2035.png)

- For each term in the expectation above, it corresponds to
    - `T` → in this case $n=0$﻿
    - `HT` → $n = 1$﻿
    - `HHT`
    - `HHHT`
- Your theoretical winnings on this game is infinite.
- Now suppose you add a constraint to it. If you win over $65,536$﻿, you get no money at all.

![Untitled 25 32.png](attachments/Untitled%2025%2032.png)