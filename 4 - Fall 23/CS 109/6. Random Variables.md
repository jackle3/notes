---
Date: 2023-10-09
---
- When you condition probability of something, all the rules of probability still hold.

$P(A|BE) = \frac{P(B|AE)P(A|E)}{P(B|E)}$

- Since we consistently condition on event $E$﻿, the Bayes’ theorem still exists. Notice that if we just delete the $E$﻿ from the conditioning, the formulas are exactly Bayes’ theorem.

# Independence and Causality

- Independence rules out causality structures

![[attachments/Untitled 171.png|Untitled 171.png]]

## Conditional Independence

- We can also look for independence relationships in the case when we condition on some other event.
    
    ![[attachments/Untitled 1 134.png|Untitled 1 134.png]]
    
    - In this case, $T$﻿ is independent of $G5$﻿ when we condition upon $G2$﻿
    
    ![[attachments/Untitled 2 133.png|Untitled 2 133.png]]
    
    - $G5$﻿ does not directly influence $T$﻿ → it influences $G2$﻿.

![[attachments/Untitled 3 130.png|Untitled 3 130.png]]

- Independent things can become dependent when you condition it upon something. Similarly, dependent things can become independent.
    
    ![[attachments/Untitled 4 125.png|Untitled 4 125.png]]
    
- **Conditional independence:** If E and F are dependent, that does not mean E and F will be dependent when another event is observed.
- **Conditional dependence:** If E and F are independent, that does not mean E and F will be independent when another event is observed.

## Conditioning on a set of events

- Suppose we want to find the probability that they wanted a certain movie, given that they watched these three other movies.
    
    ![[attachments/Untitled 5 123.png|Untitled 5 123.png]]
    
- The probability becomes easier when we make this conditionally independent assumption.
    
    ![[attachments/Untitled 6 121.png|Untitled 6 121.png]]
    

# Random variables

![[attachments/Untitled 7 118.png|Untitled 7 118.png]]

- There are many possibilities of what $Y$﻿ can be, with a probability of each occurring.
- Note that **random variables** are not the same as **events**

## Example of RV

![[attachments/Untitled 8 110.png|Untitled 8 110.png]]

## Properties of random variables

- Probability mass function: $P(X = a)$﻿
- Expectation: $E[X]$﻿
- Variance: $\text{Var}(X)$﻿

# Probability mass unfction

- The relationship between values a random variable can take on, and he corresponding probability.
- Let $Y$﻿ be a random variable.
    - E.g. $Y$﻿ is the number of heads in 5 coin flips.
- It is an event when $Y$﻿ takes on a value
    - For example $Y$﻿ = 2, the event that $Y$﻿ is two heads.
    - Any boolean operation you do with a random variable is an event.
- This is the probability of an event happening
    
    ![[attachments/Untitled 9 106.png|Untitled 9 106.png]]
    
- This is the **probability mass function**
    
    ![[attachments/Untitled 10 101.png|Untitled 10 101.png]]
    
    ![[attachments/Untitled 11 97.png|Untitled 11 97.png]]
    
- **Key idea**: If a random variable is _discrete_, we call this function the **probability mass function**

## Notation

![[attachments/Untitled 12 94.png|Untitled 12 94.png]]

## Examples of PMFs

![[attachments/Untitled 13 88.png|Untitled 13 88.png]]

![[attachments/Untitled 14 77.png|Untitled 14 77.png]]

![[attachments/Untitled 15 74.png|Untitled 15 74.png]]

- You can define the PMF as a graph, a function, code, etc.

## Sum over a PMF

- The sum over the probabilities for all the possibilities of a random variable is $1$﻿.

![[attachments/Untitled 16 70.png|Untitled 16 70.png]]

- This is basically the probability of getting any outcome in the sample space.

# Expectation

- If $X$﻿ is your random variable, what is your most central outcome?

![[attachments/Untitled 17 65.png|Untitled 17 65.png]]

- It’s basically just a weighted average of all the outcomes.

## Examples

![[attachments/Untitled 18 59.png|Untitled 18 59.png]]

![[attachments/Untitled 19 52.png|Untitled 19 52.png]]

![[attachments/Untitled 20 51.png|Untitled 20 51.png]]

![[attachments/Untitled 21 46.png|Untitled 21 46.png]]

## Properties of expectations

![[attachments/Untitled 22 42.png|Untitled 22 42.png]]

- For linearity, $a$﻿ and $b$﻿ are scalars.
- For the expectation of a sum, $X$﻿ and $Y$﻿ are both random variables.

![[attachments/Untitled 23 39.png|Untitled 23 39.png]]

## Example with expectations

![[attachments/Untitled 24 35.png|Untitled 24 35.png]]

- For each term in the expectation above, it corresponds to
    - `T` → in this case $n=0$﻿
    - `HT` → $n = 1$﻿
    - `HHT`
    - `HHHT`
- Your theoretical winnings on this game is infinite.
- Now suppose you add a constraint to it. If you win over $65,536$﻿, you get no money at all.

![[attachments/Untitled 25 32.png|Untitled 25 32.png]]