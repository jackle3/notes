---
Date: 2023-10-27
---
# Joint Distribution

- Given two random variables, produce a function or table etc where for any assignment to the variables, we can give the joint of it.
    
    ![[attachments/Untitled 165.png|Untitled 165.png]]
    

# Bayesian Network

- Express models of multiple random variables by drawing them out as a network

![[attachments/Untitled 1 128.png|Untitled 1 128.png]]

- If we had multiple random variables, describing the joint would be very large.
    - For a case with four random bernoulli variables, the joint has $2^4 = 16$﻿ joint probabilities.
- We can represent the joint by describing causality.
    - Each random variable is caused by its parents → we care about P(node | parents)

![[attachments/Untitled 2 127.png|Untitled 2 127.png]]

## Conditional Independence

- Each random variable is conditionally independent of its non-descendants, given its parents.
- We can use this network to find implied independence!
    - It assumes that flu and being an undergrad are independent.
    - It also assumes that fever and tired are conditionally independent given flu.

## Properties

- **Bayesian networks must be acyclic**

![[attachments/Untitled 3 124.png|Untitled 3 124.png]]

## Reconstructing the joint

![[attachments/Untitled 4 119.png|Untitled 4 119.png]]

![[attachments/Untitled 5 117.png|Untitled 5 117.png]]

- By Bayes theorem, the joint is just the product of each random variable, given the parents of that random variable.
    
    ![[attachments/Untitled 6 115.png|Untitled 6 115.png]]
    

## Inference with Bayes Net

![[attachments/Untitled 7 112.png|Untitled 7 112.png]]

- During inference, we can also pull from the joint distribution using marginalization (LOTP)
    
    ![[attachments/Untitled 8 104.png|Untitled 8 104.png]]
    

# Independent discrete RVs

![[attachments/Untitled 9 100.png|Untitled 9 100.png]]

- All events $(X = x, Y = y)$﻿ must be independent for $X, Y$﻿ to be independent RVs.

# Space complexity

- The number of entries in a joint table is $O(k^n)$﻿, where $n$﻿ is the number of random variables and $k$﻿ is the number of values that each random variable can take on.
- In comparison, Bayes Nets are $O(n)$﻿
    - We create a Bayes net, and then we infer with conditional probabilities to save space

# Covariance

- Describes the extent to which two variables change together
    
    - Positive covariance means they tend to move in the same directio
    - Negative covariance means they move in opposite directions
    - Covariance of zero suggests no linear relationship → does not imply independence
    
    ![[attachments/Untitled 10 95.png|Untitled 10 95.png]]
    
- Below is a graph of two random variables. The left graph are two that are independent. The right graph are two that are dependent.
    
    ![[attachments/Untitled 11 92.png|Untitled 11 92.png]]
    
- Independent random variables always have zero covariance → this is because one variable does not change in any predictable way when the other one changes.
- If two variables have nonzero covariance, then it implies that they are dependent.
    
    ![[attachments/Untitled 12 89.png|Untitled 12 89.png]]
    

## Definition

![[attachments/Untitled 13 83.png|Untitled 13 83.png]]

## Properties

![[attachments/Untitled 14 72.png|Untitled 14 72.png]]

## Cauchy Schwarz

![[attachments/Untitled 15 69.png|Untitled 15 69.png]]

## Correlation

![[attachments/Untitled 16 65.png|Untitled 16 65.png]]