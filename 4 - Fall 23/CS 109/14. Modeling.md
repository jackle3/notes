---
Date: 2023-10-27
---
# Joint Distribution

- Given two random variables, produce a function or table etc where for any assignment to the variables, we can give the joint of it.
    
    ![Untitled 165.png](../../attachments/Untitled%20165.png)
    

# Bayesian Network

- Express models of multiple random variables by drawing them out as a network

![Untitled 1 128.png](../../attachments/Untitled%201%20128.png)

- If we had multiple random variables, describing the joint would be very large.
    - For a case with four random bernoulli variables, the joint has $2^4 = 16$﻿ joint probabilities.
- We can represent the joint by describing causality.
    - Each random variable is caused by its parents → we care about P(node | parents)

![Untitled 2 127.png](../../attachments/Untitled%202%20127.png)

## Conditional Independence

- Each random variable is conditionally independent of its non-descendants, given its parents.
- We can use this network to find implied independence!
    - It assumes that flu and being an undergrad are independent.
    - It also assumes that fever and tired are conditionally independent given flu.

## Properties

- **Bayesian networks must be acyclic**

![Untitled 3 124.png](../../attachments/Untitled%203%20124.png)

## Reconstructing the joint

![Untitled 4 119.png](../../attachments/Untitled%204%20119.png)

![Untitled 5 117.png](../../attachments/Untitled%205%20117.png)

- By Bayes theorem, the joint is just the product of each random variable, given the parents of that random variable.
    
    ![Untitled 6 115.png](../../attachments/Untitled%206%20115.png)
    

## Inference with Bayes Net

![Untitled 7 112.png](../../attachments/Untitled%207%20112.png)

- During inference, we can also pull from the joint distribution using marginalization (LOTP)
    
    ![Untitled 8 104.png](../../attachments/Untitled%208%20104.png)
    

# Independent discrete RVs

![Untitled 9 100.png](../../attachments/Untitled%209%20100.png)

- All events $(X = x, Y = y)$﻿ must be independent for $X, Y$﻿ to be independent RVs.

# Space complexity

- The number of entries in a joint table is $O(k^n)$﻿, where $n$﻿ is the number of random variables and $k$﻿ is the number of values that each random variable can take on.
- In comparison, Bayes Nets are $O(n)$﻿
    - We create a Bayes net, and then we infer with conditional probabilities to save space

# Covariance

- Describes the extent to which two variables change together
    
    - Positive covariance means they tend to move in the same directio
    - Negative covariance means they move in opposite directions
    - Covariance of zero suggests no linear relationship → does not imply independence
    
    ![Untitled 10 95.png](../../attachments/Untitled%2010%2095.png)
    
- Below is a graph of two random variables. The left graph are two that are independent. The right graph are two that are dependent.
    
    ![Untitled 11 92.png](../../attachments/Untitled%2011%2092.png)
    
- Independent random variables always have zero covariance → this is because one variable does not change in any predictable way when the other one changes.
- If two variables have nonzero covariance, then it implies that they are dependent.
    
    ![Untitled 12 89.png](../../attachments/Untitled%2012%2089.png)
    

## Definition

![Untitled 13 83.png](../../attachments/Untitled%2013%2083.png)

## Properties

![Untitled 14 72.png](../../attachments/Untitled%2014%2072.png)

## Cauchy Schwarz

![Untitled 15 69.png](../../attachments/Untitled%2015%2069.png)

## Correlation

![Untitled 16 65.png](../../attachments/Untitled%2016%2065.png)