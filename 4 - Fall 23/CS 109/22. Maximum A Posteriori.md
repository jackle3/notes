---
Date: 2023-11-15
---
# Issues with MLE

- MLE tends to overfit data → it does not generalize to unseen data
- For example, when trying to find the parameters for a uniform, it will set $\alpha$﻿ to be the minimum element, and $\beta$﻿ to be the maximum element.
    
    ![Untitled 187.png](../../attachments/Untitled%20187.png)
    

## MLE vs Beta

- Using something like Beta instead of MLE allows you to incorporate a prior belief, giving us not only a distribution but also a more informed posterior belief.

![Untitled 1 150.png](../../attachments/Untitled%201%20150.png)

# Maximum A Posteriori

- This allows us to incorporate priors in parameter estimation. This means we can **avoid overfitting**, which was the main issue with MLE.
- Using Bayesian terminology, the MAP estimate is the mode of the "posterior" distribution for our parameters $\theta$﻿.

## MAP vs MLE

- MLE chooses parameters that makes the data most likely (e.g. maximizes likelihood). It incorporates prior beliefs
    
    ![Untitled 2 149.png](../../attachments/Untitled%202%20149.png)
    
- MAP chooses the most likely parameter given the values of the data
    
    ![Untitled 3 146.png](../../attachments/Untitled%203%20146.png)
    
- In other words:
    - MLE finds **what parameter makes the data most likely**
    - MAP finds **what is the most likely parameter of the data**
- By Bayes’ Theorem, it’s related by your prior belief of the parameters.
    
    - MAP is basically just MLE, but you also multiply it by your prior belief of the parameter.
    
    ![Untitled 4 141.png](../../attachments/Untitled%204%20141.png)
    

## Notation Shorthand

![Untitled 5 139.png](../../attachments/Untitled%205%20139.png)

## MAP with Bernoulli and Binomial

- This would basically be like taking the argmax of a Beta distribution

![Untitled 6 137.png](../../attachments/Untitled%206%20137.png)

- If we had a Beta as our prior, it would be like below. Using $Beta(1, 1) = Uni(0, 1)$﻿ is the same as saying we haven’t seen any imaginary trials yet.

![Untitled 7 133.png](../../attachments/Untitled%207%20133.png)

- The mode of a beta distribution is $\frac{\alpha - 1}{\alpha + \beta - 2}$﻿. Notice that the MAP result is the mode of the posterior distribution.

# Conjugate Distributions

- These are distributions that describe the most likely parameters for the distribution given our data.
    - Basically, if we express our prior as that distribution, our posterior will also be that distribution. The **mode** of the posterior distribution is the best parameter.

![Untitled 8 124.png](../../attachments/Untitled%208%20124.png)

- For example for Bernoulli, if we use a beta prior, our posterior will also be a beta. Then, we take the mode of that posterior beta distribution to get out parameter $p$﻿

## MAP for Poisson: Gamma

![Untitled 9 120.png](../../attachments/Untitled%209%20120.png)

![Untitled 10 115.png](../../attachments/Untitled%2010%20115.png)

## MAP for Multinomial: Dirichlet

- The Dirichlet is basically just a Beta but for a multinomial

![Untitled 11 111.png](../../attachments/Untitled%2011%20111.png)

# Finding an MAP estimate

1. We want to find the parameter that is most likely, given our dataset
    
    ![Untitled 12 108.png](../../attachments/Untitled%2012%20108.png)
    
2. We can use Baye’s Theorem to rewrite our expressoin
    
    ![Untitled 13 100.png](../../attachments/Untitled%2013%20100.png)
    
3. Since our data points are IID, we can extract them using the product rule
    
    ![Untitled 14 88.png](../../attachments/Untitled%2014%2088.png)
    
4. Notice that the normalization constant does not depend on $\theta$﻿. Since we are finding argmax w.r.t $\theta$﻿, we can drop the normalization constant.
    
    ![Untitled 15 84.png](../../attachments/Untitled%2015%2084.png)
    
5. To make our computations easier, we can take the $\log$﻿ of this expression
    
    ![Untitled 16 78.png](../../attachments/Untitled%2016%2078.png)
    

## Example MAP with Normal Prior

![Untitled 17 73.png](../../attachments/Untitled%2017%2073.png)

![Untitled 18 66.png](../../attachments/Untitled%2018%2066.png)

# Summary of MAP

![Untitled 19 59.png](../../attachments/Untitled%2019%2059.png)

- Note that $g(\theta)$﻿ is just the prior likelihood of that parameter based on the prior distribution. This can be the PDF or the PMF.