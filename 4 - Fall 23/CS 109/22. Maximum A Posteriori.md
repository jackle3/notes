---
Date: 2023-11-15
---
# Issues with MLE

- MLE tends to overfit data → it does not generalize to unseen data
- For example, when trying to find the parameters for a uniform, it will set $\alpha$﻿ to be the minimum element, and $\beta$﻿ to be the maximum element.
    
    ![[attachments/Untitled 187.png|Untitled 187.png]]
    

## MLE vs Beta

- Using something like Beta instead of MLE allows you to incorporate a prior belief, giving us not only a distribution but also a more informed posterior belief.

![[attachments/Untitled 1 150.png|Untitled 1 150.png]]

# Maximum A Posteriori

- This allows us to incorporate priors in parameter estimation. This means we can **avoid overfitting**, which was the main issue with MLE.
- Using Bayesian terminology, the MAP estimate is the mode of the "posterior" distribution for our parameters $\theta$﻿.

## MAP vs MLE

- MLE chooses parameters that makes the data most likely (e.g. maximizes likelihood). It incorporates prior beliefs
    
    ![[attachments/Untitled 2 149.png|Untitled 2 149.png]]
    
- MAP chooses the most likely parameter given the values of the data
    
    ![[attachments/Untitled 3 146.png|Untitled 3 146.png]]
    
- In other words:
    - MLE finds **what parameter makes the data most likely**
    - MAP finds **what is the most likely parameter of the data**
- By Bayes’ Theorem, it’s related by your prior belief of the parameters.
    
    - MAP is basically just MLE, but you also multiply it by your prior belief of the parameter.
    
    ![[attachments/Untitled 4 141.png|Untitled 4 141.png]]
    

## Notation Shorthand

![[attachments/Untitled 5 139.png|Untitled 5 139.png]]

## MAP with Bernoulli and Binomial

- This would basically be like taking the argmax of a Beta distribution

![[attachments/Untitled 6 137.png|Untitled 6 137.png]]

- If we had a Beta as our prior, it would be like below. Using $Beta(1, 1) = Uni(0, 1)$﻿ is the same as saying we haven’t seen any imaginary trials yet.

![[attachments/Untitled 7 133.png|Untitled 7 133.png]]

- The mode of a beta distribution is $\frac{\alpha - 1}{\alpha + \beta - 2}$﻿. Notice that the MAP result is the mode of the posterior distribution.

# Conjugate Distributions

- These are distributions that describe the most likely parameters for the distribution given our data.
    - Basically, if we express our prior as that distribution, our posterior will also be that distribution. The **mode** of the posterior distribution is the best parameter.

![[attachments/Untitled 8 124.png|Untitled 8 124.png]]

- For example for Bernoulli, if we use a beta prior, our posterior will also be a beta. Then, we take the mode of that posterior beta distribution to get out parameter $p$﻿

## MAP for Poisson: Gamma

![[attachments/Untitled 9 120.png|Untitled 9 120.png]]

![[attachments/Untitled 10 115.png|Untitled 10 115.png]]

## MAP for Multinomial: Dirichlet

- The Dirichlet is basically just a Beta but for a multinomial

![[attachments/Untitled 11 111.png|Untitled 11 111.png]]

# Finding an MAP estimate

1. We want to find the parameter that is most likely, given our dataset
    
    ![[attachments/Untitled 12 108.png|Untitled 12 108.png]]
    
2. We can use Baye’s Theorem to rewrite our expressoin
    
    ![[attachments/Untitled 13 100.png|Untitled 13 100.png]]
    
3. Since our data points are IID, we can extract them using the product rule
    
    ![[attachments/Untitled 14 88.png|Untitled 14 88.png]]
    
4. Notice that the normalization constant does not depend on $\theta$﻿. Since we are finding argmax w.r.t $\theta$﻿, we can drop the normalization constant.
    
    ![[attachments/Untitled 15 84.png|Untitled 15 84.png]]
    
5. To make our computations easier, we can take the $\log$﻿ of this expression
    
    ![[attachments/Untitled 16 78.png|Untitled 16 78.png]]
    

## Example MAP with Normal Prior

![[attachments/Untitled 17 73.png|Untitled 17 73.png]]

![[attachments/Untitled 18 66.png|Untitled 18 66.png]]

# Summary of MAP

![[attachments/Untitled 19 59.png|Untitled 19 59.png]]

- Note that $g(\theta)$﻿ is just the prior likelihood of that parameter based on the prior distribution. This can be the PDF or the PMF.