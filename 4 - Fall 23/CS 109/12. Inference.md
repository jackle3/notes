---
Date: 2023-10-23
---
# Epsilon trick

- Recall that $f(X = x)$﻿ is just zero, since $f$﻿ is the PDF function which is the likelihood, not the probability.
- We have to get an area under the curve of $f$﻿ to get our probability.
- For a very small $\epsilon_x$﻿, the epsilon trick is the picture below.
    
    ![Untitled 178.png](../../attachments/Untitled%20178.png)
    
    - As $\epsilon$﻿ approaches zero, this becomes more and more true.

## Example

- Recall that $f(X = x)$﻿ is the likelihood. We can use the epsilon trick to solve this, and see that the epsilons cancel out.

![Untitled 1 141.png](../../attachments/Untitled%201%20141.png)

- The ratio of $P(X = 10)$﻿ to $P(X = 5)$﻿ tells us how much more likely you are to complete in 10 hours than in 5 hours.
- You are 518 times more likely to finish your PSET in 10 hours than in 5 hours.

# Inference

- Updating one’s belief about a random variable (or multiple) based on conditional knowledge regarding another random variable (or multiple) in a probabilistic model.
    - TLDR: conditional probability with random variables.
- If you see an observation/evidence, how do you change the PDF or the PMF of a RV?

## Example

- Suppose $P(R = r)$﻿ is the probability that they have a roommate, and $Y$﻿ be the class year.
- Once we condition the room type by the year, it changes the distribution significiantly.
    
    ![Untitled 2 140.png](../../attachments/Untitled%202%20140.png)
    
- Here is the joint table, as well as a table of the inference
    
    ![Untitled 3 137.png](../../attachments/Untitled%203%20137.png)
    
- This is the graph. You can see that the probability of getting each type of room changes dramatically when conditioned on different years.
    
    ![Untitled 4 132.png](../../attachments/Untitled%204%20132.png)
    

# Revisit Bayes

![Untitled 5 130.png](../../attachments/Untitled%205%20130.png)

- Bayes theorem is taking a prior relief, and calculating an updated (posterior) belief when given evidence.
    - $P(E | B)$﻿ is how likely is it to see the evidence given our prior belief?

# Inference with Discrete

![Untitled 6 128.png](../../attachments/Untitled%206%20128.png)

# Baby Hearing Example

![Untitled 7 125.png](../../attachments/Untitled%207%20125.png)

- The left column is $P(X = x | Y = 1)$﻿, and the right column is $P(X = x | Y = 0)$﻿.
- We also know that $P(Y = 1) = 3/4$﻿
- We want to know $P(Y = 1 | X = 0)$﻿!

$P(Y = 1 | X=0) = \frac{P(X=0|Y=1)P(Y=1)}{P(X = 0)}$

- Expand the normalization constant by the law of total probability to get our results.
    
    ![Untitled 8 117.png](../../attachments/Untitled%208%20117.png)
    

# Inference with Discrete and Continuous

## Conditioning on Continuous

![Untitled 9 113.png](../../attachments/Untitled%209%20113.png)

## Conditioning on Discrete

![Untitled 10 108.png](../../attachments/Untitled%2010%20108.png)

## Summary

![Untitled 11 104.png](../../attachments/Untitled%2011%20104.png)

## Chain rule and LOTP

- With continous, you can just use the PDF as a substitute for the PMF.

![Untitled 12 101.png](../../attachments/Untitled%2012%20101.png)

# Elephant Example

![Untitled 13 95.png](../../attachments/Untitled%2013%2095.png)

- There is a 1/2 prior probability of an elephant being a girl or a boy.

![Untitled 14 83.png](../../attachments/Untitled%2014%2083.png)

- The equation above simplifies to become

$\frac{f(X = 163 | G = 1)}{f(X = 163 | G = 1) + f(X = 163 | G = 0)}$

- From here, we can just use the equation of the PDF for each of the terms to solve this.

## Finding the joint probability

- This is implied. Since we have X | G = 1 and X | G = 0, and we also know P(G = 1), we can calculate it using chain rule.

![Untitled 15 79.png](../../attachments/Untitled%2015%2079.png)