---
Date: 2023-10-23
---
# Epsilon trick

- Recall that $f(X = x)$﻿ is just zero, since $f$﻿ is the PDF function which is the likelihood, not the probability.
- We have to get an area under the curve of $f$﻿ to get our probability.
- For a very small $\epsilon_x$﻿, the epsilon trick is the picture below.
    
    ![[attachments/Untitled 178.png|Untitled 178.png]]
    
    - As $\epsilon$﻿ approaches zero, this becomes more and more true.

## Example

- Recall that $f(X = x)$﻿ is the likelihood. We can use the epsilon trick to solve this, and see that the epsilons cancel out.

![[attachments/Untitled 1 141.png|Untitled 1 141.png]]

- The ratio of $P(X = 10)$﻿ to $P(X = 5)$﻿ tells us how much more likely you are to complete in 10 hours than in 5 hours.
- You are 518 times more likely to finish your PSET in 10 hours than in 5 hours.

# Inference

- Updating one’s belief about a random variable (or multiple) based on conditional knowledge regarding another random variable (or multiple) in a probabilistic model.
    - TLDR: conditional probability with random variables.
- If you see an observation/evidence, how do you change the PDF or the PMF of a RV?

## Example

- Suppose $P(R = r)$﻿ is the probability that they have a roommate, and $Y$﻿ be the class year.
- Once we condition the room type by the year, it changes the distribution significiantly.
    
    ![[attachments/Untitled 2 140.png|Untitled 2 140.png]]
    
- Here is the joint table, as well as a table of the inference
    
    ![[attachments/Untitled 3 137.png|Untitled 3 137.png]]
    
- This is the graph. You can see that the probability of getting each type of room changes dramatically when conditioned on different years.
    
    ![[attachments/Untitled 4 132.png|Untitled 4 132.png]]
    

# Revisit Bayes

![[attachments/Untitled 5 130.png|Untitled 5 130.png]]

- Bayes theorem is taking a prior relief, and calculating an updated (posterior) belief when given evidence.
    - $P(E | B)$﻿ is how likely is it to see the evidence given our prior belief?

# Inference with Discrete

![[attachments/Untitled 6 128.png|Untitled 6 128.png]]

# Baby Hearing Example

![[attachments/Untitled 7 125.png|Untitled 7 125.png]]

- The left column is $P(X = x | Y = 1)$﻿, and the right column is $P(X = x | Y = 0)$﻿.
- We also know that $P(Y = 1) = 3/4$﻿
- We want to know $P(Y = 1 | X = 0)$﻿!

$P(Y = 1 | X=0) = \frac{P(X=0|Y=1)P(Y=1)}{P(X = 0)}$

- Expand the normalization constant by the law of total probability to get our results.
    
    ![[attachments/Untitled 8 117.png|Untitled 8 117.png]]
    

# Inference with Discrete and Continuous

## Conditioning on Continuous

![[attachments/Untitled 9 113.png|Untitled 9 113.png]]

## Conditioning on Discrete

![[attachments/Untitled 10 108.png|Untitled 10 108.png]]

## Summary

![[attachments/Untitled 11 104.png|Untitled 11 104.png]]

## Chain rule and LOTP

- With continous, you can just use the PDF as a substitute for the PMF.

![[attachments/Untitled 12 101.png|Untitled 12 101.png]]

# Elephant Example

![[attachments/Untitled 13 95.png|Untitled 13 95.png]]

- There is a 1/2 prior probability of an elephant being a girl or a boy.

![[attachments/Untitled 14 83.png|Untitled 14 83.png]]

- The equation above simplifies to become

$\frac{f(X = 163 | G = 1)}{f(X = 163 | G = 1) + f(X = 163 | G = 0)}$

- From here, we can just use the equation of the PDF for each of the terms to solve this.

## Finding the joint probability

- This is implied. Since we have X | G = 1 and X | G = 0, and we also know P(G = 1), we can calculate it using chain rule.

![[attachments/Untitled 15 79.png|Untitled 15 79.png]]