---
Date: 2023-11-27
---
# Background

## Sigmoid Function

- The sigmoid function is very useful because regardless of what input you give it, the output will be between 0 and 1 → can potentially be used for probability.

![[attachments/Untitled 174.png|Untitled 174.png]]

- The inflection point is at $z = 0$﻿, and $\sigma(0) = 0.5$﻿.
    - If the input to the sigmoid is negative, then the output is less than 0.5.
    - If the input to the sigmoid is positive, then the output is more than 0.5.

## Notation

![[attachments/Untitled 1 137.png|Untitled 1 137.png]]

## Chain Rule

- We can decompose $f(x)$﻿ to say that $f$﻿ is actually a function of some other function $z(x)$﻿

![[attachments/Untitled 2 136.png|Untitled 2 136.png]]

# Logistic Regression

- Instead of making an assumption like Naive Bayes, build a machine to directly figure out the probability $P(Y | \mathbf{X})$﻿
    
    ![[attachments/Untitled 3 133.png|Untitled 3 133.png]]
    
- GIven the input $\mathbf{x}$﻿ and and some parameters $\theta$﻿, directly predict $P(Y = 1 | \mathbf{x})$﻿
    
    ![[attachments/Untitled 4 128.png|Untitled 4 128.png]]
    
- $\theta$﻿ is essentially the **weight of each feature** in the input $\mathbf{x}$﻿
    
    ![[attachments/Untitled 5 126.png|Untitled 5 126.png]]
    
    - The inputs are $\mathbf{x} = [0, 1, 1, \dots]$﻿
    - The weights are $\theta = [\theta_1, \theta_2, \dots]$﻿
        - Note that $\theta_j \in \R$﻿.
        - **The weights determine how good your model is. It affects the prediction.**
    - It first does a weighted sum of all the inputs, product a dot product $z = \theta^T \mathbf{x}$﻿.
    - Then, it uses the squashing function (sigmoid) to turn $z$﻿ it into a probability
    - The prediction $P(Y = 1 | \mathbf{x})$﻿ is the result of the squashing function.
- Note that $x_0$﻿ does not correspond to any feature, and it’s always set to $1$﻿.
    - This is essentially the **bias term**. It allows the model to change $\theta_0$﻿ and always include it, as the bias for our weighted sum.

## Primary Assumption

![[attachments/Untitled 6 124.png|Untitled 6 124.png]]

![[attachments/Untitled 7 121.png|Untitled 7 121.png]]

- Recall that $\sigma(0) = 0.5$﻿. Also consider that $\sigma(z)$﻿ is the probability that $Y = 1$﻿.
    - If the input to the sigmoid is positive, then $\sigma(z) > 0.5$﻿, so predict 1.
    - If the input is negative, then predict 0.

# Classification

![[attachments/Untitled 8 113.png|Untitled 8 113.png]]

# Learning Weights

![[attachments/Untitled 9 109.png|Untitled 9 109.png]]

- Remember that the weights (aka the parameters or the $\theta$﻿s) define how well our model works.
    - We can learn the weights using MLE!
- Remember that the log likelihood tells you “how likely does the data look under the parameters $\theta$﻿”
    - This is essentially the score of the parameters
    - The score tells you whether the parameters make the data likely not.
    - You can use score to tell the difference between good and bad parameters.
- Once we have the derivative, we can learn the weights (parameters) using gradient ascent.
    
    ![[attachments/Untitled 10 104.png|Untitled 10 104.png]]
    
    - This works really well because the logistic log likelihood function is **convex**.
    - Alternatively, use gradient descent with the negative log likelihood.

## Gradient Ascent Code

![[attachments/Untitled 11 100.png|Untitled 11 100.png]]

- Note that $x_j$﻿ is the j-th feature of the input vector $\mathbf{x}$﻿
    - Also note that $x_0 = 1$﻿, which allows $\theta_0$﻿ to be the bias/intercept term

# Log-likelihood function

- Recall that the continuous PMF of a Bernoulli is $P(Y = y) = p^y (1-p)^{1-y}$﻿
- We have our probabilities from the logistic assumption
    
    ![[attachments/Untitled 12 97.png|Untitled 12 97.png]]
    
- In this case, our prediction $P(Y | X)$﻿ is a Bernoulli, so we can build our likelihood using the PMF of the Bernoulli.
    
    ![[attachments/Untitled 13 91.png|Untitled 13 91.png]]
    
- Because the input data (training examples) is IID, we can get the likelihood of **all** the data
    
    ![[attachments/Untitled 14 79.png|Untitled 14 79.png]]
    
    - Note that $\mathbf{x}^{(i)}$﻿ mean the i-th data point that we are given.
- We then just take the log of this to get our log-likelihood function
    
    ![[attachments/Untitled 15 76.png|Untitled 15 76.png]]
    

## Gradient of the prediction (sigmoid)

- The sigmoid has a very beautiful derivative.
    
    ![[attachments/Untitled 16 71.png|Untitled 16 71.png]]
    
- If we apply this rule with the sigmoid functions from our Bernoulli above:
    
    ![[attachments/Untitled 17 66.png|Untitled 17 66.png]]
    
    $\frac{\partial z}{\partial \theta_j} = \frac{\partial }{\partial \theta_j}\theta^T \mathbf{x} = \frac{\partial}{\partial \theta_j}(\theta_tx_1 + \dots + \theta_jx_j + \dots) = x_j$
    
- Putting this back together, the derivative of $P(Y = 1 | X = \mathbf{x}) =\hat{y} = \sigma(\theta^T\mathbf{x})$﻿ is just
    
    ![[attachments/Untitled 18 60.png|Untitled 18 60.png]]
    

## Gradient of the LL function

![[attachments/Untitled 19 53.png|Untitled 19 53.png]]

- For notation, say that $\hat{y} = \sigma(\theta^T \mathbf{x})$﻿

  

- We can focus on just a single example, since the derivative of a sum is the sum of derivatives
    
    ![[attachments/Untitled 20 52.png|Untitled 20 52.png]]
    
- Use the chain rule to make the derivative much easier.
    
    ![[attachments/Untitled 21 47.png|Untitled 21 47.png]]
    
- We can then put this back into our original sum to ge tthe final derivative!
    
    ![[attachments/Untitled 22 43.png|Untitled 22 43.png]]
    

# Summary

![[attachments/Untitled 23 40.png|Untitled 23 40.png]]

- Note that $\mathbf{x}^{(i)}$﻿ mean the i-th data point that we are given.
    - $x_j^{(i)}$﻿ is the j-th entry of the i-th data point
- Once we have the derivative, put it into gradient ascent/descent to optimize it
    
    ![[attachments/Untitled 11 100.png|Untitled 11 100.png]]
    
- To make a prediction, use our logistic regression assumption
    
    ![[attachments/Untitled 24 36.png|Untitled 24 36.png]]
    

# Logistic Regression vs Naive Bayes

- Logistic regression easily deals with **continuous inputs**
- Naive Bayes naturally handles multi-valued discrete features since it uses the multinomial distribution for $P(X_i | Y)$