---
Date: 2023-11-27
---
# Background

## Sigmoid Function

- The sigmoid function is very useful because regardless of what input you give it, the output will be between 0 and 1 → can potentially be used for probability.

![Untitled 174.png](../../attachments/Untitled%20174.png)

- The inflection point is at $z = 0$﻿, and $\sigma(0) = 0.5$﻿.
    - If the input to the sigmoid is negative, then the output is less than 0.5.
    - If the input to the sigmoid is positive, then the output is more than 0.5.

## Notation

![Untitled 1 137.png](../../attachments/Untitled%201%20137.png)

## Chain Rule

- We can decompose $f(x)$﻿ to say that $f$﻿ is actually a function of some other function $z(x)$﻿

![Untitled 2 136.png](../../attachments/Untitled%202%20136.png)

# Logistic Regression

- Instead of making an assumption like Naive Bayes, build a machine to directly figure out the probability $P(Y | \mathbf{X})$﻿
    
    ![Untitled 3 133.png](../../attachments/Untitled%203%20133.png)
    
- GIven the input $\mathbf{x}$﻿ and and some parameters $\theta$﻿, directly predict $P(Y = 1 | \mathbf{x})$﻿
    
    ![Untitled 4 128.png](../../attachments/Untitled%204%20128.png)
    
- $\theta$﻿ is essentially the **weight of each feature** in the input $\mathbf{x}$﻿
    
    ![Untitled 5 126.png](../../attachments/Untitled%205%20126.png)
    
    - The inputs are $\mathbf{x} = [0, 1, 1, \dots]$﻿
    - The weights are $\theta = [\theta_1, \theta_2, \dots]$﻿
        - Note that $\theta_j \in \R$﻿.
        - **The weights determine how good your model is. It affects the prediction.**
    - It first does a weighted sum of all the inputs, product a dot product $z = \theta^T \mathbf{x}$﻿.
    - Then, it uses the squashing function (sigmoid) to turn $z$﻿ it into a probability
    - The prediction $P(Y = 1 | \mathbf{x})$﻿ is the result of the squashing function.
- Note that $x_0$﻿ does not correspond to any feature, and it’s always set to $1$﻿.
    - This is essentially the **bias term**. It allows the model to change $\theta_0$﻿ and always include it, as the bias for our weighted sum.

## Primary Assumption

![Untitled 6 124.png](../../attachments/Untitled%206%20124.png)

![Untitled 7 121.png](../../attachments/Untitled%207%20121.png)

- Recall that $\sigma(0) = 0.5$﻿. Also consider that $\sigma(z)$﻿ is the probability that $Y = 1$﻿.
    - If the input to the sigmoid is positive, then $\sigma(z) > 0.5$﻿, so predict 1.
    - If the input is negative, then predict 0.

# Classification

![Untitled 8 113.png](../../attachments/Untitled%208%20113.png)

# Learning Weights

![Untitled 9 109.png](../../attachments/Untitled%209%20109.png)

- Remember that the weights (aka the parameters or the $\theta$﻿s) define how well our model works.
    - We can learn the weights using MLE!
- Remember that the log likelihood tells you “how likely does the data look under the parameters $\theta$﻿”
    - This is essentially the score of the parameters
    - The score tells you whether the parameters make the data likely not.
    - You can use score to tell the difference between good and bad parameters.
- Once we have the derivative, we can learn the weights (parameters) using gradient ascent.
    
    ![Untitled 10 104.png](../../attachments/Untitled%2010%20104.png)
    
    - This works really well because the logistic log likelihood function is **convex**.
    - Alternatively, use gradient descent with the negative log likelihood.

## Gradient Ascent Code

![Untitled 11 100.png](../../attachments/Untitled%2011%20100.png)

- Note that $x_j$﻿ is the j-th feature of the input vector $\mathbf{x}$﻿
    - Also note that $x_0 = 1$﻿, which allows $\theta_0$﻿ to be the bias/intercept term

# Log-likelihood function

- Recall that the continuous PMF of a Bernoulli is $P(Y = y) = p^y (1-p)^{1-y}$﻿
- We have our probabilities from the logistic assumption
    
    ![Untitled 12 97.png](../../attachments/Untitled%2012%2097.png)
    
- In this case, our prediction $P(Y | X)$﻿ is a Bernoulli, so we can build our likelihood using the PMF of the Bernoulli.
    
    ![Untitled 13 91.png](../../attachments/Untitled%2013%2091.png)
    
- Because the input data (training examples) is IID, we can get the likelihood of **all** the data
    
    ![Untitled 14 79.png](../../attachments/Untitled%2014%2079.png)
    
    - Note that $\mathbf{x}^{(i)}$﻿ mean the i-th data point that we are given.
- We then just take the log of this to get our log-likelihood function
    
    ![Untitled 15 76.png](../../attachments/Untitled%2015%2076.png)
    

## Gradient of the prediction (sigmoid)

- The sigmoid has a very beautiful derivative.
    
    ![Untitled 16 71.png](../../attachments/Untitled%2016%2071.png)
    
- If we apply this rule with the sigmoid functions from our Bernoulli above:
    
    ![Untitled 17 66.png](../../attachments/Untitled%2017%2066.png)
    
    $\frac{\partial z}{\partial \theta_j} = \frac{\partial }{\partial \theta_j}\theta^T \mathbf{x} = \frac{\partial}{\partial \theta_j}(\theta_tx_1 + \dots + \theta_jx_j + \dots) = x_j$
    
- Putting this back together, the derivative of $P(Y = 1 | X = \mathbf{x}) =\hat{y} = \sigma(\theta^T\mathbf{x})$﻿ is just
    
    ![Untitled 18 60.png](../../attachments/Untitled%2018%2060.png)
    

## Gradient of the LL function

![Untitled 19 53.png](../../attachments/Untitled%2019%2053.png)

- For notation, say that $\hat{y} = \sigma(\theta^T \mathbf{x})$﻿

  

- We can focus on just a single example, since the derivative of a sum is the sum of derivatives
    
    ![Untitled 20 52.png](../../attachments/Untitled%2020%2052.png)
    
- Use the chain rule to make the derivative much easier.
    
    ![Untitled 21 47.png](../../attachments/Untitled%2021%2047.png)
    
- We can then put this back into our original sum to ge tthe final derivative!
    
    ![Untitled 22 43.png](../../attachments/Untitled%2022%2043.png)
    

# Summary

![Untitled 23 40.png](../../attachments/Untitled%2023%2040.png)

- Note that $\mathbf{x}^{(i)}$﻿ mean the i-th data point that we are given.
    - $x_j^{(i)}$﻿ is the j-th entry of the i-th data point
- Once we have the derivative, put it into gradient ascent/descent to optimize it
    
    ![Untitled 11 100.png](../../attachments/Untitled%2011%20100.png)
    
- To make a prediction, use our logistic regression assumption
    
    ![Untitled 24 36.png](../../attachments/Untitled%2024%2036.png)
    

# Logistic Regression vs Naive Bayes

- Logistic regression easily deals with **continuous inputs**
- Naive Bayes naturally handles multi-valued discrete features since it uses the multinomial distribution for $P(X_i | Y)$