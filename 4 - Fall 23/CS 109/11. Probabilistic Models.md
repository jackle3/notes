---
Date: 2023-10-20
---
# Discrete Probabilistic Models

- There often has to be multiple random variables interacting with each other for problems.

## Joint PMF

- Gives us the likelihood to one assignment, as well as the other assignment.

![Untitled 182.png](attachments/Untitled%20182.png)

- Generally, the Joint PMF of two discrete RVs is represented in a table.

![Untitled 1 145.png](attachments/Untitled%201%20145.png)

- Each cell is a joint probability. For example, there is a `0.10` probability that someone is **both** a junior **and** in a relationship.
- Every cell of the joint table is mutually exclusive, and they span the sample space.
    
    $\sum_{x\in X}\sum_{y \in Y}P(X=x, Y=y) = 1$
    
    where X is dating status and Y is year.
    

## Marginal Distribution

- Adding up the rows tells us the probability of the class year, $P(Y = \text{year})$﻿.
- Adding up the columns tells us the probabilty of that relationship status, $P(X =\text{relation})$﻿.
- This is called the marginal / marginalization. We have a joint table, and we marginalize all the other RVs to extract a single RV from the table. This is the **LOTP**!

![Untitled 2 144.png](attachments/Untitled%202%20144.png)

## More RVs

- The joint table can get very big when there are more random variables. In the case of three, it requires two separate tables, for each disease status.

![Untitled 3 141.png](attachments/Untitled%203%20141.png)

- In this case, there are a total of 12 entires. There are 2 assignments for $D$﻿, 2 assignments for $S$﻿, and three assignments for $F$﻿.
- The marginal is a double sum over all assignments of the remaining random variables.

![Untitled 4 136.png](attachments/Untitled%204%20136.png)

## More efficient ways to represent multiple RVs

- If we were to roll 100 dice, there are 6 random variables that come out of it, for each $X_i$﻿.

![Untitled 5 134.png](attachments/Untitled%205%20134.png)

- If we were to write this into a joint table, there are $100^6$﻿ entries in the table.

# Multinomial Random Variable

- Instead of thinking about coin flips with two outcomes, we think about dice rolls with $m$﻿ possible outcomes.

![Untitled 6 132.png](attachments/Untitled%206%20132.png)

![Untitled 7 128.png](attachments/Untitled%207%20128.png)

- $p_i^{c_i}$﻿ basically asks what are the chances of getting outcome $i$﻿, and how many times do we want to see outcome $i$﻿?
- Once we get all the probabilities, we also have to think about the number of ways to order all those outcomes, similar to how we thought about the binomial.

## Parameters of Multinomial RV

![Untitled 8 119.png](attachments/Untitled%208%20119.png)

## Dice Example

![Untitled 9 115.png](attachments/Untitled%209%20115.png)

- The term ${7 \choose {1, 1, 0, 2, 0, 3}}$﻿ is the same as $\frac{7!}{1!1!0!2!0!3!} = \frac{7!}{2!3!}$﻿

# Probabilistic text analysis

![Untitled 10 110.png](attachments/Untitled%2010%20110.png)

- In this multinomial, we’re basically still rolling dice $n$﻿ times. However now, our dice is weighted and has a side for every word in the english language.
    - It’s essentially rolling a $988,968$﻿-sided die, where not every outcome is equally likely.

## Modal text as a multinomial

![Untitled 11 106.png](attachments/Untitled%2011%20106.png)

- In the case above, $p_{\text{viagra}}$﻿ is the chance that a **spam** writer writes the word viagra.

# Federalist Papers Example

![Untitled 12 103.png](attachments/Untitled%2012%20103.png)

- We count up the probability of the author writing each word, and compare them.
    
    ![Untitled 13 97.png](attachments/Untitled%2013%2097.png)
    
- $P(D | H)$﻿ can be calculated using the multinomial. Think of the document as a dictionary of counts of words. What is the probability of getting that dictionary based on Hamilton’s probablities?
    
    - The probabilities of each word $h_i$﻿ are what we calculated previously, with the document containing all the words that Hamilton wrote.
    
    ![Untitled 14 85.png](attachments/Untitled%2014%2085.png)
    
- Since we want to compare Hamilton and Madison, we can just divide their probabilities.
    
    - If the ratio is greater than one, then we know that $P(H|D) > P(M>D)$﻿
    
    ![Untitled 15 81.png](attachments/Untitled%2015%2081.png)
    
    - In the example above, $P(M) == P(H)$﻿
- If we calculate the above using code, we see that the probabilities of both is 0!
    
    ![Untitled 16 75.png](attachments/Untitled%2016%2075.png)
    
    - This is because the computer doesn’t work that well with super small probabilities.

# Log Probabilities

![Untitled 17 70.png](attachments/Untitled%2017%2070.png)

- If the log is positive, its input was greater than 1. If the log is negative, it’s input was between 0 and 1.