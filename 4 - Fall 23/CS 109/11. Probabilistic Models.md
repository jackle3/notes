---
Date: 2023-10-20
---
# Discrete Probabilistic Models

- There often has to be multiple random variables interacting with each other for problems.

## Joint PMF

- Gives us the likelihood to one assignment, as well as the other assignment.

![[attachments/Untitled 182.png|Untitled 182.png]]

- Generally, the Joint PMF of two discrete RVs is represented in a table.

![[attachments/Untitled 1 145.png|Untitled 1 145.png]]

- Each cell is a joint probability. For example, there is a `0.10` probability that someone is **both** a junior **and** in a relationship.
- Every cell of the joint table is mutually exclusive, and they span the sample space.
    
    $\sum_{x\in X}\sum_{y \in Y}P(X=x, Y=y) = 1$
    
    where X is dating status and Y is year.
    

## Marginal Distribution

- Adding up the rows tells us the probability of the class year, $P(Y = \text{year})$﻿.
- Adding up the columns tells us the probabilty of that relationship status, $P(X =\text{relation})$﻿.
- This is called the marginal / marginalization. We have a joint table, and we marginalize all the other RVs to extract a single RV from the table. This is the **LOTP**!

![[attachments/Untitled 2 144.png|Untitled 2 144.png]]

## More RVs

- The joint table can get very big when there are more random variables. In the case of three, it requires two separate tables, for each disease status.

![[attachments/Untitled 3 141.png|Untitled 3 141.png]]

- In this case, there are a total of 12 entires. There are 2 assignments for $D$﻿, 2 assignments for $S$﻿, and three assignments for $F$﻿.
- The marginal is a double sum over all assignments of the remaining random variables.

![[attachments/Untitled 4 136.png|Untitled 4 136.png]]

## More efficient ways to represent multiple RVs

- If we were to roll 100 dice, there are 6 random variables that come out of it, for each $X_i$﻿.

![[attachments/Untitled 5 134.png|Untitled 5 134.png]]

- If we were to write this into a joint table, there are $100^6$﻿ entries in the table.

# Multinomial Random Variable

- Instead of thinking about coin flips with two outcomes, we think about dice rolls with $m$﻿ possible outcomes.

![[attachments/Untitled 6 132.png|Untitled 6 132.png]]

![[attachments/Untitled 7 128.png|Untitled 7 128.png]]

- $p_i^{c_i}$﻿ basically asks what are the chances of getting outcome $i$﻿, and how many times do we want to see outcome $i$﻿?
- Once we get all the probabilities, we also have to think about the number of ways to order all those outcomes, similar to how we thought about the binomial.

## Parameters of Multinomial RV

![[attachments/Untitled 8 119.png|Untitled 8 119.png]]

## Dice Example

![[attachments/Untitled 9 115.png|Untitled 9 115.png]]

- The term ${7 \choose {1, 1, 0, 2, 0, 3}}$﻿ is the same as $\frac{7!}{1!1!0!2!0!3!} = \frac{7!}{2!3!}$﻿

# Probabilistic text analysis

![[attachments/Untitled 10 110.png|Untitled 10 110.png]]

- In this multinomial, we’re basically still rolling dice $n$﻿ times. However now, our dice is weighted and has a side for every word in the english language.
    - It’s essentially rolling a $988,968$﻿-sided die, where not every outcome is equally likely.

## Modal text as a multinomial

![[attachments/Untitled 11 106.png|Untitled 11 106.png]]

- In the case above, $p_{\text{viagra}}$﻿ is the chance that a **spam** writer writes the word viagra.

# Federalist Papers Example

![[attachments/Untitled 12 103.png|Untitled 12 103.png]]

- We count up the probability of the author writing each word, and compare them.
    
    ![[attachments/Untitled 13 97.png|Untitled 13 97.png]]
    
- $P(D | H)$﻿ can be calculated using the multinomial. Think of the document as a dictionary of counts of words. What is the probability of getting that dictionary based on Hamilton’s probablities?
    
    - The probabilities of each word $h_i$﻿ are what we calculated previously, with the document containing all the words that Hamilton wrote.
    
    ![[attachments/Untitled 14 85.png|Untitled 14 85.png]]
    
- Since we want to compare Hamilton and Madison, we can just divide their probabilities.
    
    - If the ratio is greater than one, then we know that $P(H|D) > P(M>D)$﻿
    
    ![[attachments/Untitled 15 81.png|Untitled 15 81.png]]
    
    - In the example above, $P(M) == P(H)$﻿
- If we calculate the above using code, we see that the probabilities of both is 0!
    
    ![[attachments/Untitled 16 75.png|Untitled 16 75.png]]
    
    - This is because the computer doesn’t work that well with super small probabilities.

# Log Probabilities

![[attachments/Untitled 17 70.png|Untitled 17 70.png]]

- If the log is positive, its input was greater than 1. If the log is negative, it’s input was between 0 and 1.