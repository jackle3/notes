---
Date: 2023-11-08
---
- Bootstraping allows you to know the distribution of statistics and calculate p values using computers

# Standard error of the sample variance

- Suppose we got a sample variance: our estimate of the variance from one sample of 200 people.
- We want to know how much our sample variance would change if we used another sample.
    - We will find the standard deviation of the variance.
- The sample variance **is a random variable**
    - It has uncertainty, depending on what sample we use.

# Estimating the PMF

- If we create a histogram of our samples and normalize it, we can think of it as an estimate of the PMF of the underlying distribution.
    
    ![[attachments/Untitled 176.png|Untitled 176.png]]
    
- This comes from the fundamental definition of probability.
    
    - If we had 100 samples, and 7 samples had a value of 2, our best estimate for $P(X = 2) \approx 7/100$﻿.
    - If we extend this to all values, it states that the normalized histogram of the samples are a good estimate of the underlying PMF.
    
    ![[attachments/Untitled 1 139.png|Untitled 1 139.png]]
    

# Bootstrapping

- Allows us to estimate any statistic of a population from a sample.
    - This works for any statistics as long as your samples are IID and the underlying distribution doesn’t have a long tail

![[attachments/Untitled 2 138.png|Untitled 2 138.png]]

## Estimating the mean

- First, take the sample and make a histogram out of them. Assume that is the true PMF.
    
    ![[attachments/Untitled 3 135.png|Untitled 3 135.png]]
    
- Now repeat 10000 times: draw 200 samples from the PMF, calculate the mean of the sample, then save it.
    
    ![[attachments/Untitled 4 130.png|Untitled 4 130.png]]
    
- Then, you have a distribution of sample means.
    
    ![[attachments/Untitled 5 128.png|Untitled 5 128.png]]
    

# Code

- Since we have a histogram, to sample $K$﻿ new samples from our PMF, just simply choose $K$﻿ things with replacement from the histogram of samples.

![[attachments/Untitled 6 126.png|Untitled 6 126.png]]

- Because of that, in code, we can kind of cut out of the step of constructing the PMF.
    - Since our sample builds the PMF, pulling from the PMF is the same as pulling from the sample (in code).

![[attachments/Untitled 7 123.png|Untitled 7 123.png]]

![[attachments/Untitled 8 115.png|Untitled 8 115.png]]

![[attachments/Untitled 9 111.png|Untitled 9 111.png]]

# Null Hypothesis

- This hypothesis states that you have two different groups, but they are not actually any different.
    - Formally, there is no difference between the two groups, so everyone is drawn from the same distribution. Any difference you observe is due to sampling error.

## p-value

- **p-value**: what is the chance of seeing a difference this big, if my two groups were the same?
- Basically, we have to
    - First, imagine the null hypothesis, meaning both groups are sampled from the same distribution.
    - This means that in the world of the null hypothesis, the estimated true PMF is the concatenation of samples from both groups.

![[attachments/Untitled 10 106.png|Untitled 10 106.png]]

- During bootstrapping, we do the same thing. We get two groups of samples the same size as `pop1` and `pop2`, and then take the mean of the two groups.

![[attachments/Untitled 11 102.png|Untitled 11 102.png]]

- Using this, we can create a full distribution of the statistic (in this case difference in means) that we can reason about

![[attachments/Untitled 12 99.png|Untitled 12 99.png]]

- Under the null hypothesis, it’s very likely that the difference in means is zero (because they came from the same distribution).
- We can also calculate the p-value using our distribution.

![[attachments/Untitled 13 93.png|Untitled 13 93.png]]

![[attachments/Untitled 14 81.png|Untitled 14 81.png]]

- The probability that, under the null hypothesis, you would’ve seen a 0.7 difference in means or larger is only 0.008.