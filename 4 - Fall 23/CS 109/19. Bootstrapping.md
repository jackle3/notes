---
Date: 2023-11-08
---
- Bootstraping allows you to know the distribution of statistics and calculate p values using computers

# Standard error of the sample variance

- Suppose we got a sample variance: our estimate of the variance from one sample of 200 people.
- We want to know how much our sample variance would change if we used another sample.
    - We will find the standard deviation of the variance.
- The sample variance **is a random variable**
    - It has uncertainty, depending on what sample we use.

# Estimating the PMF

- If we create a histogram of our samples and normalize it, we can think of it as an estimate of the PMF of the underlying distribution.
    
    ![Untitled 176.png](../../attachments/Untitled%20176.png)
    
- This comes from the fundamental definition of probability.
    
    - If we had 100 samples, and 7 samples had a value of 2, our best estimate for $P(X = 2) \approx 7/100$﻿.
    - If we extend this to all values, it states that the normalized histogram of the samples are a good estimate of the underlying PMF.
    
    ![Untitled 1 139.png](../../attachments/Untitled%201%20139.png)
    

# Bootstrapping

- Allows us to estimate any statistic of a population from a sample.
    - This works for any statistics as long as your samples are IID and the underlying distribution doesn’t have a long tail

![Untitled 2 138.png](../../attachments/Untitled%202%20138.png)

## Estimating the mean

- First, take the sample and make a histogram out of them. Assume that is the true PMF.
    
    ![Untitled 3 135.png](../../attachments/Untitled%203%20135.png)
    
- Now repeat 10000 times: draw 200 samples from the PMF, calculate the mean of the sample, then save it.
    
    ![Untitled 4 130.png](../../attachments/Untitled%204%20130.png)
    
- Then, you have a distribution of sample means.
    
    ![Untitled 5 128.png](../../attachments/Untitled%205%20128.png)
    

# Code

- Since we have a histogram, to sample $K$﻿ new samples from our PMF, just simply choose $K$﻿ things with replacement from the histogram of samples.

![Untitled 6 126.png](../../attachments/Untitled%206%20126.png)

- Because of that, in code, we can kind of cut out of the step of constructing the PMF.
    - Since our sample builds the PMF, pulling from the PMF is the same as pulling from the sample (in code).

![Untitled 7 123.png](../../attachments/Untitled%207%20123.png)

![Untitled 8 115.png](../../attachments/Untitled%208%20115.png)

![Untitled 9 111.png](../../attachments/Untitled%209%20111.png)

# Null Hypothesis

- This hypothesis states that you have two different groups, but they are not actually any different.
    - Formally, there is no difference between the two groups, so everyone is drawn from the same distribution. Any difference you observe is due to sampling error.

## p-value

- **p-value**: what is the chance of seeing a difference this big, if my two groups were the same?
- Basically, we have to
    - First, imagine the null hypothesis, meaning both groups are sampled from the same distribution.
    - This means that in the world of the null hypothesis, the estimated true PMF is the concatenation of samples from both groups.

![Untitled 10 106.png](../../attachments/Untitled%2010%20106.png)

- During bootstrapping, we do the same thing. We get two groups of samples the same size as `pop1` and `pop2`, and then take the mean of the two groups.

![Untitled 11 102.png](../../attachments/Untitled%2011%20102.png)

- Using this, we can create a full distribution of the statistic (in this case difference in means) that we can reason about

![Untitled 12 99.png](../../attachments/Untitled%2012%2099.png)

- Under the null hypothesis, it’s very likely that the difference in means is zero (because they came from the same distribution).
- We can also calculate the p-value using our distribution.

![Untitled 13 93.png](../../attachments/Untitled%2013%2093.png)

![Untitled 14 81.png](../../attachments/Untitled%2014%2081.png)

- The probability that, under the null hypothesis, you would’ve seen a 0.7 difference in means or larger is only 0.008.