---
Date: 2023-11-10
---
# Review of Expectation

- The fundamental definition of expected value for a discrete RV is
    
    ![[attachments/Untitled 175.png|Untitled 175.png]]
    
- There is the law of **Linearity of expectation**
    
    ![[attachments/Untitled 1 138.png|Untitled 1 138.png]]
    
- There’s also the law of the unconcious statisician
    
    ![[attachments/Untitled 2 137.png|Untitled 2 137.png]]
    

## Expectation of Binomial

- Recall that a binomial is a sum of $n$﻿ Bernoulli RVs

![[attachments/Untitled 3 134.png|Untitled 3 134.png]]

## Expectation of Negative Binomial

- Recall that a negative binomial is like a sum of Geometric RVs

![[attachments/Untitled 4 129.png|Untitled 4 129.png]]

# Computer Cluster Utiliziation

![[attachments/Untitled 5 127.png|Untitled 5 127.png]]

- In the case above, $X$﻿ is the total number of servers that are idle (have no requests)
- We want to know the number of servers that are not idle: $Y = k - X$﻿
    - $k$﻿ is the total number of servers, $X$﻿ is the number of servers that are idle
- The probability that a server is idle $P(A_i)$﻿ is simly $(1 - p_i)^n$﻿, meaning none of the $n$﻿ requests went to server $i$﻿
- Note that $B_i \sim \text{Ber}(p = P(A_i))$﻿, meaning it’s 1 if server $i$﻿ is idle and $0$﻿ otherwise.

# Hash Functions Problem

![[attachments/Untitled 6 125.png|Untitled 6 125.png]]

- We can think of the process of filling up each bucket as a random variable, where $X_i$﻿ is the number of trials since the last time an empty bucket was filled until we fill up a bucket.

![[attachments/Untitled 7 122.png|Untitled 7 122.png]]

- In other words, $X_i$﻿ is a geometric RV. Also notice that $X = \sum X_i$﻿, since each $X_i$﻿ indicates one of the buckets being filled.

![[attachments/Untitled 8 114.png|Untitled 8 114.png]]

- In the above, it should be $i = 2$﻿

![[attachments/Untitled 9 110.png|Untitled 9 110.png]]

# Conditional Expectation

![[attachments/Untitled 10 105.png|Untitled 10 105.png]]

## Dice Example

![[attachments/Untitled 11 101.png|Untitled 11 101.png]]

## Function

![[attachments/Untitled 12 98.png|Untitled 12 98.png]]

# Law of Total Expectation

- Notice that $E[X | Y]$﻿ is a function of $Y$﻿. We can further take an expectation of this.

![[attachments/Untitled 13 92.png|Untitled 13 92.png]]

![[attachments/Untitled 14 80.png|Untitled 14 80.png]]

- For the first step below, it’s the law of the unconcious statistician.
    - $E[g(y)] = \sum_y g(y) \times P(Y = y)$﻿, where $g(y) = E[X|Y]$﻿
- Similar to the law of total probability, we also have the law of total expectation.

![[attachments/Untitled 15 77.png|Untitled 15 77.png]]

## Expected runtime

- In the example below, we just simply use the law of total expectation to solve it.
    
    ![[attachments/Untitled 16 72.png|Untitled 16 72.png]]
    
- Now, if we had a cache, we can again use the same thing, but now reuse our old value
    
    ![[attachments/Untitled 17 67.png|Untitled 17 67.png]]
    
- Notice here that we can just reuse the values that we calculated from before!
    
    ![[attachments/Untitled 18 61.png|Untitled 18 61.png]]
    
    - Also notice that the approximate distribution is normal by the CLT

# Recursive analysis

- When you see an `if-else` in your code, use the law of total expectation!

![[attachments/Untitled 19 54.png|Untitled 19 54.png]]