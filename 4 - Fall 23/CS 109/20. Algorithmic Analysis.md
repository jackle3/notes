---
Date: 2023-11-10
---
# Review of Expectation

- The fundamental definition of expected value for a discrete RV is
    
    ![Untitled 175.png](attachments/Untitled%20175.png)
    
- There is the law of **Linearity of expectation**
    
    ![Untitled 1 138.png](attachments/Untitled%201%20138.png)
    
- There’s also the law of the unconcious statisician
    
    ![Untitled 2 137.png](attachments/Untitled%202%20137.png)
    

## Expectation of Binomial

- Recall that a binomial is a sum of $n$﻿ Bernoulli RVs

![Untitled 3 134.png](attachments/Untitled%203%20134.png)

## Expectation of Negative Binomial

- Recall that a negative binomial is like a sum of Geometric RVs

![Untitled 4 129.png](attachments/Untitled%204%20129.png)

# Computer Cluster Utiliziation

![Untitled 5 127.png](attachments/Untitled%205%20127.png)

- In the case above, $X$﻿ is the total number of servers that are idle (have no requests)
- We want to know the number of servers that are not idle: $Y = k - X$﻿
    - $k$﻿ is the total number of servers, $X$﻿ is the number of servers that are idle
- The probability that a server is idle $P(A_i)$﻿ is simly $(1 - p_i)^n$﻿, meaning none of the $n$﻿ requests went to server $i$﻿
- Note that $B_i \sim \text{Ber}(p = P(A_i))$﻿, meaning it’s 1 if server $i$﻿ is idle and $0$﻿ otherwise.

# Hash Functions Problem

![Untitled 6 125.png](attachments/Untitled%206%20125.png)

- We can think of the process of filling up each bucket as a random variable, where $X_i$﻿ is the number of trials since the last time an empty bucket was filled until we fill up a bucket.

![Untitled 7 122.png](attachments/Untitled%207%20122.png)

- In other words, $X_i$﻿ is a geometric RV. Also notice that $X = \sum X_i$﻿, since each $X_i$﻿ indicates one of the buckets being filled.

![Untitled 8 114.png](attachments/Untitled%208%20114.png)

- In the above, it should be $i = 2$﻿

![Untitled 9 110.png](attachments/Untitled%209%20110.png)

# Conditional Expectation

![Untitled 10 105.png](attachments/Untitled%2010%20105.png)

## Dice Example

![Untitled 11 101.png](attachments/Untitled%2011%20101.png)

## Function

![Untitled 12 98.png](attachments/Untitled%2012%2098.png)

# Law of Total Expectation

- Notice that $E[X | Y]$﻿ is a function of $Y$﻿. We can further take an expectation of this.

![Untitled 13 92.png](attachments/Untitled%2013%2092.png)

![Untitled 14 80.png](attachments/Untitled%2014%2080.png)

- For the first step below, it’s the law of the unconcious statistician.
    - $E[g(y)] = \sum_y g(y) \times P(Y = y)$﻿, where $g(y) = E[X|Y]$﻿
- Similar to the law of total probability, we also have the law of total expectation.

![Untitled 15 77.png](attachments/Untitled%2015%2077.png)

## Expected runtime

- In the example below, we just simply use the law of total expectation to solve it.
    
    ![Untitled 16 72.png](attachments/Untitled%2016%2072.png)
    
- Now, if we had a cache, we can again use the same thing, but now reuse our old value
    
    ![Untitled 17 67.png](attachments/Untitled%2017%2067.png)
    
- Notice here that we can just reuse the values that we calculated from before!
    
    ![Untitled 18 61.png](attachments/Untitled%2018%2061.png)
    
    - Also notice that the approximate distribution is normal by the CLT

# Recursive analysis

- When you see an `if-else` in your code, use the law of total expectation!

![Untitled 19 54.png](attachments/Untitled%2019%2054.png)