---
Date: 2023-11-01
---
# Definition

- This is the random variable for probabilities → gives uncertainty to the probability of some even
- Any parameter for a “parameterized” random variable can be thought of as a random variable
    - The beta is for the parameter $p$﻿ of the Binomial RV
- It can be used when you can **run trials and see whether you succeed or fail**
    - We can use beta to find a distribution on what the probability of a success was.
    - Basically, the parameter **p** to a binomial can be a random variable.

# Example

- Let $X$﻿ be the probability of flipping a heads, given that we flipped 9 heads and 1 tail.
    
    $f(X = x | H=9, T=1)$
    
    where $0 \leq x \leq 1$﻿$0 < x < 1$﻿, meaning $x$﻿ is a continous random variable for the probability.
    
- By Bayes Theorem, the above equation becomes
    
    $P(H=9, T=1 | X=x) f(X = x) \times K$
    
- If we have no information about $X$﻿, a pretty good prior belief is to say that it’s uniform.
    - All probabilities are equally likely. It’s also nice because no matter for the value of $x$﻿, the result of the PDF is just 1 in this case: $1 / (b - a) = 1 / ( 1 - 0) = 1$﻿

![[attachments/Untitled 185.png|Untitled 185.png]]

- In addition, the left term is just a simple binomial problem. It asks that with probablity $x$﻿ of getting heads, what is probability of getting $9$﻿ heads in $10$﻿ total trials.

![[attachments/Untitled 1 148.png|Untitled 1 148.png]]

## Generalized Example

![[attachments/Untitled 2 147.png|Untitled 2 147.png]]

- $c$﻿ is whatever number ends up making the equation integrate to 1, to make it a valid pdf.

![[attachments/Untitled 3 144.png|Untitled 3 144.png]]

![[attachments/Untitled 4 139.png|Untitled 4 139.png]]

# Definition, Expectation, Variance

![[attachments/Untitled 5 137.png|Untitled 5 137.png]]

# Prior was Beta

- Remember that $X$﻿ was the random variable for the probability.
- It is very possible that our prior belief about $X$﻿ was a beta random variable.
    
    ![[attachments/Untitled 6 135.png|Untitled 6 135.png]]
    
- If the prior was a beta, the result is still a beta with different parameters.
    
    ![[attachments/Untitled 7 131.png|Untitled 7 131.png]]
    

## Beta(1, 1) is uniform

![[attachments/Untitled 8 122.png|Untitled 8 122.png]]

## Laplace smoothing

![[attachments/Untitled 9 118.png|Untitled 9 118.png]]

- Beta(a, b) implies a - 1 successes and b - 1 failures when we are assuming a uniform prior to
    - If we instead had a different prior of say Beta(2, 1), that would imply a - 2 successes and b - 1 failures

# Another example

![[attachments/Untitled 10 113.png|Untitled 10 113.png]]

- In this case, we can have multiple priors for the working 80% of the time.
    - The first prior is very confident.
    - The second is medium confidence.
    - The last one is minimal confidence.

![[attachments/Untitled 11 109.png|Untitled 11 109.png]]

# Thompson Sampling

![[attachments/Untitled 12 106.png|Untitled 12 106.png]]