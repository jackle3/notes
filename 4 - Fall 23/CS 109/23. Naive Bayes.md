---
Date: 2023-11-17
---
# Review

- In MLE, we set the sliders (parameters) to make the data as likely as possible.
- In MAP, we choose the most likely sliders (parameters) given the data we see.
    - MAP incorporates a prior, indicating how likely the parameter value is before we see any data.

![Untitled 183.png](attachments/Untitled%20183.png)

# Training Data

- Suppose we have the following training data

    - IID samples mean that user 1 is independent of user 2. It does not mean that each datapoint is independent of one another.

    ![Untitled 1 146.png](attachments/Untitled%201%20146.png)

- We basically want to classify whether a user will like a movie, given the movies that they previously liked.

    ![Untitled 2 145.png](attachments/Untitled%202%20145.png)

- The feature vector is the $x^{(i)}$﻿. Notice that it is bold, meaning it is a **vector**

    ![Untitled 3 142.png](attachments/Untitled%203%20142.png)

- If we wanted to refer to a single feature value, we can use $x_j^{(i)}$﻿

    - The subscript is the index of the feature value, the superscript is which feature vector we are referring to

    ![Untitled 4 137.png](attachments/Untitled%204%20137.png)

- The output value is $y^{(i)}$﻿, which can be interpreted as the classification output

# Classification

- Use training data with feature/label pairs $(\mathbf{x}, y)$﻿ in order to estimate a function $\hat{y} = g(\mathbf{x})$﻿ that can be used to make a prediction.
- The value of $y$﻿ can take on a discrete number of values. As such, we often chose

$$
\hat{y} = g(\mathbf{x}) =\arg \max_y \hat{P}(Y = y | \mathbf{x})
$$

- In other words, it chooses the most likely label given the data (feature vector).

![Untitled 5 135.png](attachments/Untitled%205%20135.png)

- The parameters we want to find are the probabilities in the conditional probability table. In other words, it’s the possible values for $P(x | y)$﻿ and $P(y)$﻿.

    ![Untitled 6 133.png](attachments/Untitled%206%20133.png)

    ![Untitled 7 129.png](attachments/Untitled%207%20129.png)

- In this case, $P(y)$﻿ has two parameters, and $P(x | y)$﻿ has $2 \times 2 = 4$﻿ parameters, so there are 6 total parameters.

## Training: Estimating the Parameters

![Untitled 8 120.png](attachments/Untitled%208%20120.png)

- In training, to find these parameters, we can just use counting!
- In MLE, we would just do pure counting to find our probabilities.

    ![Untitled 9 116.png](attachments/Untitled%209%20116.png)

- In MAP, we would count and then add imaginary trials to prevent overfitting.

    ![Untitled 10 111.png](attachments/Untitled%2010%20111.png)


## Testing

- To find what value to classify the test user, pick the one with the higher likelihood

    - To calculate likelihood, just use our estimates of the conditional probability tables and multiply them together

    ![Untitled 11 107.png](attachments/Untitled%2011%20107.png)

    - In this case, $y=1$﻿ maximizes the probability, so pick that
    - Notice that it does not add up to 1, because we removed the normalization constant.

## Increasing the Feature Vector Size

- If $m = 2$﻿, we now have two feature values. Therefore, our probability becomes

    $P(\mathbf{x} | y) = P(x_1, x_2|y)$

    - In this case, $P(y)$﻿ still has two parameters, but $P(x_1, x_2 | y)$﻿ now has $2^3 = 8$﻿ parameters
        - There are:
            - Two possible values for $x_1$﻿
            - Two possible values for $x_2$﻿
            - Two possible values for $y$﻿
        - So there are $8$﻿ total probabilities in the conditional probability table.
- If $m = 3$﻿, the joint probability becomes $P(\mathbf{x} | y) = P(x_1, x_2, x_3|y)$﻿
    - The term $P(x_1, x_2, x_3|y)$﻿ has $2^4 = 16$﻿ parameters now
- As $m$﻿ increases, meaning there are more feature values, the number of parameters grow exponentially.

    ![Untitled 12 104.png](attachments/Untitled%2012%20104.png)


# Naive Bayes

- To reduce the number of parameters, NB assumes that every feature (datapoint) $x_i$﻿ are conditionally independent given the label $y$﻿
    - This assumption is wrong, but makes the algorithm much faster.

![Untitled 13 98.png](attachments/Untitled%2013%2098.png)

## Bayesian Network

- Notice that every feature $x_i$﻿ is independent of one another conditioned on $y$﻿

![Untitled 14 86.png](attachments/Untitled%2014%2086.png)

## Classification

![Untitled 15 82.png](attachments/Untitled%2015%2082.png)

## Training (aka Parameter Estimation)

- Due to the NB assumption, training becomes very easy because of counting

![Untitled 16 76.png](attachments/Untitled%2016%2076.png)

- Using an MLE estimate:

    ![Untitled 17 71.png](attachments/Untitled%2017%2071.png)

- Using MAP with Laplace smoothing:

    - Remember that Laplace is just the prior belief that we have seen one example of $Y = 1$﻿ and one example of $Y = 0$﻿

    ![Untitled 18 64.png](attachments/Untitled%2018%2064.png)


## Finding the Normalization Constant

- Since we know that probabilities should add up to 1, we can use that idea to find $K$﻿
- Suppose $P(Y = 1) = K \cdot 0.178$﻿ and $P(Y = 0) = K \cdot 0.126$﻿.
- We know that $P(Y = 1) + P(Y = 0) = 1$﻿ by the LOTP
- Therefore, we know that $K = 1 / (0.178 + 0.126)$﻿

## Movie Example

![Untitled 19 57.png](attachments/Untitled%2019%2057.png)

![Untitled 20 55.png](attachments/Untitled%2020%2055.png)

# Summary

- Training Naive Bayes is just estimating the parameters for a multinomial, binomial, or a bernoulli. Thus, training is just counting

## Assumption

![Untitled 21 49.png](attachments/Untitled%2021%2049.png)

## MLE Estimate

![Untitled 22 45.png](attachments/Untitled%2022%2045.png)

## MAP Estimate

![Untitled 23 42.png](attachments/Untitled%2023%2042.png)

## Prediction

![Untitled 24 38.png](attachments/Untitled%2024%2038.png)
