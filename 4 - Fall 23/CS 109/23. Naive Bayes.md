---
Date: 2023-11-17
---
# Review

- In MLE, we set the sliders (parameters) to make the data as likely as possible.
- In MAP, we choose the most likely sliders (parameters) given the data we see.
    - MAP incorporates a prior, indicating how likely the parameter value is before we see any data.

![[attachments/Untitled 183.png|Untitled 183.png]]

# Training Data

- Suppose we have the following training data
    
    - IID samples mean that user 1 is independent of user 2. It does not mean that each datapoint is independent of one another.
    
    ![[attachments/Untitled 1 146.png|Untitled 1 146.png]]
    
- We basically want to classify whether a user will like a movie, given the movies that they previously liked.
    
    ![[attachments/Untitled 2 145.png|Untitled 2 145.png]]
    
- The feature vector is the $x^{(i)}$﻿. Notice that it is bold, meaning it is a **vector**
    
    ![[attachments/Untitled 3 142.png|Untitled 3 142.png]]
    
- If we wanted to refer to a single feature value, we can use $x_j^{(i)}$﻿
    
    - The subscript is the index of the feature value, the superscript is which feature vector we are referring to
    
    ![[attachments/Untitled 4 137.png|Untitled 4 137.png]]
    
- The output value is $y^{(i)}$﻿, which can be interpreted as the classification output

# Classification

- Use training data with feature/label pairs $(\mathbf{x}, y)$﻿ in order to estimate a function $\hat{y} = g(\mathbf{x})$﻿ that can be used to make a prediction.
- The value of $y$﻿ can take on a discrete number of values. As such, we often chose

$\hat{y} = g(\mathbf{x}) =\argmax_y \hat{P}(Y = y | \mathbf{x})$

- In other words, it chooses the most likely label given the data (feature vector).

![[attachments/Untitled 5 135.png|Untitled 5 135.png]]

- The parameters we want to find are the probabilities in the conditional probability table. In other words, it’s the possible values for $P(x | y)$﻿ and $P(y)$﻿.
    
    ![[attachments/Untitled 6 133.png|Untitled 6 133.png]]
    
    ![[attachments/Untitled 7 129.png|Untitled 7 129.png]]
    
- In this case, $P(y)$﻿ has two parameters, and $P(x | y)$﻿ has $2 \times 2 = 4$﻿ parameters, so there are 6 total parameters.

## Training: Estimating the Parameters

![[attachments/Untitled 8 120.png|Untitled 8 120.png]]

- In training, to find these parameters, we can just use counting!
- In MLE, we would just do pure counting to find our probabilities.
    
    ![[attachments/Untitled 9 116.png|Untitled 9 116.png]]
    
- In MAP, we would count and then add imaginary trials to prevent overfitting.
    
    ![[attachments/Untitled 10 111.png|Untitled 10 111.png]]
    

## Testing

- To find what value to classify the test user, pick the one with the higher likelihood
    
    - To calculate likelihood, just use our estimates of the conditional probability tables and multiply them together
    
    ![[attachments/Untitled 11 107.png|Untitled 11 107.png]]
    
    - In this case, $y=1$﻿ maximizes the probability, so pick that
    - Notice that it does not add up to 1, because we removed the normalization constant.

## Increasing the feature vector size

- If $m = 2$﻿, we now have two feature values. Therefore, our probability becomes
    
    $P(\mathbf{x} | y) = P(x_1, x_2|y)$
    
    - In this case, $P(y)$﻿ still has two parameters, but $P(x_1, x_2 | y)$﻿ now has $2^3 = 8$﻿ parameters
        - There are:
            - Two possible values for $x_1$﻿
            - Two possible values for $x_2$﻿
            - Two possible values for $y$﻿
        - So there are $8$﻿ total probabilities in the conditional probability table.
- If $m = 3$﻿, the joint probability becomes $P(\mathbf{x} | y) = P(x_1, x_2, x_3|y)$﻿
    - The term $P(x_1, x_2, x_3|y)$﻿ has $2^4 = 16$﻿ parameters now
- As $m$﻿ increases, meaning there are more feature values, the number of parameters grow exponentially.
    
    ![[attachments/Untitled 12 104.png|Untitled 12 104.png]]
    

# Naive Bayes

- To reduce the number of parameters, NB assumes that every feature (datapoint) $x_i$﻿ are conditionally independent given the label $y$﻿
    - This assumption is wrong, but makes the algorithm much faster.

![[attachments/Untitled 13 98.png|Untitled 13 98.png]]

## Bayesian Network

- Notice that every feature $x_i$﻿ is independent of one another conditioned on $y$﻿

![[attachments/Untitled 14 86.png|Untitled 14 86.png]]

## Classification

![[attachments/Untitled 15 82.png|Untitled 15 82.png]]

## Training (aka Parameter Estimation)

- Due to the NB assumption, training becomes very easy because of counting

![[attachments/Untitled 16 76.png|Untitled 16 76.png]]

- Using an MLE estimate:
    
    ![[attachments/Untitled 17 71.png|Untitled 17 71.png]]
    
- Using MAP with Laplace smoothing:
    
    - Remember that Laplace is just the prior belief that we have seen one example of $Y = 1$﻿ and one example of $Y = 0$﻿
    
    ![[attachments/Untitled 18 64.png|Untitled 18 64.png]]
    

## Finding the Normalization Constant

- Since we know that probabilities should add up to 1, we can use that idea to find $K$﻿
- Suppose $P(Y = 1) = K \cdot 0.178$﻿ and $P(Y = 0) = K \cdot 0.126$﻿.
- We know that $P(Y = 1) + P(Y = 0) = 1$﻿ by the LOTP
- Therefore, we know that $K = 1 / (0.178 + 0.126)$﻿

## Movie Example

![[attachments/Untitled 19 57.png|Untitled 19 57.png]]

![[attachments/Untitled 20 55.png|Untitled 20 55.png]]

# Summary

- Training Naive Bayes is just estimating the parameters for a multinomial, binomial, or a bernoulli. Thus, training is just counting

## Assumption

![[attachments/Untitled 21 49.png|Untitled 21 49.png]]

## MLE Estimate

![[attachments/Untitled 22 45.png|Untitled 22 45.png]]

## MAP Estimate

![[attachments/Untitled 23 42.png|Untitled 23 42.png]]

## Prediction

![[attachments/Untitled 24 38.png|Untitled 24 38.png]]