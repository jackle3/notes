---
Date: 2023-11-13
---
# Parameters

- Numbers that tell you how the distribution is structured.

![Untitled 177.png](../../attachments/Untitled%20177.png)

- Models that take paramteres are called **parametric models**
    - Given a model, parameters yield actual distributions.
    - Usually refer to parameters of distributions as $\theta$﻿ → can be a vector

# Steps of Machine Learning

1. Modeling
2. Parameter estimation (training)

![Untitled 1 140.png](../../attachments/Untitled%201%20140.png)

# Unbiased Estimators

- Estimates where the expected value of our estimates should be equal to the true values we are trying to estimate

![Untitled 2 139.png](../../attachments/Untitled%202%20139.png)

![Untitled 3 136.png](../../attachments/Untitled%203%20136.png)

# Likelihood

- If we are given a series of data and we have to fit a Gaussian distribution to it, we can just iterate our parameters to find the most likely one.
- **Find the arguments that maximize the measure of likelihood**
    - This is $\argmax$﻿!

![Untitled 4 131.png](../../attachments/Untitled%204%20131.png)

- Therefore, we should **take the** $\argmax$﻿ of the log-likelihood

## Expression

![Untitled 5 129.png](../../attachments/Untitled%205%20129.png)

- In this case, $f$﻿ is either the PDF (continuous), the PMF (discrete), or the joint, where you plug in $x_i$﻿ with the parameter $\theta$﻿ for the equation.

## Log Likelihood

- Notice that when we take the log of the likelihood function, the product becomes a summation of logs.

![Untitled 6 127.png](../../attachments/Untitled%206%20127.png)

## MLE Summary

![Untitled 7 124.png](../../attachments/Untitled%207%20124.png)

# Optimization Algorithms

- There are a few ways to calculate the $\argmax$﻿ of the log likelihood function.

## Calculus

- We can use straight optimization with critical points.

![Untitled 8 116.png](../../attachments/Untitled%208%20116.png)

## Gradient Ascent

- You essentially create a graph between $\theta$﻿ and the likelihood of the samples with those parameters $\theta$﻿
    
    ![Untitled 9 112.png](../../attachments/Untitled%209%20112.png)
    
- Then, you traverse that graph in the direction of maximum increase.
    
    ![Untitled 10 107.png](../../attachments/Untitled%2010%20107.png)
    
- Then, repeat many times
    
    ![Untitled 11 103.png](../../attachments/Untitled%2011%20103.png)
    
    - This is the general algorithm.
    
    ![Untitled 12 100.png](../../attachments/Untitled%2012%20100.png)
    
- Note that this is the same as **gradient descent**
    - Use the $\argmin$﻿ of the negative log likelihood, which would allow you to use any general gradient descent algorithm.

# General MLE Formula

![Untitled 13 94.png](../../attachments/Untitled%2013%2094.png)

## Poisson Example

![Untitled 14 82.png](../../attachments/Untitled%2014%2082.png)

![Untitled 15 78.png](../../attachments/Untitled%2015%2078.png)

1. What is the likelihood of one $X_i$﻿
    
    ![Untitled 16 73.png](../../attachments/Untitled%2016%2073.png)
    
2. What is the likelihood of all the data?
    
    ![Untitled 17 68.png](../../attachments/Untitled%2017%2068.png)
    
3. What is the log-likelihood of all the data?
    
    ![Untitled 18 62.png](../../attachments/Untitled%2018%2062.png)
    
4. Find the value of $\lambda$﻿ to maximize the log likelihood
    
    ![Untitled 19 55.png](../../attachments/Untitled%2019%2055.png)
    
    - For Poisson, the MLE is the sample mean.

## Bernoulli Example

![Untitled 20 53.png](../../attachments/Untitled%2020%2053.png)

1. What is the likelihood of one $X_i$﻿?
    1. This is the PMF of the Bernoulli
        
        ![Untitled 21 48.png](../../attachments/Untitled%2021%2048.png)
        
    2. To make the function above derivable, we make it continuous with the one below, where $x$﻿ is either $0$﻿ or $1$﻿.
        
        ![Untitled 22 44.png](../../attachments/Untitled%2022%2044.png)
        
2. What is the likelihood of all the data?
    
    ![Untitled 23 41.png](../../attachments/Untitled%2023%2041.png)
    
3. What is the log-likelihood of all the data?
    
    ![Untitled 24 37.png](../../attachments/Untitled%2024%2037.png)
    
4. Find the value of $p$﻿ which maximizes the log likelihood
    
    ![Untitled 25 33.png](../../attachments/Untitled%2025%2033.png)
    
    - For Bernoulli, the MLE is also the sample mean.
    
    ## Gaussian Example
    
    ![Untitled 26 28.png](../../attachments/Untitled%2026%2028.png)
    
    1. What is the likelihood of a single $X_i$﻿
        - This is just the PDF of the gaussian
            
            ![Untitled 27 25.png](../../attachments/Untitled%2027%2025.png)
            
    2. What is the log-likelihood of all the data?
        
        ![Untitled 28 23.png](../../attachments/Untitled%2028%2023.png)
        
    3. Find the parameters that maximize the log-likelihood
        1. To do this, derive it w.r.t each of the parameters
            
            ![Untitled 29 22.png](../../attachments/Untitled%2029%2022.png)
            
            ![Untitled 30 22.png](../../attachments/Untitled%2030%2022.png)
            
        2. Then, solve the system of equations. You have two unknowns and two equations.
            
            ![Untitled 31 18.png](../../attachments/Untitled%2031%2018.png)
            
            ![Untitled 32 18.png](../../attachments/Untitled%2032%2018.png)
            

# Properties of MLE

![Untitled 33 17.png](../../attachments/Untitled%2033%2017.png)

## Small Samples and bias

![Untitled 34 13.png](../../attachments/Untitled%2034%2013.png)