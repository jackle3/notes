---
Date: 2023-11-13
---
# Parameters

- Numbers that tell you how the distribution is structured.

![[attachments/Untitled 177.png|Untitled 177.png]]

- Models that take paramteres are called **parametric models**
    - Given a model, parameters yield actual distributions.
    - Usually refer to parameters of distributions as $\theta$﻿ → can be a vector

# Steps of Machine Learning

1. Modeling
2. Parameter estimation (training)

![[attachments/Untitled 1 140.png|Untitled 1 140.png]]

# Unbiased Estimators

- Estimates where the expected value of our estimates should be equal to the true values we are trying to estimate

![[attachments/Untitled 2 139.png|Untitled 2 139.png]]

![[attachments/Untitled 3 136.png|Untitled 3 136.png]]

# Likelihood

- If we are given a series of data and we have to fit a Gaussian distribution to it, we can just iterate our parameters to find the most likely one.
- **Find the arguments that maximize the measure of likelihood**
    - This is $\argmax$﻿!

![[attachments/Untitled 4 131.png|Untitled 4 131.png]]

- Therefore, we should **take the** $\argmax$﻿ of the log-likelihood

## Expression

![[attachments/Untitled 5 129.png|Untitled 5 129.png]]

- In this case, $f$﻿ is either the PDF (continuous), the PMF (discrete), or the joint, where you plug in $x_i$﻿ with the parameter $\theta$﻿ for the equation.

## Log Likelihood

- Notice that when we take the log of the likelihood function, the product becomes a summation of logs.

![[attachments/Untitled 6 127.png|Untitled 6 127.png]]

## MLE Summary

![[attachments/Untitled 7 124.png|Untitled 7 124.png]]

# Optimization Algorithms

- There are a few ways to calculate the $\argmax$﻿ of the log likelihood function.

## Calculus

- We can use straight optimization with critical points.

![[attachments/Untitled 8 116.png|Untitled 8 116.png]]

## Gradient Ascent

- You essentially create a graph between $\theta$﻿ and the likelihood of the samples with those parameters $\theta$﻿
    
    ![[attachments/Untitled 9 112.png|Untitled 9 112.png]]
    
- Then, you traverse that graph in the direction of maximum increase.
    
    ![[attachments/Untitled 10 107.png|Untitled 10 107.png]]
    
- Then, repeat many times
    
    ![[attachments/Untitled 11 103.png|Untitled 11 103.png]]
    
    - This is the general algorithm.
    
    ![[attachments/Untitled 12 100.png|Untitled 12 100.png]]
    
- Note that this is the same as **gradient descent**
    - Use the $\argmin$﻿ of the negative log likelihood, which would allow you to use any general gradient descent algorithm.

# General MLE Formula

![[attachments/Untitled 13 94.png|Untitled 13 94.png]]

## Poisson Example

![[attachments/Untitled 14 82.png|Untitled 14 82.png]]

![[attachments/Untitled 15 78.png|Untitled 15 78.png]]

1. What is the likelihood of one $X_i$﻿
    
    ![[attachments/Untitled 16 73.png|Untitled 16 73.png]]
    
2. What is the likelihood of all the data?
    
    ![[attachments/Untitled 17 68.png|Untitled 17 68.png]]
    
3. What is the log-likelihood of all the data?
    
    ![[attachments/Untitled 18 62.png|Untitled 18 62.png]]
    
4. Find the value of $\lambda$﻿ to maximize the log likelihood
    
    ![[attachments/Untitled 19 55.png|Untitled 19 55.png]]
    
    - For Poisson, the MLE is the sample mean.

## Bernoulli Example

![[attachments/Untitled 20 53.png|Untitled 20 53.png]]

1. What is the likelihood of one $X_i$﻿?
    1. This is the PMF of the Bernoulli
        
        ![[attachments/Untitled 21 48.png|Untitled 21 48.png]]
        
    2. To make the function above derivable, we make it continuous with the one below, where $x$﻿ is either $0$﻿ or $1$﻿.
        
        ![[attachments/Untitled 22 44.png|Untitled 22 44.png]]
        
2. What is the likelihood of all the data?
    
    ![[attachments/Untitled 23 41.png|Untitled 23 41.png]]
    
3. What is the log-likelihood of all the data?
    
    ![[attachments/Untitled 24 37.png|Untitled 24 37.png]]
    
4. Find the value of $p$﻿ which maximizes the log likelihood
    
    ![[attachments/Untitled 25 33.png|Untitled 25 33.png]]
    
    - For Bernoulli, the MLE is also the sample mean.
    
    ## Gaussian Example
    
    ![[attachments/Untitled 26 28.png|Untitled 26 28.png]]
    
    1. What is the likelihood of a single $X_i$﻿
        - This is just the PDF of the gaussian
            
            ![[attachments/Untitled 27 25.png|Untitled 27 25.png]]
            
    2. What is the log-likelihood of all the data?
        
        ![[attachments/Untitled 28 23.png|Untitled 28 23.png]]
        
    3. Find the parameters that maximize the log-likelihood
        1. To do this, derive it w.r.t each of the parameters
            
            ![[attachments/Untitled 29 22.png|Untitled 29 22.png]]
            
            ![[attachments/Untitled 30 22.png|Untitled 30 22.png]]
            
        2. Then, solve the system of equations. You have two unknowns and two equations.
            
            ![[attachments/Untitled 31 18.png|Untitled 31 18.png]]
            
            ![[attachments/Untitled 32 18.png|Untitled 32 18.png]]
            

# Properties of MLE

![[attachments/Untitled 33 17.png|Untitled 33 17.png]]

## Small Samples and bias

![[attachments/Untitled 34 13.png|Untitled 34 13.png]]