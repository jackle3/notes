---
Date: 2023-10-30
---
# Building Bayes Net with Data

- Consider the example below. The grid on the left is the correlation matrix, telling us how each random variable correlate with another random variable

![Untitled 166.png](../../attachments/Untitled%20166.png)

- By looking at the areas with high correlation, we can sort of find the dependence relationship between the different music genres.

# Inference with algebra

- To compute a joint probability, you can simply use the chain rule, starting from the parent and going to the child.
    
    ![Untitled 1 129.png](../../attachments/Untitled%201%20129.png)
    

$P(F_{lu} = 0, U = 1, F_{ev}=0, T = 1) \\ = P(F_{lu}=0) * P(U = 1)* P(F_{ev} = 0 | F_{lu} = 0) * P(T=1 | F_{lu}=0, U = 1)$

- To compute conditional probablities, we can compute the joint and the definition of conditional probability.
    - This works when we conditional on all the remaining variables
        
        ![Untitled 2 128.png](../../attachments/Untitled%202%20128.png)
        
    - It also works when we only condition on some variables, using LOTP → marginalization
        
        ![Untitled 3 125.png](../../attachments/Untitled%203%20125.png)
        
        - In the case above, the denominator is $P(U = 1, T = 1)$﻿
- This can be very expensive because we need to marginalize all the unseen variables.

# Rejection sampling

- First, you need to a fully specified Bayes Net, and you define it in code.

![Untitled 4 120.png](../../attachments/Untitled%204%20120.png)

![Untitled 5 118.png](../../attachments/Untitled%205%20118.png)

- To calculate probablities:
    - We generate billions (large amount) of samples from our joint
    - To find the final probability, just resort to counting.

## Creating Samples

- To sample a ton, just continously make samples in a loop.
    
    ![Untitled 6 116.png](../../attachments/Untitled%206%20116.png)
    
- To create a single sample. Generate a single fake sample using our joint Bayes net. This sample will have information about all of our random variables in the joint
    
    ![Untitled 7 113.png](../../attachments/Untitled%207%20113.png)
    
- These samples perfectly implement the Bayesian network

## Finding the probability

- Now that we have our samples, we first do rejecting, then we count from the remaining samples. In this case, it finds the probability
    
    $P(\text{Flu}=1| \text{observation})$
    
    - We reject (throw away) all the samples that don’t align with our observation, so that we can constraint out sample space to just the ones under the observation.
    
    ![Untitled 8 105.png](../../attachments/Untitled%208%20105.png)
    

## Summary

![Untitled 9 101.png](../../attachments/Untitled%209%20101.png)

# Issue with Rejection Sampling

- It’s possible to reject _every_ sample → happens when we condition on some event that has a low probability of occuring.

# Continous Random Variables

- To sample from a normal distribution, use `stats.norm.rvs(mu, std)`
    
    ![Untitled 10 96.png](../../attachments/Untitled%2010%2096.png)
    
- The issue mentioned above can happen a lot with continous random variables.
    - If our observation was that someone had a fever of 99.5, it is very unlikely that within our samples, there will be enough samples with exactly a 99.5 degrees → rejection sampling will reject everything.
- To fix this, you can do **rounding** on your continous random variable, to increase the chances of matching.
    - This is the same as discretizing the continous random variable.