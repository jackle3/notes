---
Date: 2023-10-30
---
# Building Bayes Net with Data

- Consider the example below. The grid on the left is the correlation matrix, telling us how each random variable correlate with another random variable

![[attachments/Untitled 166.png|Untitled 166.png]]

- By looking at the areas with high correlation, we can sort of find the dependence relationship between the different music genres.

# Inference with algebra

- To compute a joint probability, you can simply use the chain rule, starting from the parent and going to the child.
    
    ![[attachments/Untitled 1 129.png|Untitled 1 129.png]]
    

$P(F_{lu} = 0, U = 1, F_{ev}=0, T = 1) \\ = P(F_{lu}=0) * P(U = 1)* P(F_{ev} = 0 | F_{lu} = 0) * P(T=1 | F_{lu}=0, U = 1)$

- To compute conditional probablities, we can compute the joint and the definition of conditional probability.
    - This works when we conditional on all the remaining variables
        
        ![[attachments/Untitled 2 128.png|Untitled 2 128.png]]
        
    - It also works when we only condition on some variables, using LOTP → marginalization
        
        ![[attachments/Untitled 3 125.png|Untitled 3 125.png]]
        
        - In the case above, the denominator is $P(U = 1, T = 1)$﻿
- This can be very expensive because we need to marginalize all the unseen variables.

# Rejection sampling

- First, you need to a fully specified Bayes Net, and you define it in code.

![[attachments/Untitled 4 120.png|Untitled 4 120.png]]

![[attachments/Untitled 5 118.png|Untitled 5 118.png]]

- To calculate probablities:
    - We generate billions (large amount) of samples from our joint
    - To find the final probability, just resort to counting.

## Creating Samples

- To sample a ton, just continously make samples in a loop.
    
    ![[attachments/Untitled 6 116.png|Untitled 6 116.png]]
    
- To create a single sample. Generate a single fake sample using our joint Bayes net. This sample will have information about all of our random variables in the joint
    
    ![[attachments/Untitled 7 113.png|Untitled 7 113.png]]
    
- These samples perfectly implement the Bayesian network

## Finding the probability

- Now that we have our samples, we first do rejecting, then we count from the remaining samples. In this case, it finds the probability
    
    $P(\text{Flu}=1| \text{observation})$
    
    - We reject (throw away) all the samples that don’t align with our observation, so that we can constraint out sample space to just the ones under the observation.
    
    ![[attachments/Untitled 8 105.png|Untitled 8 105.png]]
    

## Summary

![[attachments/Untitled 9 101.png|Untitled 9 101.png]]

# Issue with Rejection Sampling

- It’s possible to reject _every_ sample → happens when we condition on some event that has a low probability of occuring.

# Continous Random Variables

- To sample from a normal distribution, use `stats.norm.rvs(mu, std)`
    
    ![[attachments/Untitled 10 96.png|Untitled 10 96.png]]
    
- The issue mentioned above can happen a lot with continous random variables.
    - If our observation was that someone had a fever of 99.5, it is very unlikely that within our samples, there will be enough samples with exactly a 99.5 degrees → rejection sampling will reject everything.
- To fix this, you can do **rounding** on your continous random variable, to increase the chances of matching.
    - This is the same as discretizing the continous random variable.