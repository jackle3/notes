---
Date: 2023-12-03
---
# Logistic Regression Summary

- We train our weights $\theta$﻿s to calculate the probability that the label equals 1 given the input data, and use that to make our prediction.

![Untitled 186.png](../../attachments/Untitled%20186.png)

- Remember that we have to add one extra feature to our inputs, for the intercept.

## Math for Logistic Regression

![Untitled 1 149.png](../../attachments/Untitled%201%20149.png)

- The log-likelihood function was just a log-likelihood of a Bernoulli, where $p = \sigma(\theta^T \mathbf{x})$﻿
    - The log-likelihood is a scoring function. Good parameters will get a high likelihood, while bad parameters will get a low value.

## Step size - learning rate

- If we have **really large step sizes**, we can overshoot the optimum, causing us to jump back and forth between the optimum → won’t actually converge to the best answer
- If we have a **really small step size**, we will very likely converge and reach the optimum answer, but it will take a really long time → not as efficient

# Neurons

- You can think of logistic regression as **a single neuron**. It takes in some inputs, and then sends out a single output.

![Untitled 2 148.png](../../attachments/Untitled%202%20148.png)

- Deep learning is like a full brain, where multiple neurons are connected together in a network, with possibly multiple layers.

![Untitled 3 145.png](../../attachments/Untitled%203%20145.png)

# Deep Learning

- Like logistic regression, deep learning gets its intelligence from its thetas (parameters)

![Untitled 4 140.png](../../attachments/Untitled%204%20140.png)

## Digit Recognition Example

- When the computer see pictures like the digits below, it just sees it as a **series of pixels**, where is it either black or white.
    - This is our feature vector.

![Untitled 5 138.png](../../attachments/Untitled%205%20138.png)

- We can attempt to solve this with a single logistic regression, passing in the series of pixels that we have. However, this would be pretty incorrect.

![Untitled 6 136.png](../../attachments/Untitled%206%20136.png)

![Untitled 7 132.png](../../attachments/Untitled%207%20132.png)

- To do deep learning, we can connect multiple neurons together in many layers.
- The layer in the middle is called a **hidden layer**. Every thing in the hidden layer is a single logistic regression connected to the inputs.
    
    ![Untitled 8 123.png](../../attachments/Untitled%208%20123.png)
    
- Once we have a hidden layer, we have another logistic regression that takes the hidden layer as input and predicts the output.
    
    ![Untitled 9 119.png](../../attachments/Untitled%209%20119.png)
    

# Training Deep Learning

- At its core, training is still doing MLE on the thetas (parameters)

## Notation

- Each hidden layer neuron $\mathbf{h}_j$﻿ is a logistic regression. In this case, it will have a parameter for each of the neurons in the input layer.
    
    ![Untitled 10 114.png](../../attachments/Untitled%2010%20114.png)
    
    - In other words, $\mathbf{h_j} = \sigma(\mathbf{x}^T \theta^{(h)}_j)$﻿, the weighted sum between the inputs and the parameters associated with neuron $\mathbf{h}_j$﻿.
    - Every hidden layer neuron $\mathbf{h}_j$﻿ gets the same input, but they just have different parameters $\theta^{(h)}_j$﻿
- There is also a logistic regression going from the last hidden layer to the output.
    
    ![Untitled 11 110.png](../../attachments/Untitled%2011%20110.png)
    
    - Equivalently, $\hat{y} = \sigma(\mathbf{h}^T \theta^{(\hat{y})})$﻿, where $\mathbf{h}$﻿ is just a vector of numbers denoting the result of the hidden layer.

## Forward pass

- Once we have our input, we can move forwards and calculate the value of every hidden neuron using each of their respective weights.
    
    - This is also known as activating the hidden layer.
    
    ![Untitled 12 107.png](../../attachments/Untitled%2012%20107.png)
    
- Then, we can calculate the value for the final neuron, which is our output. It takes each of the hidden layer values as the inputs.
    
    ![Untitled 13 99.png](../../attachments/Untitled%2013%2099.png)
    
    - Note that the output from the final neuron is the probability that the label should be 1 (assuming we are doing binary classification).
    - Once we do our forward pass, we score the forward pass by calculating the log-likelihood. This tells us how good our parameters (for all neurons) were.

## Calculating the number of parameters

![Untitled 14 87.png](../../attachments/Untitled%2014%2087.png)

![Untitled 15 83.png](../../attachments/Untitled%2015%2083.png)

- This is because the $\hat{y}$﻿ neuron only depends on the number of neurons in the hidden layer.
    - There is one parameter for each hidden layer neuron it’s connected to.

![Untitled 16 77.png](../../attachments/Untitled%2016%2077.png)

- For each hidden layer neuron $\mathbf{h}_j$﻿, it depends on the number of neurons in the input
    - As such, there are 20 parameters for each hidden layer neuron (in each $\theta^{(h)}_j$﻿)
    - Since there are 20 hidden layer neurons, there are **800 total parameters**.

![Untitled 17 72.png](../../attachments/Untitled%2017%2072.png)

## Summary of steps so far

1. Make the deep learning assumption
    
    ![Untitled 18 65.png](../../attachments/Untitled%2018%2065.png)
    
    - This is like the logistic regression assumption, but now with many logistic regression pieces stacked together.
2. Calculate the log likelihood of all the data
    
    ![Untitled 19 58.png](../../attachments/Untitled%2019%2058.png)
    
    ![Untitled 20 56.png](../../attachments/Untitled%2020%2056.png)
    
    - This log-likelihood function is the same as regular logistic regression. If we have $n$﻿ datapoints, it has that inner sum for each of the data points.
3. Fina the partial derivative of the log likelihood
    
    ![Untitled 21 50.png](../../attachments/Untitled%2021%2050.png)
    
    - We get the partial derivative so we can use optimization techniques like gradient ascent to find the **thetas that maximize likelihood**

## Backward pass (backpropagation)

- Our goal is to calculate the partial derivative with respect to all the output parameters, as well as all the hidden layer parameters.

![Untitled 22 46.png](../../attachments/Untitled%2022%2046.png)

### Derivative with output layer

![Untitled 23 43.png](../../attachments/Untitled%2023%2043.png)

- Get the derivative of the log-likelihood with respect to the probability estimate $\hat{y}$﻿.
    - Then calculate the derivative of that probability with respect to the parameters that went into calculating that probability.

![Untitled 24 39.png](../../attachments/Untitled%2024%2039.png)

- Using the chain rule, we can just calculate each of the decomposed terms.
    
    ![Untitled 25 34.png](../../attachments/Untitled%2025%2034.png)
    
    ![Untitled 26 29.png](../../attachments/Untitled%2026%2029.png)
    

![Untitled 27 26.png](../../attachments/Untitled%2027%2026.png)

### Derivative with hidden layers

- We can do the same for the derivative with respect to the parameters of the hidden layers

![Untitled 28 24.png](../../attachments/Untitled%2028%2024.png)

- We can then calculate with respect to each of these parts
    
    ![Untitled 29 23.png](../../attachments/Untitled%2029%2023.png)
    
    ![Untitled 30 23.png](../../attachments/Untitled%2030%2023.png)
    

![Untitled 31 19.png](../../attachments/Untitled%2031%2019.png)

# Multiple Output Classification

![Untitled 32 19.png](../../attachments/Untitled%2032%2019.png)

# Sharing Weights

![Untitled 33 18.png](../../attachments/Untitled%2033%2018.png)

# Deep Reinforcement Learning

![Untitled 34 14.png](../../attachments/Untitled%2034%2014.png)