---
Date: 2023-12-03
---
# Logistic Regression Summary

- We train our weights $\theta$﻿s to calculate the probability that the label equals 1 given the input data, and use that to make our prediction.

![[attachments/Untitled 186.png|Untitled 186.png]]

- Remember that we have to add one extra feature to our inputs, for the intercept.

## Math for Logistic Regression

![[attachments/Untitled 1 149.png|Untitled 1 149.png]]

- The log-likelihood function was just a log-likelihood of a Bernoulli, where $p = \sigma(\theta^T \mathbf{x})$﻿
    - The log-likelihood is a scoring function. Good parameters will get a high likelihood, while bad parameters will get a low value.

## Step size - learning rate

- If we have **really large step sizes**, we can overshoot the optimum, causing us to jump back and forth between the optimum → won’t actually converge to the best answer
- If we have a **really small step size**, we will very likely converge and reach the optimum answer, but it will take a really long time → not as efficient

# Neurons

- You can think of logistic regression as **a single neuron**. It takes in some inputs, and then sends out a single output.

![[attachments/Untitled 2 148.png|Untitled 2 148.png]]

- Deep learning is like a full brain, where multiple neurons are connected together in a network, with possibly multiple layers.

![[attachments/Untitled 3 145.png|Untitled 3 145.png]]

# Deep Learning

- Like logistic regression, deep learning gets its intelligence from its thetas (parameters)

![[attachments/Untitled 4 140.png|Untitled 4 140.png]]

## Digit Recognition Example

- When the computer see pictures like the digits below, it just sees it as a **series of pixels**, where is it either black or white.
    - This is our feature vector.

![[attachments/Untitled 5 138.png|Untitled 5 138.png]]

- We can attempt to solve this with a single logistic regression, passing in the series of pixels that we have. However, this would be pretty incorrect.

![[attachments/Untitled 6 136.png|Untitled 6 136.png]]

![[attachments/Untitled 7 132.png|Untitled 7 132.png]]

- To do deep learning, we can connect multiple neurons together in many layers.
- The layer in the middle is called a **hidden layer**. Every thing in the hidden layer is a single logistic regression connected to the inputs.
    
    ![[attachments/Untitled 8 123.png|Untitled 8 123.png]]
    
- Once we have a hidden layer, we have another logistic regression that takes the hidden layer as input and predicts the output.
    
    ![[attachments/Untitled 9 119.png|Untitled 9 119.png]]
    

# Training Deep Learning

- At its core, training is still doing MLE on the thetas (parameters)

## Notation

- Each hidden layer neuron $\mathbf{h}_j$﻿ is a logistic regression. In this case, it will have a parameter for each of the neurons in the input layer.
    
    ![[attachments/Untitled 10 114.png|Untitled 10 114.png]]
    
    - In other words, $\mathbf{h_j} = \sigma(\mathbf{x}^T \theta^{(h)}_j)$﻿, the weighted sum between the inputs and the parameters associated with neuron $\mathbf{h}_j$﻿.
    - Every hidden layer neuron $\mathbf{h}_j$﻿ gets the same input, but they just have different parameters $\theta^{(h)}_j$﻿
- There is also a logistic regression going from the last hidden layer to the output.
    
    ![[attachments/Untitled 11 110.png|Untitled 11 110.png]]
    
    - Equivalently, $\hat{y} = \sigma(\mathbf{h}^T \theta^{(\hat{y})})$﻿, where $\mathbf{h}$﻿ is just a vector of numbers denoting the result of the hidden layer.

## Forward pass

- Once we have our input, we can move forwards and calculate the value of every hidden neuron using each of their respective weights.
    
    - This is also known as activating the hidden layer.
    
    ![[attachments/Untitled 12 107.png|Untitled 12 107.png]]
    
- Then, we can calculate the value for the final neuron, which is our output. It takes each of the hidden layer values as the inputs.
    
    ![[attachments/Untitled 13 99.png|Untitled 13 99.png]]
    
    - Note that the output from the final neuron is the probability that the label should be 1 (assuming we are doing binary classification).
    - Once we do our forward pass, we score the forward pass by calculating the log-likelihood. This tells us how good our parameters (for all neurons) were.

## Calculating the number of parameters

![[attachments/Untitled 14 87.png|Untitled 14 87.png]]

![[attachments/Untitled 15 83.png|Untitled 15 83.png]]

- This is because the $\hat{y}$﻿ neuron only depends on the number of neurons in the hidden layer.
    - There is one parameter for each hidden layer neuron it’s connected to.

![[attachments/Untitled 16 77.png|Untitled 16 77.png]]

- For each hidden layer neuron $\mathbf{h}_j$﻿, it depends on the number of neurons in the input
    - As such, there are 20 parameters for each hidden layer neuron (in each $\theta^{(h)}_j$﻿)
    - Since there are 20 hidden layer neurons, there are **800 total parameters**.

![[attachments/Untitled 17 72.png|Untitled 17 72.png]]

## Summary of steps so far

1. Make the deep learning assumption
    
    ![[attachments/Untitled 18 65.png|Untitled 18 65.png]]
    
    - This is like the logistic regression assumption, but now with many logistic regression pieces stacked together.
2. Calculate the log likelihood of all the data
    
    ![[attachments/Untitled 19 58.png|Untitled 19 58.png]]
    
    ![[attachments/Untitled 20 56.png|Untitled 20 56.png]]
    
    - This log-likelihood function is the same as regular logistic regression. If we have $n$﻿ datapoints, it has that inner sum for each of the data points.
3. Fina the partial derivative of the log likelihood
    
    ![[attachments/Untitled 21 50.png|Untitled 21 50.png]]
    
    - We get the partial derivative so we can use optimization techniques like gradient ascent to find the **thetas that maximize likelihood**

## Backward pass (backpropagation)

- Our goal is to calculate the partial derivative with respect to all the output parameters, as well as all the hidden layer parameters.

![[attachments/Untitled 22 46.png|Untitled 22 46.png]]

### Derivative with output layer

![[attachments/Untitled 23 43.png|Untitled 23 43.png]]

- Get the derivative of the log-likelihood with respect to the probability estimate $\hat{y}$﻿.
    - Then calculate the derivative of that probability with respect to the parameters that went into calculating that probability.

![[attachments/Untitled 24 39.png|Untitled 24 39.png]]

- Using the chain rule, we can just calculate each of the decomposed terms.
    
    ![[attachments/Untitled 25 34.png|Untitled 25 34.png]]
    
    ![[attachments/Untitled 26 29.png|Untitled 26 29.png]]
    

![[attachments/Untitled 27 26.png|Untitled 27 26.png]]

### Derivative with hidden layers

- We can do the same for the derivative with respect to the parameters of the hidden layers

![[attachments/Untitled 28 24.png|Untitled 28 24.png]]

- We can then calculate with respect to each of these parts
    
    ![[attachments/Untitled 29 23.png|Untitled 29 23.png]]
    
    ![[attachments/Untitled 30 23.png|Untitled 30 23.png]]
    

![[attachments/Untitled 31 19.png|Untitled 31 19.png]]

# Multiple Output Classification

![[attachments/Untitled 32 19.png|Untitled 32 19.png]]

# Sharing Weights

![[attachments/Untitled 33 18.png|Untitled 33 18.png]]

# Deep Reinforcement Learning

![[attachments/Untitled 34 14.png|Untitled 34 14.png]]